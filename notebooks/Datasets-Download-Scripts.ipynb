{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee65cf4-5a6a-4f3a-a674-aa3566183aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared helpers\n",
    "\n",
    "from pathlib import Path\n",
    "import os, time, ssl, random\n",
    "import urllib.request, urllib.error\n",
    "import concurrent.futures\n",
    "import re  # used later\n",
    "\n",
    "# Repo-relative base (safe for reviewers)\n",
    "BASE_DIR = Path(\"./data\")\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# HTTPS context + polite UA (some hosts 403 without a UA)\n",
    "_SSL_CTX = ssl.create_default_context()\n",
    "_USER_AGENT = \"Mozilla/5.0 (compatible; dataset-downloader/1.0)\"\n",
    "\n",
    "def fetch_url_to_file(\n",
    "    url: str,\n",
    "    local: Path,\n",
    "    retries: int = 3,\n",
    "    sleep_s: float = 2.0,\n",
    "    chunk_size: int = 1 << 20\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Download 'url' to 'local' with retries and atomic write (.part file).\n",
    "    Returns: 'skip …', 'done …', or 'FAIL … (error)'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if local.exists() and local.stat().st_size > 0:\n",
    "            return f\"skip  {local.name}\"\n",
    "\n",
    "        local.parent.mkdir(parents=True, exist_ok=True)\n",
    "        tmp = local.with_suffix(local.suffix + \".part\")\n",
    "        if tmp.exists():\n",
    "            try: tmp.unlink()\n",
    "            except Exception: pass\n",
    "\n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                req = urllib.request.Request(url, headers={\"User-Agent\": _USER_AGENT, \"Accept\": \"*/*\"})\n",
    "                with urllib.request.urlopen(req, context=_SSL_CTX, timeout=120) as resp, open(tmp, \"wb\") as f:\n",
    "                    while True:\n",
    "                        block = resp.read(chunk_size)\n",
    "                        if not block:\n",
    "                            break\n",
    "                        f.write(block)\n",
    "                os.replace(tmp, local)  # atomic move\n",
    "                return f\"done  {local.name}\"\n",
    "\n",
    "            except urllib.error.HTTPError as e:\n",
    "                if e.code in (429, 500, 502, 503, 504):\n",
    "                    time.sleep(sleep_s * attempt * (1.0 + 0.25 * random.random()))\n",
    "                    continue\n",
    "                return f\"FAIL  {local.name}  (HTTP {e.code}: {e.reason})\"\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt == retries:\n",
    "                    return f\"FAIL  {local.name}  ({e})\"\n",
    "                time.sleep(sleep_s * attempt * (1.0 + 0.25 * random.random()))\n",
    "\n",
    "        return f\"FAIL  {local.name}  (exhausted retries)\"\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            tmp = local.with_suffix(local.suffix + \".part\")\n",
    "            if tmp.exists():\n",
    "                tmp.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def run_parallel(items, worker_fn, max_workers: int = 8):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as exe:\n",
    "        for msg in exe.map(worker_fn, items):\n",
    "            print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f155f-847d-4d91-8f34-f70501792093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRISM — daily precipitation (ppt), 4 km, CONUS (1991–2013)\n",
    "# Source: https://prism.oregonstate.edu/time_series/us/4km/an/ppt/daily/YYYY/\n",
    "# Files : PRISM_ppt_stable_4kmD2_YYYYMMDD_bil.zip\n",
    "# Cite  : Daly et al., 2008; PRISM Climate Group, OSU\n",
    "\n",
    "from datetime import date, timedelta\n",
    "\n",
    "PRISM_OUT    = BASE_DIR / \"prism\" / \"ppt_daily_zip\"\n",
    "PRISM_WORKERS= 8\n",
    "PRISM_Y0, PRISM_Y1 = 1991, 2013\n",
    "PRISM_BASE   = \"https://data.prism.oregonstate.edu/daily\"\n",
    "\n",
    "def _date_range(y0: int, y1: int):\n",
    "    d, end, step = date(y0,1,1), date(y1,12,31), timedelta(days=1)\n",
    "    while d <= end:\n",
    "        yield d\n",
    "        d += step\n",
    "\n",
    "def prism_tasks():\n",
    "    for d in _date_range(PRISM_Y0, PRISM_Y1):\n",
    "        fname = f\"PRISM_ppt_stable_4kmD2_{d:%Y%m%d}_bil.zip\"\n",
    "        url   = f\"{PRISM_BASE}/ppt/{d:%Y}/{fname}\"\n",
    "        local = PRISM_OUT / f\"{d:%Y}\" / fname\n",
    "        yield (url, local)\n",
    "\n",
    "def prism_worker(task):\n",
    "    url, local = task\n",
    "    return fetch_url_to_file(url, local)\n",
    "\n",
    "PRISM_RUN = False  # flip True to download\n",
    "if PRISM_RUN:\n",
    "    tasks = list(prism_tasks())\n",
    "    print(f\"{len(tasks):,} files → {PRISM_OUT}\")\n",
    "    run_parallel(tasks, prism_worker, max_workers=PRISM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9fae8-bc9a-4c83-8a84-0ea0d9279730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHIRPS v2.0 — daily precipitation, 0.05° global (1991–2013)\n",
    "# Dir   : https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/netcdf/p05/\n",
    "# Files : chirps-v2.0.YYYY.days_p05.nc\n",
    "# Cite  : Funk et al., 2015 (Sci Data)\n",
    "\n",
    "CHIRPS_OUT     = BASE_DIR / \"chirps\" / \"p05_daily_nc\"\n",
    "CHIRPS_WORKERS = 6\n",
    "CHIRPS_Y0, CHIRPS_Y1 = 1991, 2013\n",
    "CHIRPS_BASE    = \"https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/netcdf/p05\"\n",
    "\n",
    "def chirps_tasks():\n",
    "    for y in range(CHIRPS_Y0, CHIRPS_Y1 + 1):\n",
    "        fname = f\"chirps-v2.0.{y}.days_p05.nc\"\n",
    "        url   = f\"{CHIRPS_BASE}/{fname}\"\n",
    "        local = CHIRPS_OUT / fname\n",
    "        yield (url, local)\n",
    "\n",
    "def chirps_worker(task):\n",
    "    url, local = task\n",
    "    return fetch_url_to_file(url, local)\n",
    "\n",
    "CHIRPS_RUN = False\n",
    "if CHIRPS_RUN:\n",
    "    tasks = list(chirps_tasks())\n",
    "    print(f\"{len(tasks):,} files → {CHIRPS_OUT}\")\n",
    "    run_parallel(tasks, chirps_worker, max_workers=CHIRPS_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88657387-a9c6-4b12-b48a-83c93cd5fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5 — hourly single levels (example: t2m, tp), 1991–2013\n",
    "# Dataset : reanalysis-era5-single-levels\n",
    "# Setup   : pip install cdsapi ; create ~/.cdsapirc with your CDS key\n",
    "# Output  : one NetCDF per year (adjust area/vars as needed)\n",
    "# Cite    : Hersbach et al., 2020; Copernicus C3S\n",
    "\n",
    "import cdsapi\n",
    "\n",
    "ERA5_OUT    = BASE_DIR / \"era5\" / \"hourly_single_levels\"\n",
    "ERA5_Y0, ERA5_Y1 = 1991, 2013\n",
    "ERA5_AREA   = [55.0, -95.0, 40.0, -74.0]  # N, W, S, E (edit if needed)\n",
    "ERA5_VARS   = [\"2m_temperature\", \"total_precipitation\"]\n",
    "\n",
    "def era5_download_year(y: int):\n",
    "    target = ERA5_OUT / f\"era5_hourly_t2m_tp_{y}.nc\"\n",
    "    if target.exists():\n",
    "        print(f\"skip  {target.name}\")\n",
    "        return\n",
    "    c = cdsapi.Client()\n",
    "    c.retrieve(\n",
    "        \"reanalysis-era5-single-levels\",\n",
    "        {\n",
    "            \"product_type\": \"reanalysis\",\n",
    "            \"variable\": ERA5_VARS,\n",
    "            \"year\": f\"{y}\",\n",
    "            \"month\": [f\"{m:02d}\" for m in range(1, 13)],\n",
    "            \"day\":   [f\"{d:02d}\" for d in range(1, 32)],  # CDS accepts 1..31\n",
    "            \"time\":  [f\"{h:02d}:00\" for h in range(24)],\n",
    "            \"format\": \"netcdf\",\n",
    "            \"area\": ERA5_AREA,\n",
    "        },\n",
    "        str(target),\n",
    "    )\n",
    "    print(f\"done  {target.name}\")\n",
    "\n",
    "ERA5_RUN = False\n",
    "if ERA5_RUN:\n",
    "    ERA5_OUT.mkdir(parents=True, exist_ok=True)\n",
    "    for y in range(ERA5_Y0, ERA5_Y1 + 1):\n",
    "        era5_download_year(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7bb506-b474-4b74-a5e4-d329db7cf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDRS v2.1 (1-hour) — Ouranos THREDDS NetCDF Subset (NCSS)\n",
    "# Catalog: https://pavics.ouranos.ca/.../thredds/catalog/datasets/reanalyses/catalog.html\n",
    "# Dataset: datasets/reanalyses/1hr_RDRSv2.1_NAM.ncml\n",
    "# Note   : Confirm variable names on NCSS UI\n",
    "# Cite   : (project docs / Ouranos)\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "RDRS_OUT     = BASE_DIR / \"rdrs\" / \"v2_1_ncss\"\n",
    "RDRS_WORKERS = 4\n",
    "RDRS_Y0, RDRS_Y1 = 1991, 2013\n",
    "\n",
    "NCSS_BASE = (\n",
    "    \"https://pavics.ouranos.ca/twitcher/ows/proxy/thredds\"\n",
    "    \"/ncss/datasets/reanalyses/1hr_RDRSv2.1_NAM.ncml\"\n",
    ")\n",
    "\n",
    "# edit to your domain and vars\n",
    "RDRS_BBOX = dict(miny=40.0, minx=-95.0, maxy=55.0, maxx=-74.0)\n",
    "RDRS_VARS = [\"tas\", \"pr\"]\n",
    "\n",
    "def rdrs_ncss_url_for_year(y: int) -> str:\n",
    "    params = {\n",
    "        \"var\": RDRS_VARS,\n",
    "        \"north\": RDRS_BBOX[\"maxy\"], \"south\": RDRS_BBOX[\"miny\"],\n",
    "        \"east\":  RDRS_BBOX[\"maxx\"], \"west\":  RDRS_BBOX[\"minx\"],\n",
    "        \"time_start\": f\"{y}-01-01T00:00:00Z\",\n",
    "        \"time_end\":   f\"{y}-12-31T23:00:00Z\",\n",
    "        \"accept\": \"netcdf\",\n",
    "    }\n",
    "    return f\"{NCSS_BASE}?{urlencode(params, doseq=True)}\"\n",
    "\n",
    "def rdrs_worker(y: int):\n",
    "    url   = rdrs_ncss_url_for_year(y)\n",
    "    local = RDRS_OUT / f\"RDRSv2.1_{y}_subset.nc\"\n",
    "    return fetch_url_to_file(url, local)\n",
    "\n",
    "RDRS_RUN = False\n",
    "if RDRS_RUN:\n",
    "    years = list(range(RDRS_Y0, RDRS_Y1 + 1))\n",
    "    print(f\"{len(years)} files → {RDRS_OUT}\")\n",
    "    run_parallel(years, rdrs_worker, max_workers=RDRS_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003293a5-3f1e-402d-ae1f-6bc1c79bb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMDNA — download selected files (1991–2013) directly from FRDR (Dataverse APIs)\n",
    "# DOI  : 10.20383/101.0275\n",
    "# Docs : Native API (list files) + Data Access API (download by file id)\n",
    "# Cite : Tang et al., 2021 (ESSD)\n",
    "\n",
    "import json, urllib.request, urllib.parse\n",
    "\n",
    "EMDNA_OUT   = BASE_DIR / \"emdna\"\n",
    "FRDR_HOST   = \"https://www.frdr-dfdr.ca\"\n",
    "DATASET_DOI = \"doi:10.20383/101.0275\"\n",
    "\n",
    "LIST_URL = (f\"{FRDR_HOST}/api/datasets/:persistentId/versions/:latest-published/files\"\n",
    "            f\"?persistentId={urllib.parse.quote(DATASET_DOI)}\")\n",
    "\n",
    "# Filter to years/variables you need\n",
    "YEAR_MIN, YEAR_MAX = 1991, 2013\n",
    "KEEP_VARS = (\"pr\", \"tmean\", \"trange\")  # adapt as needed\n",
    "_year_re = re.compile(r\"(19|20)\\d{2}\")\n",
    "\n",
    "def _is_keep(label: str) -> bool:\n",
    "    lname = label.lower()\n",
    "    if KEEP_VARS and not any(v in lname for v in KEEP_VARS):\n",
    "        return False\n",
    "    m = _year_re.search(lname)\n",
    "    if not m:\n",
    "        return False\n",
    "    y = int(m.group(0))\n",
    "    return YEAR_MIN <= y <= YEAR_MAX\n",
    "\n",
    "def emdna_list_files():\n",
    "    with urllib.request.urlopen(LIST_URL, context=_SSL_CTX, timeout=120) as r:\n",
    "        payload = json.load(r)\n",
    "    files = payload.get(\"data\", []) if isinstance(payload, dict) else []\n",
    "    recs = []\n",
    "    for f in files:\n",
    "        meta  = f.get(\"dataFile\") or {}\n",
    "        fid   = meta.get(\"id\")\n",
    "        label = f.get(\"label\") or meta.get(\"filename\")\n",
    "        if fid and label and _is_keep(label):\n",
    "            recs.append({\"id\": fid, \"label\": label})\n",
    "    return recs\n",
    "\n",
    "def emdna_tasks():\n",
    "    for rec in emdna_list_files():\n",
    "        fid, label = rec[\"id\"], rec[\"label\"]\n",
    "        url   = f\"{FRDR_HOST}/api/access/datafile/{fid}?format=original\"\n",
    "        local = EMDNA_OUT / label\n",
    "        yield (url, local)\n",
    "\n",
    "def emdna_worker(task):\n",
    "    url, local = task\n",
    "    return fetch_url_to_file(url, local)\n",
    "\n",
    "EMDNA_RUN = False\n",
    "if EMDNA_RUN:\n",
    "    tasks = list(emdna_tasks())\n",
    "    print(f\"{len(tasks):,} files → {EMDNA_OUT}\")\n",
    "    run_parallel(tasks, emdna_worker, max_workers=6)\n",
    "\n",
    "\n",
    "## Or you may simply go to this website and download any ensemble member of EMDNA: https://www.frdr-dfdr.ca/repo/dataset/4bb24ee2-73e1-43a8-a929-126d2eb2bfa3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d0e12-856c-4c09-88a7-a159b4518548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERRA-2 — hourly single-level diagnostics (daily granules), 1991–2013\n",
    "# Products: M2T1NXSLV.5.12.4 (T2M etc.), M2T1NXFLX.5.12.4 (precip rate/flux)\n",
    "# Streams : 1980–1991:100, 1992–2000:200, 2001–2010:300, 2011–present:400\n",
    "# Host   : goldsmr4.gesdisc.eosdis.nasa.gov (may vary to goldsmr5,…)\n",
    "# Auth   : Earthdata Login via ~/.netrc (DO NOT commit credentials)\n",
    "# Cite   : Gelaro et al., 2017\n",
    "\n",
    "import requests\n",
    "from datetime import date, timedelta\n",
    "\n",
    "MERRA_OUT   = BASE_DIR / \"merra2\"\n",
    "MERRA_HOST  = \"https://goldsmr4.gesdisc.eosdis.nasa.gov\"\n",
    "COLL_SLV    = \"MERRA2/M2T1NXSLV.5.12.4\"\n",
    "COLL_FLX    = \"MERRA2/M2T1NXFLX.5.12.4\"\n",
    "DATE0, DATE1 = date(1991, 1, 1), date(2013, 12, 31)\n",
    "PRODUCTS    = [(\"slv\", COLL_SLV), (\"flx\", COLL_FLX)]  # trim as needed\n",
    "\n",
    "def _stream(y: int) -> int:\n",
    "    if y <= 1991:  return 100\n",
    "    if y <= 2000:  return 200\n",
    "    if y <= 2010:  return 300\n",
    "    return 400\n",
    "\n",
    "def _daily_url(prod_coll: str, d: date):\n",
    "    stream = _stream(d.year)\n",
    "    short  = prod_coll.split(\"/\")[1].split(\".\", 1)[0]  # e.g., M2T1NXSLV\n",
    "    group  = \"slv\" if \"SLV\" in short.upper() else \"flx\"\n",
    "    fname  = f\"MERRA2_{stream}.tavg1_2d_{group}_Nx.{d:%Y%m%d}.nc4\"\n",
    "    url    = f\"{MERRA_HOST}/data/{prod_coll}/{d:%Y/%m}/{fname}\"\n",
    "    return url, fname\n",
    "\n",
    "def _date_range(d0: date, d1: date):\n",
    "    d, step = d0, timedelta(days=1)\n",
    "    while d <= d1:\n",
    "        yield d\n",
    "        d += step\n",
    "\n",
    "def merra_worker(task):\n",
    "    url, local = task\n",
    "    s = requests.Session()  # uses ~/.netrc for Earthdata during redirects\n",
    "    try:\n",
    "        with s.get(url, stream=True, timeout=120) as r:\n",
    "            r.raise_for_status()\n",
    "            if local.exists():\n",
    "                return f\"skip  {local.name}\"\n",
    "            local.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(local, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=1 << 20):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        return f\"done  {local.name}\"\n",
    "    except Exception as e:\n",
    "        return f\"FAIL  {local.name}  ({e})\"\n",
    "\n",
    "def merra_tasks():\n",
    "    for tag, coll in PRODUCTS:\n",
    "        for d in _date_range(DATE0, DATE1):\n",
    "            url, fname = _daily_url(coll, d)\n",
    "            local = MERRA_OUT / tag / f\"{d:%Y}\" / fname\n",
    "            yield (url, local)\n",
    "\n",
    "MERRA_RUN = False  # WARNING: very large if True for full period+both products\n",
    "if MERRA_RUN:\n",
    "    tasks = list(merra_tasks())\n",
    "    print(f\"{len(tasks):,} daily files → {MERRA_OUT}  (consider narrowing dates/products)\")\n",
    "    run_parallel(tasks, merra_worker, max_workers=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
