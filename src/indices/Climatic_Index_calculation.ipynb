{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31fa8a-964a-48e3-ab57-07c1f190f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here in these scripts, we have the calculation methods of all climatic indices for both precipitation and temperature indices for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac4983-5d29-450e-8a0a-ad45c7357393",
   "metadata": {},
   "outputs": [],
   "source": [
    "                 #########################                                           ##############################\n",
    "                 #########################                  PRISM                    ##############################\n",
    "                 #########################                                           ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60afbe0c-8515-4878-aa6f-c7f9124b59a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the climatic indices for PRISM (Precipitation)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIG\n",
    "###############################################################################\n",
    "csv_file       = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\PRISM_GLB_Precipitation\\daily_loop3\\prism_vs_stations_8Nearest_LWR_1991_2012.csv\"\n",
    "physical_file  = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\Split_by_country\\filtered_stations_US.csv\"\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "output_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\PRISM_GLB_Precipitation\\ClimaticIndices-8Nearest-3\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY CSV DATA\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data (obs, prism_lwr8_val) ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "# parse 'time' as datetime if needed\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# unify station_name: remove leading/trailing spaces, uppercase\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(f\"df_data shape = {df_data.shape}\")\n",
    "print(\"Columns:\", df_data.columns.tolist())\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. (OPTIONAL) MERGE WITH PHYSICAL FILE TO GET LAT/LON\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "# unify station_name\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "###############################################################################\n",
    "def calc_rx1day(series):\n",
    "    \"\"\"Max 1-day precipitation.\"\"\"\n",
    "    return series.max(skipna=True)\n",
    "\n",
    "def calc_rx5day(series):\n",
    "    \"\"\"Max 5-day running sum.\"\"\"\n",
    "    roll_5 = series.rolling(5, min_periods=1).sum()\n",
    "    return roll_5.max(skipna=True)\n",
    "\n",
    "def calc_cdd(series, dry_threshold=1.0):\n",
    "    \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "    is_dry = series < dry_threshold\n",
    "    maxr, curr = 0, 0\n",
    "    for val in is_dry:\n",
    "        if val:\n",
    "            curr += 1\n",
    "            maxr = max(maxr, curr)\n",
    "        else:\n",
    "            curr = 0\n",
    "    return maxr\n",
    "\n",
    "def calc_cwd(series, wet_threshold=1.0):\n",
    "    \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "    is_wet = series >= wet_threshold\n",
    "    maxr, curr = 0, 0\n",
    "    for val in is_wet:\n",
    "        if val:\n",
    "            curr += 1\n",
    "            maxr = max(maxr, curr)\n",
    "        else:\n",
    "            curr = 0\n",
    "    return maxr\n",
    "\n",
    "def calc_r95p_r99p(series, percentile=(95,99)):\n",
    "    \"\"\"R95p, R99p TOT in mm, plus percentage of total.\"\"\"\n",
    "    # only wet days >=1 mm for percentile\n",
    "    wet = series[series >= 1.0]\n",
    "    if len(wet) < 5:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    p95 = np.percentile(wet, percentile[0])\n",
    "    p99 = np.percentile(wet, percentile[1])\n",
    "    r95_amt = wet[wet > p95].sum()\n",
    "    r99_amt = wet[wet > p99].sum()\n",
    "    total   = series.sum(skipna=True)\n",
    "    r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "    r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "    return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "\n",
    "def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "    \"\"\"Count #wet days >=5 mm, #dry days <1 mm.\"\"\"\n",
    "    w = (series >= wet_thr).sum()\n",
    "    d = (series <  dry_thr).sum()\n",
    "    return w, d\n",
    "\n",
    "###############################################################################\n",
    "# 5. COMPUTE INDICES FOR EACH STATION\n",
    "###############################################################################\n",
    "rx1_list, rx5_list = [], []\n",
    "cdd_list, cwd_list = [], []\n",
    "r95_list, r99_list = [], []\n",
    "wet_list, dry_list = [], []\n",
    "\n",
    "print(\"Computing indices for each station...\")\n",
    "\n",
    "grouped = df_data.groupby(\"station_name\", as_index=False)\n",
    "for st_name, grp in grouped:\n",
    "    # Sort by time (just in case)\n",
    "    grp = grp.sort_values(\"time\")\n",
    "\n",
    "    # daily obs/era5\n",
    "    obs_series = grp[\"obs\"].dropna().reset_index(drop=True)\n",
    "    prism_series = grp[\"prism_lwr8_val\"].dropna().reset_index(drop=True)  # <--- REFERENCE CHANGED HERE\n",
    "\n",
    "    # A) Rx1day\n",
    "    obs_rx1 = calc_rx1day(obs_series)\n",
    "    prism_rx1 = calc_rx1day(prism_series)\n",
    "    rx1_list.append({\"station_name\": st_name,\n",
    "                     \"obs_rx1day\": obs_rx1,\n",
    "                     \"prism_rx1day\": prism_rx1})\n",
    "\n",
    "    # B) Rx5day\n",
    "    obs_rx5 = calc_rx5day(obs_series)\n",
    "    prism_rx5 = calc_rx5day(prism_series)\n",
    "    rx5_list.append({\"station_name\": st_name,\n",
    "                     \"obs_rx5day\": obs_rx5,\n",
    "                     \"prism_rx5day\": prism_rx5})\n",
    "\n",
    "    # C) CDD\n",
    "    obs_cdd_val = calc_cdd(obs_series)\n",
    "    prism_cdd_val = calc_cdd(prism_series)\n",
    "    cdd_list.append({\"station_name\": st_name,\n",
    "                     \"obs_cdd\": obs_cdd_val,\n",
    "                     \"prism_cdd\": prism_cdd_val})\n",
    "\n",
    "    # D) CWD\n",
    "    obs_cwd_val = calc_cwd(obs_series)\n",
    "    prism_cwd_val = calc_cwd(prism_series)\n",
    "    cwd_list.append({\"station_name\": st_name,\n",
    "                     \"obs_cwd\": obs_cwd_val,\n",
    "                     \"prism_cwd\": prism_cwd_val})\n",
    "\n",
    "    # E) R95 / R99\n",
    "    or95a, or95p, or99a, or99p = calc_r95p_r99p(obs_series)\n",
    "    er95a, er95p, er99a, er99p = calc_r95p_r99p(prism_series)\n",
    "    r95_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_r95amt\": or95a, \"obs_r95pct\": or95p,\n",
    "        \"prism_r95amt\": er95a, \"prism_r95pct\": er95p\n",
    "    })\n",
    "    r99_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_r99amt\": or99a, \"obs_r99pct\": or99p,\n",
    "        \"prism_r99amt\": er99a, \"prism_r99pct\": er99p\n",
    "    })\n",
    "\n",
    "    # F) wet/dry days\n",
    "    obs_wet5, obs_dry = calc_wetdays_drydays(obs_series)\n",
    "    prism_wet5, prism_dry = calc_wetdays_drydays(prism_series)\n",
    "    wet_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_wetdays5mm\": obs_wet5, \n",
    "        \"prism_wetdays5mm\": prism_wet5\n",
    "    })\n",
    "    dry_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_drydays\": obs_dry,\n",
    "        \"prism_drydays\": prism_dry\n",
    "    })\n",
    "\n",
    "print(\"Finished computing. Now merging lat/lon from physical file ...\")\n",
    "\n",
    "def attach_coords(df_in):\n",
    "    \"\"\"Attach lat, lon, elev from df_phys on station_name.\"\"\"\n",
    "    df_out = pd.merge(\n",
    "        df_in, \n",
    "        df_phys[[\"station_name\",\"lat\",\"lon\",\"elev\"]],\n",
    "        on=\"station_name\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "    return df_out\n",
    "\n",
    "df_rx1 = attach_coords(pd.DataFrame(rx1_list))\n",
    "df_rx5 = attach_coords(pd.DataFrame(rx5_list))\n",
    "df_cdd = attach_coords(pd.DataFrame(cdd_list))\n",
    "df_cwd = attach_coords(pd.DataFrame(cwd_list))\n",
    "df_r95 = attach_coords(pd.DataFrame(r95_list))\n",
    "df_r99 = attach_coords(pd.DataFrame(r99_list))\n",
    "df_wet = attach_coords(pd.DataFrame(wet_list))\n",
    "df_dry = attach_coords(pd.DataFrame(dry_list))\n",
    "\n",
    "###############################################################################\n",
    "# 6. SAVE OUTPUT\n",
    "###############################################################################\n",
    "print(\"Saving index tables to Excel in:\", output_dir)\n",
    "df_rx1.to_excel(os.path.join(output_dir, \"rx1day.xlsx\"),  index=False)\n",
    "df_rx5.to_excel(os.path.join(output_dir, \"rx5day.xlsx\"),  index=False)\n",
    "df_cdd.to_excel(os.path.join(output_dir, \"cdd.xlsx\"),     index=False)\n",
    "df_cwd.to_excel(os.path.join(output_dir, \"cwd.xlsx\"),     index=False)\n",
    "df_r95.to_excel(os.path.join(output_dir, \"r95p.xlsx\"),    index=False)\n",
    "df_r99.to_excel(os.path.join(output_dir, \"r99p.xlsx\"),    index=False)\n",
    "df_wet.to_excel(os.path.join(output_dir, \"wetdays.xlsx\"), index=False)\n",
    "df_dry.to_excel(os.path.join(output_dir, \"drydays.xlsx\"), index=False)\n",
    "\n",
    "print(\"\\nAll precipitation-based indices have been saved to Excel.\")\n",
    "\n",
    "###############################################################################\n",
    "# (OPTIONAL) QUICK MAP EXAMPLE\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nQuick map example for obs_rx5day ...\")\n",
    "    # Load shapefiles\n",
    "    gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "    gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        df_rx5,\n",
    "        geometry=gpd.points_from_xy(df_rx5[\"lon\"], df_rx5[\"lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "    for geom in gdf_basin.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='blue', linewidth=1)\n",
    "    for geom in gdf_lakes.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='cyan', linewidth=1)\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "    sc = ax.scatter(gdf_stations.geometry.x, gdf_stations.geometry.y,\n",
    "                    c=gdf_stations[\"obs_rx5day\"], cmap=\"Reds\", s=60,\n",
    "                    transform=ccrs.PlateCarree(), edgecolor=\"k\")\n",
    "    plt.colorbar(sc, ax=ax, label=\"Obs Rx5day (mm)\")\n",
    "    ax.set_extent([-95.5, -72, 38.5, 52.5])  # approximate bounding box\n",
    "    gl = ax.gridlines(draw_labels=True, linestyle='--', color='gray')\n",
    "    gl.right_labels = False\n",
    "    gl.top_labels   = False\n",
    "\n",
    "    plt.title(\"Obs Rx5day (from CSV daily data)\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Mapping step failed:\", e)\n",
    "\n",
    "print(\"\\n✅ Done computing precipitation-based indices from 'prism_lwr8_val' column!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f025922-4190-4f51-a196-e230f240e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the climatic indices for PRISM (Temperature - Tmin/Tmax)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIG\n",
    "###############################################################################\n",
    "csv_file = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\PRISM_GLB_Temperature\\daily_loop\\prism_vs_stations_8Nearest_LWR_1991_2012.csv\"\n",
    "physical_file = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation_Temperature.csv\"\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "output_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\PRISM_GLB_Temperature\\ClimaticIndices-8Nearest\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY CSV DATA\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data (obs, prism_lwr5_val) ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(f\"df_data shape = {df_data.shape}\")\n",
    "print(\"Columns:\", df_data.columns.tolist())\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. MERGE WITH PHYSICAL FILE TO GET LAT/LON\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS – FULL ETCCDI SET FOR Tmin / Tmax\n",
    "###############################################################################\n",
    "def absolute_extremes(series, kind):\n",
    "    \"\"\"Return the single-day absolute extreme.\"\"\"\n",
    "    if kind == \"max\":\n",
    "        return series.max(skipna=True)\n",
    "    else:                       # \"min\"\n",
    "        return series.min(skipna=True)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# percentile thresholds (5-day moving window, baseline = 1991-2012)\n",
    "# -------------------------------------------------------------------------\n",
    "BASE_START, BASE_END = \"1991-01-01\", \"2012-12-31\"\n",
    "\n",
    "def _climatology_percentiles(s, p):\n",
    "    \"\"\"Return a Series (index = 1…366) of the p-th percentile.\"\"\"\n",
    "    # drop 29 Feb so every year has 365 days\n",
    "    s = s[~((s.index.month == 2) & (s.index.day == 29))]\n",
    "    df = pd.DataFrame({\"val\": s, \"doy\": s.index.dayofyear})\n",
    "    climo = []\n",
    "    for d in range(1, 366):\n",
    "        win = list(range(d-2, d+3))                       # ±2-day window\n",
    "        win = [(x-1) % 365 + 1 for x in win]              # wrap around ends\n",
    "        vals = df.loc[df[\"doy\"].isin(win), \"val\"]\n",
    "        climo.append(np.nanpercentile(vals, p) if len(vals) else np.nan)\n",
    "    return pd.Series(climo, index=range(1, 366), name=f\"p{p}\")\n",
    "\n",
    "def percentile_flags(s, perc_series, side):\n",
    "    \"\"\"Return Boolean Series: True where value is < or > percentile.\"\"\"\n",
    "    doy = s.index.dayofyear\n",
    "    thr = perc_series.reindex(doy).values\n",
    "    if side == \"low\":\n",
    "        return s < thr\n",
    "    else:\n",
    "        return s > thr\n",
    "\n",
    "def spell_length(bool_series, min_run=6):\n",
    "    \"\"\"Total # days in spells of ≥ min_run consecutive Trues.\"\"\"\n",
    "    is_true = bool_series.fillna(False).values\n",
    "    # identify run lengths\n",
    "    run_ends = np.where(np.diff(np.concatenate(([0], is_true, [0]))))[0]\n",
    "    lengths  = run_ends[1::2] - run_ends[::2]\n",
    "    return lengths[lengths >= min_run].sum()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# absolute-threshold counters\n",
    "# -------------------------------------------------------------------------\n",
    "def count_threshold(series, op, thr):\n",
    "    if op == \"<\":\n",
    "        return (series < thr).sum()\n",
    "    else:\n",
    "        return (series > thr).sum()\n",
    "\n",
    "###############################################################################\n",
    "# 5.  ANNUAL ETCCDI INDICES  –  FORMAT COMPATIBLE WITH THE SEASONAL SCRIPT\n",
    "#     • keeps:   TXx  TNn  TX90p  TN10p  FD  WSDI  CSDI\n",
    "#     • drops:   TR, ID, SU, TXn, TNx\n",
    "###############################################################################\n",
    "rows = []\n",
    "\n",
    "print(\"→ computing *annual* indices …\")\n",
    "for st_name, st_grp in df_data.groupby(\"station_name\"):\n",
    "\n",
    "    st_grp = st_grp.set_index(\"time\").sort_index()\n",
    "\n",
    "    # build daily Series ................................................................\n",
    "    obs_max = st_grp.loc[st_grp[\"var\"] == \"tmax\", \"obs\"].asfreq(\"D\")\n",
    "    obs_min = st_grp.loc[st_grp[\"var\"] == \"tmin\", \"obs\"].asfreq(\"D\")\n",
    "    prism_max = st_grp.loc[st_grp[\"var\"] == \"tmax\", \"prism_lwr5_val\"].asfreq(\"D\")\n",
    "    prism_min = st_grp.loc[st_grp[\"var\"] == \"tmin\", \"prism_lwr5_val\"].asfreq(\"D\")\n",
    "    if obs_max.empty:          # station has no data at all\n",
    "        continue\n",
    "\n",
    "    # ── fixed-year climatology (1991-2012, 5-day moving window) ─────────────-\n",
    "    p90_TX_obs = _climatology_percentiles(obs_max[BASE_START:BASE_END], 90)\n",
    "    p10_TN_obs = _climatology_percentiles(obs_min[BASE_START:BASE_END], 10)\n",
    "    p90_TX_prism = _climatology_percentiles(prism_max[BASE_START:BASE_END], 90)\n",
    "    p10_TN_prism = _climatology_percentiles(prism_min[BASE_START:BASE_END], 10)\n",
    "\n",
    "    # flags for the *full* record (faster than recomputing year-by-year)\n",
    "    flags = {\n",
    "        \"obs_TX90\": obs_max >\n",
    "                     p90_TX_obs.reindex(obs_max.index.dayofyear).values,\n",
    "        \"prism_TX90\": prism_max >\n",
    "                     p90_TX_prism.reindex(prism_max.index.dayofyear).values,\n",
    "        \"obs_TN10\": obs_min <\n",
    "                     p10_TN_obs.reindex(obs_min.index.dayofyear).values,\n",
    "        \"prism_TN10\": prism_min <\n",
    "                     p10_TN_prism.reindex(prism_min.index.dayofyear).values,\n",
    "    }\n",
    "\n",
    "    # ── iterate over years (December belongs to the following DJF year) ──────\n",
    "    years = np.unique(obs_max.index.year)\n",
    "    for yr in years:\n",
    "        mask = obs_max.index.year == yr\n",
    "        if mask.sum() < 200:         # at least ~55 % of a year\n",
    "            continue\n",
    "\n",
    "        def _sel(s):          # helper to slice one year\n",
    "            return s[s.index.year == yr]\n",
    "\n",
    "        # intensity ....................................................................\n",
    "        TXx_obs = _sel(obs_max).max()\n",
    "        TXx_prism = _sel(prism_max).max()\n",
    "        TNn_obs = _sel(obs_min).min()\n",
    "        TNn_prism = _sel(prism_min).min()\n",
    "\n",
    "        # percentile frequencies (percentage of days)\n",
    "        TX90p_obs = flags[\"obs_TX90\"][mask].mean() * 100.0\n",
    "        TX90p_prism = flags[\"prism_TX90\"][mask].mean() * 100.0\n",
    "        TN10p_obs = flags[\"obs_TN10\"][mask].mean() * 100.0\n",
    "        TN10p_prism = flags[\"prism_TN10\"][mask].mean() * 100.0\n",
    "\n",
    "        # spell duration (≥6 consecutive days)\n",
    "        WSDI_obs = spell_length(flags[\"obs_TX90\"][mask], min_run=6)\n",
    "        WSDI_prism = spell_length(flags[\"prism_TX90\"][mask], min_run=6)\n",
    "        CSDI_obs = spell_length(flags[\"obs_TN10\"][mask], min_run=6)\n",
    "        CSDI_prism = spell_length(flags[\"prism_TN10\"][mask], min_run=6)\n",
    "\n",
    "        # absolute‐threshold count\n",
    "        FD_obs = (_sel(obs_min) < 0).sum()\n",
    "        FD_prism = (_sel(prism_min) < 0).sum()\n",
    "\n",
    "        # helper for ratios (avoid /0)\n",
    "        ratio = lambda o, e: np.nan if (o == 0 or np.isnan(o)) else e / o\n",
    "\n",
    "        rows.append(dict(\n",
    "            station_name=st_name, year=yr,\n",
    "            TXx_obs=TXx_obs,   TXx_prism=TXx_prism,   TXx_ratio=ratio(TXx_obs, TXx_prism),\n",
    "            TNn_obs=TNn_obs,   TNn_prism=TNn_prism,   TNn_ratio=ratio(TNn_obs, TNn_prism),\n",
    "            TX90p_obs=TX90p_obs, TX90p_prism=TX90p_prism,\n",
    "            TX90p_ratio=ratio(TX90p_obs, TX90p_prism),\n",
    "            TN10p_obs=TN10p_obs, TN10p_prism=TN10p_prism,\n",
    "            TN10p_ratio=ratio(TN10p_obs, TN10p_prism),\n",
    "            FD_obs=FD_obs,     FD_prism=FD_prism,     FD_ratio=ratio(FD_obs, FD_prism),\n",
    "            WSDI_obs=WSDI_obs, WSDI_prism=WSDI_prism,\n",
    "            WSDI_ratio=ratio(WSDI_obs, WSDI_prism),\n",
    "            CSDI_obs=CSDI_obs, CSDI_prism=CSDI_prism,\n",
    "            CSDI_ratio=ratio(CSDI_obs, CSDI_prism)\n",
    "        ))\n",
    "\n",
    "df_yr = (pd.DataFrame(rows)\n",
    "         .merge(df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "                on=\"station_name\", how=\"left\")\n",
    "         .sort_values([\"station_name\", \"year\"]))\n",
    "\n",
    "###############################################################################\n",
    "# 6.  SAVE OUTPUT  – ONE PARQUET ( + XLSX )  +  OPTIONAL per-index sheets\n",
    "###############################################################################\n",
    "annual_pq  = os.path.join(output_dir, \"Indices_Annual.parquet\")\n",
    "annual_xls = annual_pq.replace(\".parquet\", \".xlsx\")\n",
    "\n",
    "df_yr.to_parquet(annual_pq, index=False)\n",
    "df_yr.to_excel  (annual_xls, index=False)\n",
    "print(f\"✓ Annual indices saved → {annual_pq}\")\n",
    "print(f\"✓ …and also saved as   → {annual_xls}\")\n",
    "\n",
    "# OPTIONAL: write one Excel file per index in the familiar “wide” format\n",
    "# ---------------------------------------------------------------------------\n",
    "index_roots = [\"TXx\", \"TNn\", \"TX90p\", \"TN10p\", \"FD\", \"WSDI\", \"CSDI\"]\n",
    "def _wide(idx_root: str) -> pd.DataFrame:\n",
    "    return (df_yr\n",
    "            .pivot_table(index=\"station_name\",\n",
    "                         values=[f\"{idx_root}_obs\", f\"{idx_root}_prism\",\n",
    "                                 f\"{idx_root}_ratio\"])\n",
    "            .reset_index()\n",
    "            .merge(df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "                   on=\"station_name\", how=\"left\"))\n",
    "\n",
    "print(\"\\n(optional) individual workbooks …\")\n",
    "for idx in index_roots:\n",
    "    w = _wide(idx)\n",
    "    fp = os.path.join(output_dir, f\"{idx}.xlsx\")\n",
    "    w.to_excel(fp, index=False)\n",
    "    print(\"  •\", os.path.basename(fp))\n",
    "\n",
    "print(\"\\n✅  Annual-index workflow finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d76be-2c0f-4f7f-9113-5ae12a354475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal stratification of precipitation climatic indices for PRISM for having the seasonal indices\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIGURATION\n",
    "###############################################################################\n",
    "csv_file      = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\PRISM_GLB_Precipitation\\daily_loop3\\prism_vs_stations_8Nearest_LWR_1991_2012.csv\"\n",
    "physical_file = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\Split_by_country\\filtered_stations_US.csv\"\n",
    "output_dir    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\PRISM_GLB_Precipitation\\ClimaticIndices-Seasonal-3\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY DATA & ADD TEMPORAL FIELDS\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# Standardize station_name\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Add month (1..12) and season (DJF, MAM, JJA, SON)\n",
    "df_data[\"month\"] = df_data[\"time\"].dt.month\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"DJF\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"MAM\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"JJA\"\n",
    "    else:\n",
    "        return \"SON\"\n",
    "\n",
    "df_data[\"season\"] = df_data[\"month\"].apply(get_season)\n",
    "\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. LOAD PHYSICAL FILE & MERGE COORDINATES\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "###############################################################################\n",
    "def calc_rx1day(series):\n",
    "    \"\"\"Max 1-day precipitation.\"\"\"\n",
    "    return series.max(skipna=True)\n",
    "\n",
    "def calc_rx5day(series):\n",
    "    \"\"\"Max 5-day running sum.\"\"\"\n",
    "    return series.rolling(5, min_periods=1).sum().max(skipna=True)\n",
    "\n",
    "def calc_cdd(series, dry_threshold=1.0):\n",
    "    \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "    is_dry = series < dry_threshold\n",
    "    max_run, current_run = 0, 0\n",
    "    for val in is_dry:\n",
    "        if val:\n",
    "            current_run += 1\n",
    "            max_run = max(max_run, current_run)\n",
    "        else:\n",
    "            current_run = 0\n",
    "    return max_run\n",
    "\n",
    "def calc_cwd(series, wet_threshold=1.0):\n",
    "    \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "    is_wet = series >= wet_threshold\n",
    "    max_run, current_run = 0, 0\n",
    "    for val in is_wet:\n",
    "        if val:\n",
    "            current_run += 1\n",
    "            max_run = max(max_run, current_run)\n",
    "        else:\n",
    "            current_run = 0\n",
    "    return max_run\n",
    "\n",
    "def calc_r95p_r99p(series, percentiles=(95,99)):\n",
    "    \"\"\"\n",
    "    r95amt, r95pct, r99amt, r99pct:\n",
    "    - r95amt = sum of daily prcp above 95th percentile\n",
    "    - r95pct = (r95amt / total) * 100\n",
    "    - similarly for 99th percentile\n",
    "    \"\"\"\n",
    "    wet = series[series >= 1.0]\n",
    "    if len(wet) < 5:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    p95 = np.percentile(wet, percentiles[0])\n",
    "    p99 = np.percentile(wet, percentiles[1])\n",
    "    r95_amt = wet[wet > p95].sum()\n",
    "    r99_amt = wet[wet > p99].sum()\n",
    "    total = series.sum(skipna=True)\n",
    "    r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "    r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "    return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "\n",
    "def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "    \"\"\"\n",
    "    wetdays = #days >= wet_thr\n",
    "    drydays = #days < dry_thr\n",
    "    \"\"\"\n",
    "    return (series >= wet_thr).sum(), (series < dry_thr).sum()\n",
    "\n",
    "###############################################################################\n",
    "# 5. FUNCTION TO COMPUTE INDICES FOR A GROUP (MONTHLY or SEASONAL)\n",
    "###############################################################################\n",
    "def compute_indices(df_group):\n",
    "    \"\"\"\n",
    "    For a subset of daily data (e.g. station+month, or station+season),\n",
    "    compute the climate indices for Obs vs PRISM, plus ratio columns.\n",
    "    \"\"\"\n",
    "    obs_series = df_group[\"obs\"].dropna().reset_index(drop=True)\n",
    "    prism_series = df_group[\"prism_lwr8_val\"].dropna().reset_index(drop=True)\n",
    "    if len(obs_series) == 0 or len(prism_series) == 0:\n",
    "        return None\n",
    "    \n",
    "    res = {}\n",
    "    # Rx1day / Rx5day\n",
    "    res[\"rx1day_obs\"] = calc_rx1day(obs_series)\n",
    "    res[\"rx1day_prism\"] = calc_rx1day(prism_series)\n",
    "    res[\"rx5day_obs\"] = calc_rx5day(obs_series)\n",
    "    res[\"rx5day_prism\"] = calc_rx5day(prism_series)\n",
    "    \n",
    "    # CDD / CWD\n",
    "    res[\"cdd_obs\"] = calc_cdd(obs_series)\n",
    "    res[\"cdd_prism\"] = calc_cdd(prism_series)\n",
    "    res[\"cwd_obs\"] = calc_cwd(obs_series)\n",
    "    res[\"cwd_prism\"] = calc_cwd(prism_series)\n",
    "    \n",
    "    # R95 / R99\n",
    "    r95_obs = calc_r95p_r99p(obs_series)\n",
    "    r95_prism = calc_r95p_r99p(prism_series)\n",
    "    res[\"r95amt_obs\"] = r95_obs[0]\n",
    "    res[\"r95pct_obs\"] = r95_obs[1]\n",
    "    res[\"r95amt_prism\"] = r95_prism[0]\n",
    "    res[\"r95pct_prism\"] = r95_prism[1]\n",
    "    res[\"r99amt_obs\"] = r95_obs[2]\n",
    "    res[\"r99pct_obs\"] = r95_obs[3]\n",
    "    res[\"r99amt_prism\"] = r95_prism[2]\n",
    "    res[\"r99pct_prism\"] = r95_prism[3]\n",
    "    \n",
    "    # Wet / Dry days\n",
    "    wet_obs, dry_obs = calc_wetdays_drydays(obs_series)\n",
    "    wet_prism, dry_prism = calc_wetdays_drydays(prism_series)\n",
    "    res[\"wetdays_obs\"] = wet_obs\n",
    "    res[\"wetdays_prism\"] = wet_prism\n",
    "    res[\"drydays_obs\"] = dry_obs\n",
    "    res[\"drydays_prism\"] = dry_prism\n",
    "    \n",
    "    # Ratio columns: prism/OBS if obs != 0\n",
    "    if res[\"rx1day_obs\"]:\n",
    "        res[\"rx1day_ratio\"] = res[\"rx1day_prism\"] / res[\"rx1day_obs\"]\n",
    "    if res[\"rx5day_obs\"]:\n",
    "        res[\"rx5day_ratio\"] = res[\"rx5day_prism\"] / res[\"rx5day_obs\"]\n",
    "    if res[\"cdd_obs\"]:\n",
    "        res[\"cdd_ratio\"] = res[\"cdd_prism\"] / res[\"cdd_obs\"]\n",
    "    if res[\"cwd_obs\"]:\n",
    "        res[\"cwd_ratio\"] = res[\"cwd_prism\"] / res[\"cwd_obs\"]\n",
    "    if res[\"r95amt_obs\"]:\n",
    "        res[\"r95amt_ratio\"] = res[\"r95amt_prism\"] / res[\"r95amt_obs\"]\n",
    "    if res[\"r95pct_obs\"]:\n",
    "        res[\"r95pct_ratio\"] = res[\"r95pct_prism\"] / res[\"r95pct_obs\"]\n",
    "    if res[\"r99amt_obs\"]:\n",
    "        res[\"r99amt_ratio\"] = res[\"r99amt_prism\"] / res[\"r99amt_obs\"]\n",
    "    if res[\"r99pct_obs\"]:\n",
    "        res[\"r99pct_ratio\"] = res[\"r99pct_prism\"] / res[\"r99pct_obs\"]\n",
    "    if res[\"wetdays_obs\"]:\n",
    "        res[\"wetdays_ratio\"] = res[\"wetdays_prism\"] / res[\"wetdays_obs\"]\n",
    "    if res[\"drydays_obs\"]:\n",
    "        res[\"drydays_ratio\"] = res[\"drydays_prism\"] / res[\"drydays_obs\"]\n",
    "    \n",
    "    return res\n",
    "\n",
    "###############################################################################\n",
    "# 6. MONTHLY INDICES\n",
    "###############################################################################\n",
    "monthly_results = []\n",
    "group_month = df_data.groupby([\"station_name\", \"month\"])\n",
    "for (st_name, mon), group in group_month:\n",
    "    indices = compute_indices(group)\n",
    "    if indices is None:\n",
    "        continue\n",
    "    indices[\"station_name\"] = st_name\n",
    "    indices[\"month\"] = mon\n",
    "    monthly_results.append(indices)\n",
    "\n",
    "df_monthly = pd.DataFrame(monthly_results)\n",
    "df_monthly = pd.merge(\n",
    "    df_monthly,\n",
    "    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_monthly = df_monthly.sort_values([\"station_name\", \"month\"])\n",
    "monthly_out = os.path.join(output_dir, \"Indices_Monthly.xlsx\")\n",
    "df_monthly.to_excel(monthly_out, index=False)\n",
    "print(\"Monthly indices saved =>\", monthly_out)\n",
    "\n",
    "###############################################################################\n",
    "# 7. SEASONAL INDICES\n",
    "###############################################################################\n",
    "seasonal_results = []\n",
    "group_season = df_data.groupby([\"station_name\", \"season\"])\n",
    "for (st_name, seas), group in group_season:\n",
    "    indices = compute_indices(group)\n",
    "    if indices is None:\n",
    "        continue\n",
    "    indices[\"station_name\"] = st_name\n",
    "    indices[\"season\"] = seas\n",
    "    seasonal_results.append(indices)\n",
    "\n",
    "df_seasonal = pd.DataFrame(seasonal_results)\n",
    "df_seasonal = pd.merge(\n",
    "    df_seasonal,\n",
    "    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_seasonal = df_seasonal.sort_values([\"station_name\", \"season\"])\n",
    "seasonal_out = os.path.join(output_dir, \"Indices_Seasonal.xlsx\")\n",
    "df_seasonal.to_excel(seasonal_out, index=False)\n",
    "print(\"Seasonal indices saved =>\", seasonal_out)\n",
    "\n",
    "###############################################################################\n",
    "# 8. DONE\n",
    "###############################################################################\n",
    "print(\"\\nAll monthly and seasonal indices have been saved. (No extreme-event stratification.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f9e22-39e5-4b71-9d5d-4ebb2cdd03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DJF for PRISM\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import Point\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIG & PATHS\n",
    "###############################################################################\n",
    "indices_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\PRISM_GLB_Precipitation\\ClimaticIndices-Seasonal-3\"\n",
    "seasonal_file = os.path.join(indices_dir, \"Indices_Seasonal.xlsx\")  # single file\n",
    "output_plots  = os.path.join(indices_dir, \"AnalysisPlots_DJF\")\n",
    "os.makedirs(output_plots, exist_ok=True)\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "# Indices in your seasonal file\n",
    "index_list = [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"r95p\",\"r99p\",\"wetdays\",\"drydays\"]\n",
    "\n",
    "# For summary stats, define how to find obs vs. emd columns\n",
    "index_columns = {\n",
    "    \"rx1day\":  (\"rx1day_obs\",  \"rx1day_prism\"),\n",
    "    \"rx5day\":  (\"rx5day_obs\",  \"rx5day_prism\"),\n",
    "    \"cdd\":     (\"cdd_obs\",     \"cdd_prism\"),\n",
    "    \"cwd\":     (\"cwd_obs\",     \"cwd_prism\"),\n",
    "    \"r95p\":    ((\"r95amt_obs\",\"r95pct_obs\"), (\"r95amt_prism\",\"r95pct_prism\")),\n",
    "    \"r99p\":    ((\"r99amt_obs\",\"r99pct_obs\"), (\"r99amt_prism\",\"r99pct_prism\")),\n",
    "    \"wetdays\": (\"wetdays_obs\",\"wetdays_prism\"),\n",
    "    \"drydays\": (\"drydays_obs\",\"drydays_prism\"),\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD SEASONAL FILE & FILTER TO DJF\n",
    "###############################################################################\n",
    "df_season = pd.read_excel(seasonal_file)\n",
    "print(\"Loaded =>\", seasonal_file, \"| shape =\", df_season.shape)\n",
    "\n",
    "# Filter to DJF\n",
    "df_season = df_season[df_season[\"season\"]==\"DJF\"].copy()\n",
    "df_season = df_season.dropna(subset=[\"lat\",\"lon\"])  # ensure lat/lon exist\n",
    "print(\"After filtering to DJF => shape =\", df_season.shape)\n",
    "\n",
    "mdf = df_season.reset_index(drop=True)\n",
    "master_xlsx = os.path.join(output_plots, \"MasterTable_Seasonal_DJF.xlsx\")\n",
    "mdf.to_excel(master_xlsx, index=False)\n",
    "print(f\"\\n(A) Master table (DJF) saved => {master_xlsx}\")\n",
    "print(\"Columns:\", mdf.columns.tolist())\n",
    "\n",
    "###############################################################################\n",
    "# 3. SUMMARY TABLE (MBE, RMSE, STD, CC, d) for DJF\n",
    "###############################################################################\n",
    "def index_of_agreement(obs, model):\n",
    "    obs_mean = np.mean(obs)\n",
    "    num = np.sum((model - obs)**2)\n",
    "    den = np.sum((abs(model - obs_mean) + abs(obs - obs_mean))**2)\n",
    "    if den == 0:\n",
    "        return np.nan\n",
    "    return 1 - num/den\n",
    "\n",
    "def rmse(a, b):\n",
    "    return np.sqrt(np.mean((a-b)**2))\n",
    "\n",
    "def std_of_residuals(a, b):\n",
    "    return np.std(a-b, ddof=1)\n",
    "\n",
    "def mean_bias_error(a, b):\n",
    "    return np.mean(b-a)\n",
    "\n",
    "summary_rows = []\n",
    "for idx_name in index_list:\n",
    "    obs_cols = index_columns[idx_name][0]\n",
    "    prism_cols = index_columns[idx_name][1]\n",
    "\n",
    "    if isinstance(obs_cols, tuple):\n",
    "        # multiple columns\n",
    "        for oc, ec in zip(obs_cols, prism_cols):\n",
    "            valid = mdf[[oc, ec]].dropna()\n",
    "            if len(valid) < 2:\n",
    "                continue\n",
    "            obs_vals = valid[oc].values\n",
    "            prism_vals = valid[ec].values\n",
    "            MB  = mean_bias_error(obs_vals, prism_vals)\n",
    "            RM  = rmse(obs_vals, prism_vals)\n",
    "            SR  = std_of_residuals(obs_vals, prism_vals)\n",
    "            CC  = pearsonr(obs_vals, prism_vals)[0] if len(obs_vals) > 1 else np.nan\n",
    "            dd  = index_of_agreement(obs_vals, prism_vals)\n",
    "            idx_label = f\"{idx_name}_{oc.replace('_obs','')}\"\n",
    "            summary_rows.append({\n",
    "                \"Index\": idx_label,\n",
    "                \"Count\": len(valid),\n",
    "                \"MBE\": MB,\n",
    "                \"RMSE\": RM,\n",
    "                \"STDres\": SR,\n",
    "                \"CC\": CC,\n",
    "                \"d\": dd,\n",
    "            })\n",
    "    else:\n",
    "        oc = obs_cols\n",
    "        ec = prism_cols\n",
    "        valid = mdf[[oc, ec]].dropna()\n",
    "        if len(valid) < 2:\n",
    "            continue\n",
    "        obs_vals = valid[oc].values\n",
    "        prism_vals = valid[ec].values\n",
    "        MB = mean_bias_error(obs_vals, prism_vals)\n",
    "        RM = rmse(obs_vals, prism_vals)\n",
    "        SR = std_of_residuals(obs_vals, prism_vals)\n",
    "        CC = pearsonr(obs_vals, prism_vals)[0] if len(obs_vals) > 1 else np.nan\n",
    "        dd = index_of_agreement(obs_vals, prism_vals)\n",
    "        summary_rows.append({\n",
    "            \"Index\": idx_name,\n",
    "            \"Count\": len(valid),\n",
    "            \"MBE\": MB,\n",
    "            \"RMSE\": RM,\n",
    "            \"STDres\": SR,\n",
    "            \"CC\": CC,\n",
    "            \"d\": dd,\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_cols = [\"Index\",\"Count\",\"MBE\",\"RMSE\",\"STDres\",\"CC\",\"d\"]\n",
    "summary_df = summary_df[summary_cols]\n",
    "summary_xlsx = os.path.join(output_plots, \"SummaryTable_Extremes_DJF.xlsx\")\n",
    "summary_df.to_excel(summary_xlsx, index=False)\n",
    "print(f\"(B) Summary Table (DJF) => {summary_xlsx}\\n{summary_df}\")\n",
    "\n",
    "###############################################################################\n",
    "# 4. MAPPING: Combine Observed, prism, Ratio in One Figure\n",
    "###############################################################################\n",
    "gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "\n",
    "def add_basin_lakes(ax):\n",
    "    #ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    for geom in gdf_basin.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(),\n",
    "                          facecolor='none', edgecolor='black', linewidth=1)\n",
    "    for geom in gdf_lakes.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(),\n",
    "                          facecolor='none', edgecolor='cyan', linewidth=1)\n",
    "\n",
    "def plot_map_triptych(df, obs_col, prism_col, ratio_col, idx_name, out_png):\n",
    "    \"\"\"\n",
    "    Creates a single figure with 3 subplots (side-by-side):\n",
    "      1) Observed\n",
    "      2) prism\n",
    "      3) Ratio (MERRA2/OBS)\n",
    "    Each subplot has a colorbar, a 90th-percentile hotspot circle, etc.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6),\n",
    "                             subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "\n",
    "    # We'll define a small helper to do each subplot\n",
    "    def scatter_map(ax, value_col, title):\n",
    "        ax.set_extent([-95.5, -72, 38.5, 52.5])\n",
    "        add_basin_lakes(ax)\n",
    "        sc = ax.scatter(df[\"lon\"], df[\"lat\"], c=df[value_col], cmap=\"viridis\",\n",
    "                        s=60, transform=ccrs.PlateCarree(), edgecolor=\"k\", zorder=10)\n",
    "        cb = plt.colorbar(sc, ax=ax, shrink=0.8)\n",
    "        cb.set_label(value_col)\n",
    "\n",
    "        # Hotspots => top 10%\n",
    "        vals = df[value_col].dropna().values\n",
    "        if len(vals) > 0:\n",
    "            thr = np.percentile(vals, 90)\n",
    "            is_hot = df[value_col]>=thr\n",
    "            ax.scatter(df.loc[is_hot,\"lon\"], df.loc[is_hot,\"lat\"],\n",
    "                       marker='o', facecolors='none', edgecolors='red', s=80,\n",
    "                       transform=ccrs.PlateCarree(), zorder=11,\n",
    "                       label=f\"Hotspot >= {thr:.2f}\")\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        gl = ax.gridlines(draw_labels=True, linestyle='--', color='gray')\n",
    "        gl.right_labels = False\n",
    "        gl.top_labels   = False\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    scatter_map(axes[0], obs_col,  f\"{idx_name} Observed (DJF)\")\n",
    "    scatter_map(axes[1], prism_col,  f\"{idx_name} PRISM (DJF)\")\n",
    "    scatter_map(axes[2], ratio_col,f\"{idx_name} (PRISM/OBS) (DJF)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved =>\", out_png)\n",
    "\n",
    "def get_map_cols(idx_name):\n",
    "    if idx_name in [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"drydays\"]:\n",
    "        obs  = f\"{idx_name}_obs\"\n",
    "        prism  = f\"{idx_name}_prism\"\n",
    "        ratio= f\"{idx_name}_ratio\"\n",
    "        return obs, prism, ratio\n",
    "    elif idx_name == \"wetdays\":\n",
    "        obs  = \"wetdays_obs\"\n",
    "        prism  = \"wetdays_prism\"\n",
    "        ratio= \"wetdays_ratio\"\n",
    "        return obs, prism, ratio\n",
    "    elif idx_name == \"r95p\":\n",
    "        obs  = \"r95amt_obs\"\n",
    "        prism  = \"r95amt_prism\"\n",
    "        ratio= \"r95amt_ratio\"\n",
    "        return obs, prism, ratio\n",
    "    elif idx_name == \"r99p\":\n",
    "        obs  = \"r99amt_obs\"\n",
    "        prism  = \"r99amt_prism\"\n",
    "        ratio= \"r99amt_ratio\"\n",
    "        return obs, prism, ratio\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "for idx_name in index_list:\n",
    "    obs_col, prism_col, ratio_col = get_map_cols(idx_name)\n",
    "    if obs_col is None:\n",
    "        continue\n",
    "\n",
    "    needed_cols = [obs_col, prism_col, ratio_col, \"lat\", \"lon\"]\n",
    "    if not all(c in mdf.columns for c in needed_cols):\n",
    "        print(f\"Skipping map for {idx_name} - missing columns.\")\n",
    "        continue\n",
    "\n",
    "    subdf = mdf.dropna(subset=[\"lat\",\"lon\"]).copy()\n",
    "    out_png = os.path.join(output_plots, f\"DJF_{idx_name}_MAP_3panel.png\")\n",
    "    plot_map_triptych(subdf, obs_col, prism_col, ratio_col, idx_name, out_png)\n",
    "\n",
    "###############################################################################\n",
    "# 5. DISTRIBUTION & BOX/CDF/Scatter in One Figure\n",
    "###############################################################################\n",
    "def plot_distribution_triptych(df, obs_col, prism_col, label, out_png):\n",
    "    \"\"\"\n",
    "    Creates a single figure with 3 subplots side-by-side:\n",
    "      1) Boxplot\n",
    "      2) CDF\n",
    "      3) Scatter\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,6))\n",
    "\n",
    "    # A) Boxplot\n",
    "    ax_box = axes[0]\n",
    "    data = pd.DataFrame({\"Obs\": df[obs_col], \"prism\": df[prism_col]}).melt(\n",
    "        var_name=\"Dataset\", value_name=label\n",
    "    )\n",
    "    sns.boxplot(data=data, x=\"Dataset\", y=label, ax=ax_box)\n",
    "    ax_box.set_title(f\"Boxplot: {label} (DJF)\")\n",
    "\n",
    "    # B) CDF\n",
    "    ax_cdf = axes[1]\n",
    "    obs_vals = df[obs_col].dropna()\n",
    "    prism_vals = df[prism_col].dropna()\n",
    "\n",
    "    def ecdf(x):\n",
    "        xs = np.sort(x)\n",
    "        ys = np.arange(1, len(xs)+1)/len(xs)\n",
    "        return xs, ys\n",
    "\n",
    "    if len(obs_vals)>=2 and len(prism_vals)>=2:\n",
    "        xs_o, ys_o = ecdf(obs_vals)\n",
    "        xs_e, ys_e = ecdf(prism_vals)\n",
    "        ax_cdf.plot(xs_o, ys_o, label=\"Obs\")\n",
    "        ax_cdf.plot(xs_e, ys_e, label=\"PRISM\")\n",
    "        ax_cdf.set_title(f\"CDF of {label} (DJF)\")\n",
    "        ax_cdf.set_xlabel(label)\n",
    "        ax_cdf.set_ylabel(\"Probability\")\n",
    "        ax_cdf.legend()\n",
    "    else:\n",
    "        ax_cdf.set_title(f\"CDF: not enough data ({label})\")\n",
    "\n",
    "    # C) Scatter\n",
    "    ax_scat = axes[2]\n",
    "    valid = df[[obs_col, prism_col]].dropna()\n",
    "    if len(valid)>=2:\n",
    "        x = valid[obs_col]\n",
    "        y = valid[prism_col]\n",
    "        cc, _ = pearsonr(x, y)\n",
    "        ax_scat.scatter(x, y, edgecolors='k', alpha=0.7)\n",
    "        mn, mx = np.nanmin([x.min(), y.min()]), np.nanmax([x.max(), y.max()])\n",
    "        ax_scat.plot([mn, mx],[mn, mx],'r--')\n",
    "        ax_scat.set_xlabel(f\"Obs {label} (DJF)\")\n",
    "        ax_scat.set_ylabel(f\"PRISM {label} (DJF)\")\n",
    "        ax_scat.set_title(f\"{label} (Corr={cc:.2f}, DJF)\")\n",
    "    else:\n",
    "        ax_scat.set_title(f\"Scatter: not enough data ({label})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved =>\", out_png)\n",
    "\n",
    "for idx_name in index_list:\n",
    "    # figure out obs, emd columns\n",
    "    if idx_name in [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"drydays\"]:\n",
    "        obs_col  = f\"{idx_name}_obs\"\n",
    "        prism_col  = f\"{idx_name}_prism\"\n",
    "    elif idx_name == \"wetdays\":\n",
    "        obs_col  = \"wetdays_obs\"\n",
    "        prism_col  = \"wetdays_prism\"\n",
    "    elif idx_name == \"r95p\":\n",
    "        obs_col  = \"r95amt_obs\"\n",
    "        prism_col  = \"r95amt_prism\"\n",
    "    elif idx_name == \"r99p\":\n",
    "        obs_col  = \"r99amt_obs\"\n",
    "        prism_col  = \"r99amt_prism\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    if obs_col not in mdf.columns or prism_col not in mdf.columns:\n",
    "        print(f\"Skipping distribution for {idx_name} - missing columns.\")\n",
    "        continue\n",
    "\n",
    "    subdf = mdf[[obs_col, prism_col]].dropna()\n",
    "    if len(subdf)<2:\n",
    "        print(f\"Skipping distribution for {idx_name} - not enough data.\")\n",
    "        continue\n",
    "\n",
    "    out_3panel = os.path.join(output_plots, f\"DJF_{idx_name}_Distribution_3panel.png\")\n",
    "    plot_distribution_triptych(subdf, obs_col, prism_col, idx_name, out_3panel)\n",
    "\n",
    "###############################################################################\n",
    "# 6. DONE\n",
    "###############################################################################\n",
    "print(\"\\nAll DJF steps completed! See outputs in:\", output_plots)\n",
    "\n",
    "## For the other seasons, just change any DJF to JJA, MAM, or SON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e39e8-d90e-4b91-82ea-9e329196b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "                 #########################                                           ##############################\n",
    "                 #########################                  EMDNA                    ##############################\n",
    "                 #########################                                           ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85e784-8384-488b-88ca-fc20e7f22fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the climatic indices for EMDNA (prcp)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIG          \n",
    "###############################################################################\n",
    "root_dir = (r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\"\n",
    "            r\"\\Ensemble files\\EMDNA_GLB_Precipitation\")\n",
    "\n",
    "# the 10 ensemble sub-folders we need to process\n",
    "ENSEMBLES = [1, 11, 21, 31, 41, 51, 61, 71, 81, 91]\n",
    "\n",
    "# static reference files (same for every ensemble)\n",
    "physical_file  = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv\"\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# start looping over ensembles – EVERYTHING that follows now belongs inside\n",
    "# this for-loop\n",
    "# ---------------------------------------------------------------------------\n",
    "for ENS in ENSEMBLES:\n",
    "    print(\"\\n\" + \"=\"*84)\n",
    "    print(f\"⧉  Processing ensemble {ENS:03d}  ⧉\")\n",
    "    print(\"=\"*84)\n",
    "\n",
    "    ens_dir   = os.path.join(root_dir, str(ENS))\n",
    "    csv_file  = os.path.join(ens_dir, \"daily_loop\",\n",
    "                             f\"emdna_vs_stations_25km_LWR_1991_2012_prcp_{ENS:03d}.csv\")\n",
    "    output_dir = os.path.join(ens_dir, \"ClimaticIndices-25KM\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY CSV DATA\n",
    "###############################################################################\n",
    "    if not os.path.isfile(csv_file):\n",
    "        print(f\"   ⚠  Daily CSV not found, skipping ensemble {ENS:03d}\")\n",
    "        continue\n",
    "    \n",
    "    print(\"Loading daily CSV data …\")\n",
    "    df_data = pd.read_csv(csv_file)\n",
    "    df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "    \n",
    "    # standardise station names\n",
    "    df_data[\"station_name\"] = (\n",
    "        df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "    )\n",
    "    \n",
    "    # keep only precipitation rows (defensive; file *should* contain just prcp)\n",
    "    if \"var\" in df_data.columns:\n",
    "        df_data = df_data[df_data[\"var\"] == \"prcp\"].copy()\n",
    "    \n",
    "    # unify column name expected later in the script\n",
    "    #df_data = df_data.rename(columns={\"emdna_lwr25_val\": \"emdna_val\"})\n",
    "    \n",
    "    print(f\"df_data shape = {df_data.shape}\")\n",
    "    print(\"Columns:\", df_data.columns.tolist())\n",
    "    print(\"Time range:\", df_data['time'].min(), \"to\", df_data['time'].max())\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 3. (OPTIONAL) MERGE WITH PHYSICAL FILE TO GET LAT/LON\n",
    "    ###############################################################################\n",
    "    df_phys = pd.read_csv(physical_file)\n",
    "    df_phys = df_phys.rename(columns={\n",
    "        \"NAME\": \"station_name\",\n",
    "        \"LATITUDE\": \"lat\",\n",
    "        \"LONGITUDE\": \"lon\",\n",
    "        \"Elevation\": \"elev\"\n",
    "    })\n",
    "    # unify station_name\n",
    "    df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "    \n",
    "    # We'll merge lat/lon AFTER computing the indices, so each final row has lat/lon.\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "    ###############################################################################\n",
    "    def calc_rx1day(series):\n",
    "        \"\"\"Max 1-day precipitation.\"\"\"\n",
    "        return series.max(skipna=True)\n",
    "    \n",
    "    def calc_rx5day(series):\n",
    "        \"\"\"Max 5-day running sum.\"\"\"\n",
    "        roll_5 = series.rolling(5, min_periods=1).sum()\n",
    "        return roll_5.max(skipna=True)\n",
    "    \n",
    "    def calc_cdd(series, dry_threshold=1.0):\n",
    "        \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "        is_dry = series < dry_threshold\n",
    "        maxr, curr = 0, 0\n",
    "        for val in is_dry:\n",
    "            if val:\n",
    "                curr += 1\n",
    "                maxr = max(maxr, curr)\n",
    "            else:\n",
    "                curr = 0\n",
    "        return maxr\n",
    "    \n",
    "    def calc_cwd(series, wet_threshold=1.0):\n",
    "        \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "        is_wet = series >= wet_threshold\n",
    "        maxr, curr = 0, 0\n",
    "        for val in is_wet:\n",
    "            if val:\n",
    "                curr += 1\n",
    "                maxr = max(maxr, curr)\n",
    "            else:\n",
    "                curr = 0\n",
    "        return maxr\n",
    "    \n",
    "    def calc_r95p_r99p(series, percentile=(95,99)):\n",
    "        \"\"\"R95p, R99p TOT in mm, plus percentage of total.\"\"\"\n",
    "        # only wet days >=1 mm for percentile\n",
    "        wet = series[series >= 1.0]\n",
    "        if len(wet) < 5:\n",
    "            return np.nan, np.nan, np.nan, np.nan\n",
    "        p95 = np.percentile(wet, percentile[0])\n",
    "        p99 = np.percentile(wet, percentile[1])\n",
    "        r95_amt = wet[wet > p95].sum()\n",
    "        r99_amt = wet[wet > p99].sum()\n",
    "        total   = series.sum(skipna=True)\n",
    "        r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "        r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "        return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "    \n",
    "    def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "        \"\"\"Count #wet days >=5 mm, #dry days <1 mm.\"\"\"\n",
    "        w = (series >= wet_thr).sum()\n",
    "        d = (series <  dry_thr).sum()\n",
    "        return w, d\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 5. COMPUTE INDICES FOR EACH STATION\n",
    "    ###############################################################################\n",
    "    # We'll store results in lists of dicts, then convert to DataFrame => Excel.\n",
    "    rx1_list, rx5_list = [], []\n",
    "    cdd_list, cwd_list = [], []\n",
    "    r95_list, r99_list = [], []\n",
    "    wet_list, dry_list = [], []\n",
    "    \n",
    "    print(\"Computing indices for each station...\")\n",
    "    grouped = df_data.groupby(\"station_name\", as_index=False)\n",
    "    \n",
    "    for st_name, grp in grouped:\n",
    "        # Sort by time (just in case)\n",
    "        grp = grp.sort_values(\"time\")\n",
    "        # daily obs/emd\n",
    "        obs_series = grp[\"obs\"].dropna().reset_index(drop=True)\n",
    "        emd_series = grp[\"emdna_lwr25_val\"].dropna().reset_index(drop=True)\n",
    "    \n",
    "        # A) Rx1day\n",
    "        obs_rx1 = calc_rx1day(obs_series)\n",
    "        emd_rx1 = calc_rx1day(emd_series)\n",
    "        rx1_list.append({\"station_name\": st_name,\n",
    "                         \"obs_rx1day\": obs_rx1,\n",
    "                         \"emd_rx1day\": emd_rx1})\n",
    "    \n",
    "        # B) Rx5day\n",
    "        obs_rx5 = calc_rx5day(obs_series)\n",
    "        emd_rx5 = calc_rx5day(emd_series)\n",
    "        rx5_list.append({\"station_name\": st_name,\n",
    "                         \"obs_rx5day\": obs_rx5,\n",
    "                         \"emd_rx5day\": emd_rx5})\n",
    "    \n",
    "        # C) CDD\n",
    "        obs_cdd_val = calc_cdd(obs_series)\n",
    "        emd_cdd_val = calc_cdd(emd_series)\n",
    "        cdd_list.append({\"station_name\": st_name,\n",
    "                         \"obs_cdd\": obs_cdd_val,\n",
    "                         \"emd_cdd\": emd_cdd_val})\n",
    "    \n",
    "        # D) CWD\n",
    "        obs_cwd_val = calc_cwd(obs_series)\n",
    "        emd_cwd_val = calc_cwd(emd_series)\n",
    "        cwd_list.append({\"station_name\": st_name,\n",
    "                         \"obs_cwd\": obs_cwd_val,\n",
    "                         \"emd_cwd\": emd_cwd_val})\n",
    "    \n",
    "        # E) R95 / R99\n",
    "        or95a, or95p, or99a, or99p = calc_r95p_r99p(obs_series)\n",
    "        er95a, er95p, er99a, er99p = calc_r95p_r99p(emd_series)\n",
    "        r95_list.append({\n",
    "            \"station_name\": st_name,\n",
    "            \"obs_r95amt\": or95a, \"obs_r95pct\": or95p,\n",
    "            \"emd_r95amt\": er95a, \"emd_r95pct\": er95p\n",
    "        })\n",
    "        r99_list.append({\n",
    "            \"station_name\": st_name,\n",
    "            \"obs_r99amt\": or99a, \"obs_r99pct\": or99p,\n",
    "            \"emd_r99amt\": er99a, \"emd_r99pct\": er99p\n",
    "        })\n",
    "    \n",
    "        # F) wet/dry days\n",
    "        obs_wet5, obs_dry = calc_wetdays_drydays(obs_series)\n",
    "        emd_wet5, emd_dry = calc_wetdays_drydays(emd_series)\n",
    "        wet_list.append({\n",
    "            \"station_name\": st_name,\n",
    "            \"obs_wetdays5mm\": obs_wet5, \n",
    "            \"emd_wetdays5mm\": emd_wet5\n",
    "        })\n",
    "        dry_list.append({\n",
    "            \"station_name\": st_name,\n",
    "            \"obs_drydays\": obs_dry,\n",
    "            \"emd_drydays\": emd_dry\n",
    "        })\n",
    "    \n",
    "    print(\"Finished computing. Now merging lat/lon from physical file ...\")\n",
    "    \n",
    "    # Convert each list to DataFrame => merge lat,lon => save\n",
    "    def attach_coords(df_in):\n",
    "        \"\"\"Attach lat, lon, elev from df_phys on station_name.\"\"\"\n",
    "        df_out = pd.merge(\n",
    "            df_in, \n",
    "            df_phys[[\"station_name\",\"lat\",\"lon\",\"elev\"]],\n",
    "            on=\"station_name\", \n",
    "            how=\"left\"\n",
    "        )\n",
    "        return df_out\n",
    "    \n",
    "    df_rx1 = attach_coords(pd.DataFrame(rx1_list))\n",
    "    df_rx5 = attach_coords(pd.DataFrame(rx5_list))\n",
    "    df_cdd = attach_coords(pd.DataFrame(cdd_list))\n",
    "    df_cwd = attach_coords(pd.DataFrame(cwd_list))\n",
    "    df_r95 = attach_coords(pd.DataFrame(r95_list))\n",
    "    df_r99 = attach_coords(pd.DataFrame(r99_list))\n",
    "    df_wet = attach_coords(pd.DataFrame(wet_list))\n",
    "    df_dry = attach_coords(pd.DataFrame(dry_list))\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 6. SAVE OUTPUT (SAME FILE NAMES AS BEFORE)\n",
    "    ###############################################################################\n",
    "    print(\"Saving index tables to Excel in:\", output_dir)\n",
    "    df_rx1.to_excel(os.path.join(output_dir, \"rx1day.xlsx\"),  index=False)\n",
    "    df_rx5.to_excel(os.path.join(output_dir, \"rx5day.xlsx\"),  index=False)\n",
    "    df_cdd.to_excel(os.path.join(output_dir, \"cdd.xlsx\"),     index=False)\n",
    "    df_cwd.to_excel(os.path.join(output_dir, \"cwd.xlsx\"),     index=False)\n",
    "    df_r95.to_excel(os.path.join(output_dir, \"r95p.xlsx\"),    index=False)\n",
    "    df_r99.to_excel(os.path.join(output_dir, \"r99p.xlsx\"),    index=False)\n",
    "    df_wet.to_excel(os.path.join(output_dir, \"wetdays.xlsx\"), index=False)\n",
    "    df_dry.to_excel(os.path.join(output_dir, \"drydays.xlsx\"), index=False)\n",
    "    \n",
    "    print(\"\\nAll precipitation-based indices have been saved to Excel with station_name (and lat/lon).\")\n",
    "    \n",
    "    ###############################################################################\n",
    "    # (OPTIONAL) QUICK MAP EXAMPLE (like obs_rx5day)\n",
    "    ###############################################################################\n",
    "    try:\n",
    "        print(\"\\nQuick map example for obs_rx5day ...\")\n",
    "        # Load shapefiles\n",
    "        gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "        gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "    \n",
    "        gdf_stations = gpd.GeoDataFrame(\n",
    "            df_rx5,\n",
    "            geometry=gpd.points_from_xy(df_rx5[\"lon\"], df_rx5[\"lat\"]),\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(10,8), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "        for geom in gdf_basin.geometry:\n",
    "            ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='blue', linewidth=1)\n",
    "        for geom in gdf_lakes.geometry:\n",
    "            ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='cyan', linewidth=1)\n",
    "    \n",
    "        ax.add_feature(cfeature.COASTLINE)\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    \n",
    "        sc = ax.scatter(gdf_stations.geometry.x, gdf_stations.geometry.y,\n",
    "                        c=gdf_stations[\"obs_rx5day\"], cmap=\"Reds\", s=60,\n",
    "                        transform=ccrs.PlateCarree(), edgecolor=\"k\")\n",
    "        plt.colorbar(sc, ax=ax, label=\"Obs Rx5day (mm)\")\n",
    "    \n",
    "        ax.set_extent([-95.5, -72, 38.5, 52.5])  # approximate bounding\n",
    "        gl = ax.gridlines(draw_labels=True, linestyle='--', color='gray')\n",
    "        gl.right_labels = False\n",
    "        gl.top_labels   = False\n",
    "    \n",
    "        plt.title(\"Obs Rx5day (from CSV daily data)\", fontsize=14)\n",
    "        plt.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Mapping step failed:\", e)\n",
    "    \n",
    "    print(\"\\n✅ Done computing precipitation-based indices from CSV, with station names included!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec0824d-70ae-4253-b5ac-1f6cdfb23719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the climatic indices for EMDNA (temperature (tmin-tmax)) \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 0.  WHICH ENSEMBLES TO PROCESS\n",
    "###############################################################################\n",
    "ENSEMBLES = [1, 11, 21, 31, 41, 51, 61, 71, 81, 91]     \n",
    "\n",
    "for ens in ENSEMBLES:                                   \n",
    "    print(f\"\\n================  ENSEMBLE {ens}  ================\\n\")\n",
    "\n",
    "    ###############################################################################\n",
    "    # 1. FILE PATHS & CONFIG\n",
    "    ###############################################################################\n",
    "    csv_file = rf\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\EMDNA_GLB_Temperature\\{ens}\\daily_loop\\emdna_vs_stations_25km_LWR_1991_2012_tmin_tmax.csv\"\n",
    "    physical_file = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation_Temperature.csv\"\n",
    "\n",
    "    shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "    lakes_shp = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "    output_dir = rf\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\EMDNA_GLB_Temperature\\{ens}\\ClimaticIndices-25KM\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    ###############################################################################\n",
    "    # 2. LOAD DAILY CSV DATA\n",
    "    ###############################################################################\n",
    "    print(\"Loading daily CSV data (obs, emdna_lwr25_val) ...\")\n",
    "    df_data = pd.read_csv(csv_file)\n",
    "    df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "    df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "    print(f\"df_data shape = {df_data.shape}\")\n",
    "    print(\"Columns:\", df_data.columns.tolist())\n",
    "    print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "    ###############################################################################\n",
    "    # 3. MERGE WITH PHYSICAL FILE TO GET LAT/LON\n",
    "    ###############################################################################\n",
    "    df_phys = pd.read_csv(physical_file)\n",
    "    df_phys = df_phys.rename(columns={\n",
    "        \"NAME\": \"station_name\",\n",
    "        \"LATITUDE\": \"lat\",\n",
    "        \"LONGITUDE\": \"lon\",\n",
    "        \"Elevation\": \"elev\"\n",
    "    })\n",
    "    df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "    ###############################################################################\n",
    "    # 4. HELPER FUNCTIONS – FULL ETCCDI SET FOR Tmin / Tmax\n",
    "    ###############################################################################\n",
    "    def absolute_extremes(series, kind):\n",
    "        \"\"\"Return the single-day absolute extreme.\"\"\"\n",
    "        if kind == \"max\":\n",
    "            return series.max(skipna=True)\n",
    "        else:                       # \"min\"\n",
    "            return series.min(skipna=True)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # percentile thresholds (5-day moving window, baseline = 1991-2012)\n",
    "    # -------------------------------------------------------------------------\n",
    "    BASE_START, BASE_END = \"1991-01-01\", \"2012-12-31\"\n",
    "\n",
    "    def _climatology_percentiles(s, p):\n",
    "        \"\"\"Return a Series (index = 1…366) of the p-th percentile.\"\"\"\n",
    "        # drop 29 Feb so every year has 365 days\n",
    "        s = s[~((s.index.month == 2) & (s.index.day == 29))]\n",
    "        df = pd.DataFrame({\"val\": s, \"doy\": s.index.dayofyear})\n",
    "        climo = []\n",
    "        for d in range(1, 366):\n",
    "            win = list(range(d-2, d+3))                       # ±2-day window\n",
    "            win = [(x-1) % 365 + 1 for x in win]              # wrap around ends\n",
    "            vals = df.loc[df[\"doy\"].isin(win), \"val\"]\n",
    "            climo.append(np.nanpercentile(vals, p) if len(vals) else np.nan)\n",
    "        return pd.Series(climo, index=range(1, 366), name=f\"p{p}\")\n",
    "\n",
    "    def percentile_flags(s, perc_series, side):\n",
    "        \"\"\"Return Boolean Series: True where value is < or > percentile.\"\"\"\n",
    "        doy = s.index.dayofyear\n",
    "        thr = perc_series.reindex(doy).values\n",
    "        if side == \"low\":\n",
    "            return s < thr\n",
    "        else:\n",
    "            return s > thr\n",
    "\n",
    "    def spell_length(bool_series, min_run=6):\n",
    "        \"\"\"Total # days in spells of ≥ min_run consecutive Trues.\"\"\"\n",
    "        is_true = bool_series.fillna(False).values\n",
    "        # identify run lengths\n",
    "        run_ends = np.where(np.diff(np.concatenate(([0], is_true, [0]))))[0]\n",
    "        lengths  = run_ends[1::2] - run_ends[::2]\n",
    "        return lengths[lengths >= min_run].sum()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # absolute-threshold counters\n",
    "    # -------------------------------------------------------------------------\n",
    "    def count_threshold(series, op, thr):\n",
    "        if op == \"<\":\n",
    "            return (series < thr).sum()\n",
    "        else:\n",
    "            return (series > thr).sum()\n",
    "\n",
    "    ###############################################################################\n",
    "    # 5.  ANNUAL ETCCDI INDICES  –  FORMAT COMPATIBLE WITH THE SEASONAL SCRIPT\n",
    "    #     • keeps:   TXx  TNn  TX90p  TN10p  FD  WSDI  CSDI\n",
    "    #     • drops:   TR, ID, SU, TXn, TNx\n",
    "    ###############################################################################\n",
    "    rows = []\n",
    "\n",
    "    print(\"→ computing *annual* indices …\")\n",
    "    for st_name, st_grp in df_data.groupby(\"station_name\"):\n",
    "\n",
    "        st_grp = st_grp.set_index(\"time\").sort_index()\n",
    "\n",
    "        # build daily Series ................................................................\n",
    "        obs_max   = st_grp.loc[st_grp[\"var\"] == \"tmax\", \"obs\"].asfreq(\"D\")\n",
    "        obs_min   = st_grp.loc[st_grp[\"var\"] == \"tmin\", \"obs\"].asfreq(\"D\")\n",
    "        emdna_max = st_grp.loc[st_grp[\"var\"] == \"tmax\", \"emdna_lwr25_val\"].asfreq(\"D\")\n",
    "        emdna_min = st_grp.loc[st_grp[\"var\"] == \"tmin\", \"emdna_lwr25_val\"].asfreq(\"D\")\n",
    "        if obs_max.empty:          # station has no data at all\n",
    "            continue\n",
    "\n",
    "        # ── fixed-year climatology (1991-2012, 5-day moving window) ─────────────-\n",
    "        p90_TX_obs   = _climatology_percentiles(obs_max [BASE_START:BASE_END], 90)\n",
    "        p10_TN_obs   = _climatology_percentiles(obs_min [BASE_START:BASE_END], 10)\n",
    "        p90_TX_emdna = _climatology_percentiles(emdna_max[BASE_START:BASE_END], 90)\n",
    "        p10_TN_emdna = _climatology_percentiles(emdna_min[BASE_START:BASE_END], 10)\n",
    "\n",
    "        # flags for the *full* record (faster than recomputing year-by-year)\n",
    "        flags = {\n",
    "            \"obs_TX90\":   obs_max  > p90_TX_obs  .reindex(obs_max .index.dayofyear).values,\n",
    "            \"emdna_TX90\": emdna_max> p90_TX_emdna.reindex(emdna_max.index.dayofyear).values,\n",
    "            \"obs_TN10\":   obs_min  < p10_TN_obs  .reindex(obs_min .index.dayofyear).values,\n",
    "            \"emdna_TN10\": emdna_min< p10_TN_emdna.reindex(emdna_min.index.dayofyear).values,\n",
    "        }\n",
    "\n",
    "        # ── iterate over years (December belongs to the following DJF year) ──────\n",
    "        years = np.unique(obs_max.index.year)\n",
    "        for yr in years:\n",
    "            mask = obs_max.index.year == yr\n",
    "            if mask.sum() < 200:         # at least ~55 % of a year\n",
    "                continue\n",
    "\n",
    "            def _sel(s):          # helper to slice one year\n",
    "                return s[s.index.year == yr]\n",
    "\n",
    "            # intensity ....................................................................\n",
    "            TXx_obs   = _sel(obs_max  ).max()\n",
    "            TXx_emdna = _sel(emdna_max).max()\n",
    "            TNn_obs   = _sel(obs_min  ).min()\n",
    "            TNn_emdna = _sel(emdna_min).min()\n",
    "\n",
    "            # percentile frequencies (percentage of days)\n",
    "            TX90p_obs   = flags[\"obs_TX90\"  ][mask].mean() * 100.0\n",
    "            TX90p_emdna = flags[\"emdna_TX90\"][mask].mean() * 100.0\n",
    "            TN10p_obs   = flags[\"obs_TN10\"  ][mask].mean() * 100.0\n",
    "            TN10p_emdna = flags[\"emdna_TN10\"][mask].mean() * 100.0\n",
    "\n",
    "            # spell duration (≥6 consecutive days)\n",
    "            WSDI_obs   = spell_length(flags[\"obs_TX90\"  ][mask], min_run=6)\n",
    "            WSDI_emdna = spell_length(flags[\"emdna_TX90\"][mask], min_run=6)\n",
    "            CSDI_obs   = spell_length(flags[\"obs_TN10\"  ][mask], min_run=6)\n",
    "            CSDI_emdna = spell_length(flags[\"emdna_TN10\"][mask], min_run=6)\n",
    "\n",
    "            # absolute‐threshold count\n",
    "            FD_obs   = (_sel(obs_min  ) < 0).sum()\n",
    "            FD_emdna = (_sel(emdna_min) < 0).sum()\n",
    "\n",
    "            # helper for ratios (avoid /0)\n",
    "            ratio = lambda o, e: np.nan if (o == 0 or np.isnan(o)) else e / o\n",
    "\n",
    "            rows.append(dict(\n",
    "                station_name=st_name, year=yr,\n",
    "                TXx_obs   = TXx_obs,   TXx_emdna   = TXx_emdna,   TXx_ratio   = ratio(TXx_obs, TXx_emdna),\n",
    "                TNn_obs   = TNn_obs,   TNn_emdna   = TNn_emdna,   TNn_ratio   = ratio(TNn_obs, TNn_emdna),\n",
    "                TX90p_obs = TX90p_obs, TX90p_emdna = TX90p_emdna, TX90p_ratio = ratio(TX90p_obs, TX90p_emdna),\n",
    "                TN10p_obs = TN10p_obs, TN10p_emdna = TN10p_emdna, TN10p_ratio = ratio(TN10p_obs, TN10p_emdna),\n",
    "                FD_obs    = FD_obs,    FD_emdna    = FD_emdna,    FD_ratio    = ratio(FD_obs,  FD_emdna),\n",
    "                WSDI_obs  = WSDI_obs,  WSDI_emdna  = WSDI_emdna,  WSDI_ratio  = ratio(WSDI_obs, WSDI_emdna),\n",
    "                CSDI_obs  = CSDI_obs,  CSDI_emdna  = CSDI_emdna,  CSDI_ratio  = ratio(CSDI_obs, CSDI_emdna)\n",
    "            ))\n",
    "\n",
    "    df_yr = (pd.DataFrame(rows)\n",
    "             .merge(df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "                    on=\"station_name\", how=\"left\")\n",
    "             .sort_values([\"station_name\", \"year\"]))\n",
    "\n",
    "    ###############################################################################\n",
    "    # 6.  SAVE OUTPUT  – ONE PARQUET ( + XLSX )  +  OPTIONAL per-index sheets\n",
    "    ###############################################################################\n",
    "    annual_pq  = os.path.join(output_dir, \"Indices_Annual.parquet\")\n",
    "    annual_xls = annual_pq.replace(\".parquet\", \".xlsx\")\n",
    "\n",
    "    df_yr.to_parquet(annual_pq, index=False)\n",
    "    df_yr.to_excel  (annual_xls, index=False)\n",
    "    print(f\"✓ Annual indices saved → {annual_pq}\")\n",
    "    print(f\"✓ …and also saved as   → {annual_xls}\")\n",
    "\n",
    "    # OPTIONAL: write one Excel file per index in the familiar “wide” format\n",
    "    # ---------------------------------------------------------------------------\n",
    "    index_roots = [\"TXx\", \"TNn\", \"TX90p\", \"TN10p\", \"FD\", \"WSDI\", \"CSDI\"]\n",
    "    def _wide(idx_root: str) -> pd.DataFrame:\n",
    "        return (df_yr\n",
    "                .pivot_table(index=\"station_name\",\n",
    "                             values=[f\"{idx_root}_obs\", f\"{idx_root}_emdna\", f\"{idx_root}_ratio\"])\n",
    "                .reset_index()\n",
    "                .merge(df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "                       on=\"station_name\", how=\"left\"))\n",
    "\n",
    "    print(\"\\n(optional) individual workbooks …\")\n",
    "    for idx in index_roots:\n",
    "        w  = _wide(idx)\n",
    "        fp = os.path.join(output_dir, f\"{idx}.xlsx\")\n",
    "        w.to_excel(fp, index=False)\n",
    "        print(\"  •\", os.path.basename(fp))\n",
    "\n",
    "    print(\"\\n✅  Annual-index workflow finished for ensemble\", ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535c7ec-2d93-488d-a0e4-992146ce6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal stratification of climatic indices for prcp EMDNA\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATION & PATHS   ★ REWRITTEN FOR MULTI-ENSEMBLE ★\n",
    "###############################################################################\n",
    "root_dir = (r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\"\n",
    "            r\"\\Ensemble files\\EMDNA_GLB_Precipitation\")\n",
    "\n",
    "# the 10 ensemble sub-folders to process\n",
    "ENSEMBLES = [1, 11, 21, 31, 41, 51, 61, 71, 81, 91]\n",
    "\n",
    "# static station-metadata file (same for every ensemble)\n",
    "physical_file = (r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\"\n",
    "                 r\"\\filtered_stations_with_elevation.csv\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# BEGIN LOOP over ensembles – everything downstream is indented inside it\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "for ENS in ENSEMBLES:\n",
    "    print(\"\\n\" + \"=\"*84)\n",
    "    print(f\"⧉  Temporal-stratified indices for ensemble {ENS:03d}  ⧉\")\n",
    "    print(\"=\"*84)\n",
    "\n",
    "    ens_dir = os.path.join(root_dir, str(ENS))\n",
    "\n",
    "    # daily-loop CSV produced by the 25-km LWR script\n",
    "    csv_file = os.path.join(\n",
    "        ens_dir, \"daily_loop\",\n",
    "        f\"emdna_vs_stations_25km_LWR_1991_2012_prcp_{ENS:03d}.csv\"\n",
    "    )\n",
    "    if not os.path.isfile(csv_file):\n",
    "        print(f\"   ⚠  {os.path.basename(csv_file)} not found – skipping ensemble\")\n",
    "        continue\n",
    "\n",
    "    # output folder for this ensemble\n",
    "    output_dir = os.path.join(ens_dir, \"ClimaticIndices-25KM\", \"TemporalStrat\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    ############################################################################\n",
    "    # 2. LOAD DAILY DATA & ADD TEMPORAL FIELDS\n",
    "    ############################################################################\n",
    "    print(\"Loading daily CSV data …\")\n",
    "    df_data = pd.read_csv(csv_file)\n",
    "    df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "\n",
    "    # Standardize station_name\n",
    "    df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "    \n",
    "    # Add month (1..12) and season (DJF, MAM, JJA, SON)\n",
    "    df_data[\"month\"] = df_data[\"time\"].dt.month\n",
    "    \n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return \"DJF\"\n",
    "        elif month in [3, 4, 5]:\n",
    "            return \"MAM\"\n",
    "        elif month in [6, 7, 8]:\n",
    "            return \"JJA\"\n",
    "        else:\n",
    "            return \"SON\"\n",
    "    \n",
    "    df_data[\"season\"] = df_data[\"month\"].apply(get_season)\n",
    "    \n",
    "    print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 3. LOAD PHYSICAL FILE & MERGE COORDINATES\n",
    "    ###############################################################################\n",
    "    df_phys = pd.read_csv(physical_file)\n",
    "    df_phys = df_phys.rename(columns={\n",
    "        \"NAME\": \"station_name\",\n",
    "        \"LATITUDE\": \"lat\",\n",
    "        \"LONGITUDE\": \"lon\",\n",
    "        \"Elevation\": \"elev\"\n",
    "    })\n",
    "    df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "    ###############################################################################\n",
    "    def calc_rx1day(series):\n",
    "        \"\"\"Max 1-day precipitation.\"\"\"\n",
    "        return series.max(skipna=True)\n",
    "    \n",
    "    def calc_rx5day(series):\n",
    "        \"\"\"Max 5-day running sum.\"\"\"\n",
    "        return series.rolling(5, min_periods=1).sum().max(skipna=True)\n",
    "    \n",
    "    def calc_cdd(series, dry_threshold=1.0):\n",
    "        \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "        is_dry = series < dry_threshold\n",
    "        max_run, current_run = 0, 0\n",
    "        for val in is_dry:\n",
    "            if val:\n",
    "                current_run += 1\n",
    "                max_run = max(max_run, current_run)\n",
    "            else:\n",
    "                current_run = 0\n",
    "        return max_run\n",
    "    \n",
    "    def calc_cwd(series, wet_threshold=1.0):\n",
    "        \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "        is_wet = series >= wet_threshold\n",
    "        max_run, current_run = 0, 0\n",
    "        for val in is_wet:\n",
    "            if val:\n",
    "                current_run += 1\n",
    "                max_run = max(max_run, current_run)\n",
    "            else:\n",
    "                current_run = 0\n",
    "        return max_run\n",
    "    \n",
    "    def calc_r95p_r99p(series, percentiles=(95,99)):\n",
    "        \"\"\"\n",
    "        r95amt, r95pct, r99amt, r99pct:\n",
    "        - r95amt = sum of daily prcp above 95th percentile\n",
    "        - r95pct = (r95amt / total) * 100\n",
    "        - similarly for 99th percentile\n",
    "        \"\"\"\n",
    "        wet = series[series >= 1.0]\n",
    "        if len(wet) < 5:\n",
    "            return np.nan, np.nan, np.nan, np.nan\n",
    "        p95 = np.percentile(wet, percentiles[0])\n",
    "        p99 = np.percentile(wet, percentiles[1])\n",
    "        r95_amt = wet[wet > p95].sum()\n",
    "        r99_amt = wet[wet > p99].sum()\n",
    "        total = series.sum(skipna=True)\n",
    "        r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "        r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "        return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "    \n",
    "    def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "        \"\"\"\n",
    "        wetdays = #days >= wet_thr\n",
    "        drydays = #days < dry_thr\n",
    "        \"\"\"\n",
    "        return (series >= wet_thr).sum(), (series < dry_thr).sum()\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 5. FUNCTION TO COMPUTE INDICES FOR A GROUP (MONTHLY or SEASONAL)\n",
    "    ###############################################################################\n",
    "    def compute_indices(df_group):\n",
    "        \"\"\"\n",
    "        For a subset of daily data (e.g. station+month, or station+season),\n",
    "        compute the climate indices for Obs vs EMD, plus ratio columns.\n",
    "        \"\"\"\n",
    "        obs_series = df_group[\"obs\"].dropna().reset_index(drop=True)\n",
    "        emd_series = df_group[\"emdna_lwr25_val\"].dropna().reset_index(drop=True)\n",
    "        if len(obs_series) == 0 or len(emd_series) == 0:\n",
    "            return None\n",
    "        \n",
    "        res = {}\n",
    "        # Rx1day / Rx5day\n",
    "        res[\"rx1day_obs\"] = calc_rx1day(obs_series)\n",
    "        res[\"rx1day_emd\"] = calc_rx1day(emd_series)\n",
    "        res[\"rx5day_obs\"] = calc_rx5day(obs_series)\n",
    "        res[\"rx5day_emd\"] = calc_rx5day(emd_series)\n",
    "        \n",
    "        # CDD / CWD\n",
    "        res[\"cdd_obs\"] = calc_cdd(obs_series)\n",
    "        res[\"cdd_emd\"] = calc_cdd(emd_series)\n",
    "        res[\"cwd_obs\"] = calc_cwd(obs_series)\n",
    "        res[\"cwd_emd\"] = calc_cwd(emd_series)\n",
    "        \n",
    "        # R95 / R99\n",
    "        r95_obs = calc_r95p_r99p(obs_series)\n",
    "        r95_emd = calc_r95p_r99p(emd_series)\n",
    "        res[\"r95amt_obs\"] = r95_obs[0]\n",
    "        res[\"r95pct_obs\"] = r95_obs[1]\n",
    "        res[\"r95amt_emd\"] = r95_emd[0]\n",
    "        res[\"r95pct_emd\"] = r95_emd[1]\n",
    "        res[\"r99amt_obs\"] = r95_obs[2]\n",
    "        res[\"r99pct_obs\"] = r95_obs[3]\n",
    "        res[\"r99amt_emd\"] = r95_emd[2]\n",
    "        res[\"r99pct_emd\"] = r95_emd[3]\n",
    "        \n",
    "        # Wet / Dry days\n",
    "        wet_obs, dry_obs = calc_wetdays_drydays(obs_series)\n",
    "        wet_emd, dry_emd = calc_wetdays_drydays(emd_series)\n",
    "        res[\"wetdays_obs\"] = wet_obs\n",
    "        res[\"wetdays_emd\"] = wet_emd\n",
    "        res[\"drydays_obs\"] = dry_obs\n",
    "        res[\"drydays_emd\"] = dry_emd\n",
    "        \n",
    "        # Ratio columns: EMD/OBS if obs != 0\n",
    "        if res[\"rx1day_obs\"]:\n",
    "            res[\"rx1day_ratio\"] = res[\"rx1day_emd\"] / res[\"rx1day_obs\"]\n",
    "        if res[\"rx5day_obs\"]:\n",
    "            res[\"rx5day_ratio\"] = res[\"rx5day_emd\"] / res[\"rx5day_obs\"]\n",
    "        if res[\"cdd_obs\"]:\n",
    "            res[\"cdd_ratio\"] = res[\"cdd_emd\"] / res[\"cdd_obs\"]\n",
    "        if res[\"cwd_obs\"]:\n",
    "            res[\"cwd_ratio\"] = res[\"cwd_emd\"] / res[\"cwd_obs\"]\n",
    "        if res[\"r95amt_obs\"]:\n",
    "            res[\"r95amt_ratio\"] = res[\"r95amt_emd\"] / res[\"r95amt_obs\"]\n",
    "        if res[\"r95pct_obs\"]:\n",
    "            res[\"r95pct_ratio\"] = res[\"r95pct_emd\"] / res[\"r95pct_obs\"]\n",
    "        if res[\"r99amt_obs\"]:\n",
    "            res[\"r99amt_ratio\"] = res[\"r99amt_emd\"] / res[\"r99amt_obs\"]\n",
    "        if res[\"r99pct_obs\"]:\n",
    "            res[\"r99pct_ratio\"] = res[\"r99pct_emd\"] / res[\"r99pct_obs\"]\n",
    "        if res[\"wetdays_obs\"]:\n",
    "            res[\"wetdays_ratio\"] = res[\"wetdays_emd\"] / res[\"wetdays_obs\"]\n",
    "        if res[\"drydays_obs\"]:\n",
    "            res[\"drydays_ratio\"] = res[\"drydays_emd\"] / res[\"drydays_obs\"]\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 6. MONTHLY INDICES\n",
    "    ###############################################################################\n",
    "    monthly_results = []\n",
    "    group_month = df_data.groupby([\"station_name\", \"month\"])\n",
    "    for (st_name, mon), group in group_month:\n",
    "        indices = compute_indices(group)\n",
    "        if indices is None:\n",
    "            continue\n",
    "        indices[\"station_name\"] = st_name\n",
    "        indices[\"month\"] = mon\n",
    "        monthly_results.append(indices)\n",
    "    \n",
    "    df_monthly = pd.DataFrame(monthly_results)\n",
    "    df_monthly = pd.merge(\n",
    "        df_monthly,\n",
    "        df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "        on=\"station_name\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    df_monthly = df_monthly.sort_values([\"station_name\", \"month\"])\n",
    "    monthly_out = os.path.join(output_dir, \"Indices_Monthly.xlsx\")\n",
    "    df_monthly.to_excel(monthly_out, index=False)\n",
    "    print(\"Monthly indices saved =>\", monthly_out)\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 7. SEASONAL INDICES\n",
    "    ###############################################################################\n",
    "    seasonal_results = []\n",
    "    group_season = df_data.groupby([\"station_name\", \"season\"])\n",
    "    for (st_name, seas), group in group_season:\n",
    "        indices = compute_indices(group)\n",
    "        if indices is None:\n",
    "            continue\n",
    "        indices[\"station_name\"] = st_name\n",
    "        indices[\"season\"] = seas\n",
    "        seasonal_results.append(indices)\n",
    "    \n",
    "    df_seasonal = pd.DataFrame(seasonal_results)\n",
    "    df_seasonal = pd.merge(\n",
    "        df_seasonal,\n",
    "        df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "        on=\"station_name\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    df_seasonal = df_seasonal.sort_values([\"station_name\", \"season\"])\n",
    "    seasonal_out = os.path.join(output_dir, \"Indices_Seasonal.xlsx\")\n",
    "    df_seasonal.to_excel(seasonal_out, index=False)\n",
    "    print(\"Seasonal indices saved =>\", seasonal_out)\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 8. DONE\n",
    "    ###############################################################################\n",
    "    print(f\"✅  Finished ensemble {ENS:03d}  →  outputs in  {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5259bcdf-d138-429c-8cd6-3457a7a24815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal (DJF / MAM / JJA / SON) analysis for prcp\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import Point\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATION & PATHS  ★ MULTI-ENSEMBLE + MULTI-SEASON ★\n",
    "###############################################################################\n",
    "root_dir = (r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\"\n",
    "            r\"\\Ensemble files\\EMDNA_GLB_Precipitation\")\n",
    "\n",
    "# ensembles to analyse\n",
    "ENSEMBLES = [1, 11, 21, 31, 41, 51, 61, 71, 81, 91]\n",
    "\n",
    "# seasons to analyse\n",
    "SEASONS = [\"DJF\", \"MAM\", \"JJA\", \"SON\"]\n",
    "\n",
    "# static files\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "# index → (obs-cols , emd-cols) mapping (unchanged)\n",
    "index_columns = {\n",
    "    \"rx1day\":  (\"rx1day_obs\",  \"rx1day_emd\"),\n",
    "    \"rx5day\":  (\"rx5day_obs\",  \"rx5day_emd\"),\n",
    "    \"cdd\":     (\"cdd_obs\",     \"cdd_emd\"),\n",
    "    \"cwd\":     (\"cwd_obs\",     \"cwd_emd\"),\n",
    "    \"r95p\":    ((\"r95amt_obs\", \"r95pct_obs\"),\n",
    "                (\"r95amt_emd\", \"r95pct_emd\")),\n",
    "    \"r99p\":    ((\"r99amt_obs\", \"r99pct_obs\"),\n",
    "                (\"r99amt_emd\", \"r99pct_emd\")),\n",
    "    \"wetdays\": (\"wetdays_obs\", \"wetdays_emd\"),\n",
    "    \"drydays\": (\"drydays_obs\", \"drydays_emd\"),\n",
    "}\n",
    "# ── BLOCK-1 : add right after index_columns ──────────────────────────────\n",
    "index_list = [\n",
    "    \"rx1day\", \"rx5day\", \"cdd\", \"cwd\",\n",
    "    \"r95p\", \"r99p\", \"wetdays\", \"drydays\"\n",
    "]\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# OUTER LOOP → ensembles\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "for ENS in ENSEMBLES:\n",
    "    indices_dir = os.path.join(root_dir, str(ENS),\n",
    "                               \"ClimaticIndices-25KM\", \"TemporalStrat\")\n",
    "    seasonal_file = os.path.join(indices_dir, \"Indices_Seasonal.xlsx\")\n",
    "    \n",
    "    # ❷ …but fall back to the legacy “ClimaticIndices2” folder if not found\n",
    "    if not os.path.isfile(seasonal_file):\n",
    "        legacy_dir   = os.path.join(root_dir, str(ENS), \"ClimaticIndices2\")\n",
    "        legacy_file  = os.path.join(legacy_dir, \"Indices_Seasonal.xlsx\")\n",
    "        if os.path.isfile(legacy_file):\n",
    "            indices_dir   = legacy_dir          # use the old folder\n",
    "            seasonal_file = legacy_file\n",
    "        else:\n",
    "            print(f\"⚠  Indices_Seasonal.xlsx not found for ensemble {ENS:03d}\")\n",
    "            continue\n",
    "\n",
    "    # load once per ensemble – we'll slice by season later\n",
    "    df_all_seasons = pd.read_excel(seasonal_file).dropna(subset=[\"lat\", \"lon\"])\n",
    "    print(f\"\\n================  ENSEMBLE {ENS:03d}  =================\")\n",
    "    print(\"Seasonal file rows:\", len(df_all_seasons))\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────────\n",
    "    # INNER LOOP → seasons\n",
    "    # ────────────────────────────────────────────────────────────────────────\n",
    "    for SEAS in SEASONS:\n",
    "        mdf = df_all_seasons[df_all_seasons[\"season\"] == SEAS].copy()\n",
    "        if mdf.empty:\n",
    "            print(f\"  · No data for {SEAS} – skipped\")\n",
    "            continue\n",
    "\n",
    "        # season-specific output folder\n",
    "        output_plots = os.path.join(indices_dir, f\"AnalysisPlots_{SEAS}\")\n",
    "        os.makedirs(output_plots, exist_ok=True)\n",
    "        print(f\"\\n—— {SEAS} ——  ({len(mdf)} station-season rows)  →  {output_plots}\")\n",
    "\n",
    "        # keep a master-table copy for this season\n",
    "        master_xlsx = os.path.join(output_plots, f\"MasterTable_{SEAS}.xlsx\")\n",
    "        mdf.to_excel(master_xlsx, index=False)\n",
    "        print(f\"(A) Master table saved → {master_xlsx}\")\n",
    "\n",
    "        # ── BLOCK-2 : full per-season workflow ──────────────────────────\n",
    "        # === 2-A  Summary statistics ===================================\n",
    "        def index_of_agreement(obs, mod):\n",
    "            om = np.mean(obs)\n",
    "            num = np.sum((mod - obs) ** 2)\n",
    "            den = np.sum((np.abs(mod - om) + np.abs(obs - om)) ** 2)\n",
    "            return np.nan if den == 0 else 1 - num / den\n",
    "\n",
    "        rmse      = lambda a, b: np.sqrt(np.mean((a - b) ** 2))\n",
    "        std_resid = lambda a, b: np.std(a - b, ddof=1)\n",
    "        mean_bias = lambda a, b: np.mean(b - a)\n",
    "\n",
    "        summary_rows = []\n",
    "        for idx in index_list:\n",
    "            obs_cols, emd_cols = index_columns[idx]\n",
    "\n",
    "            # one-column indices\n",
    "            if not isinstance(obs_cols, tuple):\n",
    "                oc, ec = obs_cols, emd_cols\n",
    "                v = mdf[[oc, ec]].dropna()\n",
    "                if len(v) < 2:   continue\n",
    "                o, e = v[oc].values, v[ec].values\n",
    "                summary_rows.append({\n",
    "                    \"Index\":  idx,\n",
    "                    \"Count\":  len(v),\n",
    "                    \"MBE\":    mean_bias(o, e),\n",
    "                    \"RMSE\":   rmse(o, e),\n",
    "                    \"STDres\": std_resid(o, e),\n",
    "                    \"CC\":     pearsonr(o, e)[0] if len(o) > 1 else np.nan,\n",
    "                    \"d\":      index_of_agreement(o, e),\n",
    "                })\n",
    "            # two-column (amt / pct) indices\n",
    "            else:\n",
    "                for oc, ec in zip(obs_cols, emd_cols):\n",
    "                    v = mdf[[oc, ec]].dropna()\n",
    "                    if len(v) < 2: continue\n",
    "                    o, e = v[oc].values, v[ec].values\n",
    "                    summary_rows.append({\n",
    "                        \"Index\":  f\"{idx}_{oc.replace('_obs','')}\",\n",
    "                        \"Count\":  len(v),\n",
    "                        \"MBE\":    mean_bias(o, e),\n",
    "                        \"RMSE\":   rmse(o, e),\n",
    "                        \"STDres\": std_resid(o, e),\n",
    "                        \"CC\":     pearsonr(o, e)[0] if len(o) > 1 else np.nan,\n",
    "                        \"d\":      index_of_agreement(o, e),\n",
    "                    })\n",
    "\n",
    "        summary_df = pd.DataFrame(summary_rows)[\n",
    "            [\"Index\", \"Count\", \"MBE\", \"RMSE\", \"STDres\", \"CC\", \"d\"]\n",
    "        ]\n",
    "        summ_xlsx = os.path.join(output_plots,\n",
    "                                 f\"SummaryTable_Extremes_{SEAS}.xlsx\")\n",
    "        summary_df.to_excel(summ_xlsx, index=False)\n",
    "        print(f\"(B) Summary table → {summ_xlsx}\")\n",
    "\n",
    "        # === 2-B  Helpers for maps / plots =============================\n",
    "        gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "        gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "\n",
    "        def add_outline(ax):\n",
    "            ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "            for g in gdf_basin.geometry:\n",
    "                ax.add_geometries([g], ccrs.PlateCarree(),\n",
    "                                  facecolor=\"none\", edgecolor=\"black\")\n",
    "            for g in gdf_lakes.geometry:\n",
    "                ax.add_geometries([g], ccrs.PlateCarree(),\n",
    "                                  facecolor=\"none\", edgecolor=\"cyan\")\n",
    "\n",
    "        def map_triptych(df, oc, ec, rc, idx):\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 6),\n",
    "                         subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "            def one(ax, col, ttl):\n",
    "                ax.set_extent([-95.5, -72, 38.5, 52.5])\n",
    "                add_outline(ax)\n",
    "                sc = ax.scatter(df[\"lon\"], df[\"lat\"], c=df[col],\n",
    "                                cmap=\"viridis\", s=60, edgecolor=\"k\",\n",
    "                                transform=ccrs.PlateCarree())\n",
    "                plt.colorbar(sc, ax=ax, shrink=0.8).set_label(col)\n",
    "                vals = df[col].dropna().values\n",
    "                if len(vals):\n",
    "                    thr = np.percentile(vals, 90)\n",
    "                    hot = df[col] >= thr\n",
    "                    ax.scatter(df.loc[hot,\"lon\"], df.loc[hot,\"lat\"],\n",
    "                               facecolors=\"none\", edgecolors=\"red\", s=80,\n",
    "                               transform=ccrs.PlateCarree())\n",
    "                ax.set_title(ttl)\n",
    "                gl = ax.gridlines(draw_labels=True, linestyle=\"--\", color=\"gray\")\n",
    "                gl.right_labels = gl.top_labels = False\n",
    "            one(axes[0], oc, f\"{idx} OBS ({SEAS})\")\n",
    "            one(axes[1], ec, f\"{idx} EMD ({SEAS})\")\n",
    "            one(axes[2], rc, f\"{idx} Ratio ({SEAS})\")\n",
    "            out_png = os.path.join(output_plots,\n",
    "                                   f\"{SEAS}_{idx}_MAP_3panel.png\")\n",
    "            plt.tight_layout(); plt.savefig(out_png, dpi=300); plt.close()\n",
    "            print(\"Map →\", out_png)\n",
    "\n",
    "        def dist_triptych(df, oc, ec, idx):\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "            # boxplot\n",
    "            sns.boxplot(data=pd.melt(df[[oc, ec]].rename(\n",
    "                    columns={oc: \"Obs\", ec: \"EMD\"})),\n",
    "                x=\"variable\", y=\"value\", ax=axes[0])\n",
    "            axes[0].set_title(f\"Box: {idx} ({SEAS})\")\n",
    "\n",
    "            # CDF\n",
    "            def ecdf(x):\n",
    "                sx = np.sort(x)\n",
    "                return sx, np.arange(1, len(sx)+1) / len(sx)\n",
    "            o, e = df[oc].dropna(), df[ec].dropna()\n",
    "            if len(o) > 1 and len(e) > 1:\n",
    "                axes[1].plot(*ecdf(o), label=\"Obs\")\n",
    "                axes[1].plot(*ecdf(e), label=\"EMD\")\n",
    "                axes[1].legend()\n",
    "            axes[1].set_title(f\"CDF: {idx} ({SEAS})\")\n",
    "\n",
    "            # scatter\n",
    "            v = df[[oc, ec]].dropna()\n",
    "            if len(v) > 1:\n",
    "                x, y = v[oc], v[ec]\n",
    "                cc   = pearsonr(x, y)[0]\n",
    "                axes[2].scatter(x, y, edgecolors=\"k\")\n",
    "                lim = [min(x.min(), y.min()), max(x.max(), y.max())]\n",
    "                axes[2].plot(lim, lim, \"r--\")\n",
    "                axes[2].set_title(f\"Scatter r={cc:.2f}\")\n",
    "            else:\n",
    "                axes[2].set_title(\"Scatter: n/a\")\n",
    "\n",
    "            out_png = os.path.join(output_plots,\n",
    "                                   f\"{SEAS}_{idx}_Distribution_3panel.png\")\n",
    "            plt.tight_layout(); plt.savefig(out_png, dpi=300); plt.close()\n",
    "            print(\"Distribution →\", out_png)\n",
    "\n",
    "        # === 2-C  Generate outputs for every index =====================\n",
    "        for idx in index_list:\n",
    "            if idx in [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"drydays\"]:\n",
    "                oc, ec, rc = f\"{idx}_obs\", f\"{idx}_emd\", f\"{idx}_ratio\"\n",
    "            elif idx == \"wetdays\":\n",
    "                oc, ec, rc = \"wetdays_obs\", \"wetdays_emd\", \"wetdays_ratio\"\n",
    "            elif idx == \"r95p\":\n",
    "                oc, ec, rc = \"r95amt_obs\", \"r95amt_emd\", \"r95amt_ratio\"\n",
    "            elif idx == \"r99p\":\n",
    "                oc, ec, rc = \"r99amt_obs\", \"r99amt_emd\", \"r99amt_ratio\"\n",
    "            else:\n",
    "                continue\n",
    "            if not all(c in mdf.columns for c in [oc, ec, rc]): continue\n",
    "            ms = mdf.dropna(subset=[\"lat\", \"lon\"])\n",
    "            map_triptych(ms, oc, ec, rc, idx)\n",
    "            dist_triptych(ms[[oc, ec]].dropna(), oc, ec, idx)\n",
    "\n",
    "        print(f\"✔ finished {SEAS}   (ensemble {ENS:03d})\")\n",
    "        # ── end BLOCK-2 ────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "        \n",
    "        ###############################################################################\n",
    "        # 3. DONE\n",
    "        ###############################################################################\n",
    "        print(f\"\\nAll {SEAS} steps completed!  See outputs in: {output_plots}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3420e8-4a7d-4a20-b8f3-fba27c8a8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "                 #########################                                           ##############################\n",
    "                 #########################                  RDRS                     ##############################\n",
    "                 #########################                                           ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fd117-ae8b-4295-a0e9-187b98555f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the climatic indices for RDRS v2.1 (prcp)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIG\n",
    "###############################################################################\n",
    "csv_file       = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\RDRS v2.1_GLB_Precipitation\\daily_loop\\rdrs_vs_stations_25km_LWR_1991_2012.csv\"\n",
    "physical_file  = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv\"\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "output_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\RDRS v2.1_GLB_Precipitation\\ClimaticIndices-25km\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY CSV DATA\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data (obs, rdrs_val) ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "# parse 'time' as datetime if needed\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# unify station_name: remove leading/trailing spaces, uppercase\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(f\"df_data shape = {df_data.shape}\")\n",
    "print(\"Columns:\", df_data.columns.tolist())\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. (OPTIONAL) MERGE WITH PHYSICAL FILE TO GET LAT/LON\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "# unify station_name\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# We'll merge lat/lon AFTER computing the indices, so each final row has lat/lon.\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "###############################################################################\n",
    "def calc_rx1day(series):\n",
    "    \"\"\"Max 1-day precipitation.\"\"\"\n",
    "    return series.max(skipna=True)\n",
    "\n",
    "def calc_rx5day(series):\n",
    "    \"\"\"Max 5-day running sum.\"\"\"\n",
    "    roll_5 = series.rolling(5, min_periods=1).sum()\n",
    "    return roll_5.max(skipna=True)\n",
    "\n",
    "def calc_cdd(series, dry_threshold=1.0):\n",
    "    \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "    is_dry = series < dry_threshold\n",
    "    maxr, curr = 0, 0\n",
    "    for val in is_dry:\n",
    "        if val:\n",
    "            curr += 1\n",
    "            maxr = max(maxr, curr)\n",
    "        else:\n",
    "            curr = 0\n",
    "    return maxr\n",
    "\n",
    "def calc_cwd(series, wet_threshold=1.0):\n",
    "    \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "    is_wet = series >= wet_threshold\n",
    "    maxr, curr = 0, 0\n",
    "    for val in is_wet:\n",
    "        if val:\n",
    "            curr += 1\n",
    "            maxr = max(maxr, curr)\n",
    "        else:\n",
    "            curr = 0\n",
    "    return maxr\n",
    "\n",
    "def calc_r95p_r99p(series, percentile=(95,99)):\n",
    "    \"\"\"R95p, R99p TOT in mm, plus percentage of total.\"\"\"\n",
    "    # only wet days >=1 mm for percentile\n",
    "    wet = series[series >= 1.0]\n",
    "    if len(wet) < 5:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    p95 = np.percentile(wet, percentile[0])\n",
    "    p99 = np.percentile(wet, percentile[1])\n",
    "    r95_amt = wet[wet > p95].sum()\n",
    "    r99_amt = wet[wet > p99].sum()\n",
    "    total   = series.sum(skipna=True)\n",
    "    r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "    r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "    return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "\n",
    "def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "    \"\"\"Count #wet days >=5 mm, #dry days <1 mm.\"\"\"\n",
    "    w = (series >= wet_thr).sum()\n",
    "    d = (series <  dry_thr).sum()\n",
    "    return w, d\n",
    "\n",
    "###############################################################################\n",
    "# 5. COMPUTE INDICES FOR EACH STATION\n",
    "###############################################################################\n",
    "# We'll store results in lists of dicts, then convert to DataFrame => Excel.\n",
    "rx1_list, rx5_list = [], []\n",
    "cdd_list, cwd_list = [], []\n",
    "r95_list, r99_list = [], []\n",
    "wet_list, dry_list = [], []\n",
    "\n",
    "print(\"Computing indices for each station...\")\n",
    "grouped = df_data.groupby(\"station_name\", as_index=False)\n",
    "\n",
    "for st_name, grp in grouped:\n",
    "    # Sort by time (just in case)\n",
    "    grp = grp.sort_values(\"time\")\n",
    "    # daily obs/era5\n",
    "    obs_series = grp[\"obs\"].dropna().reset_index(drop=True)\n",
    "    rdrs_series = grp[\"rdrs_val\"].dropna().reset_index(drop=True)\n",
    "\n",
    "    # A) Rx1day\n",
    "    obs_rx1 = calc_rx1day(obs_series)\n",
    "    rdrs_rx1 = calc_rx1day(rdrs_series)\n",
    "    rx1_list.append({\"station_name\": st_name,\n",
    "                     \"obs_rx1day\": obs_rx1,\n",
    "                     \"rdrs_rx1day\": rdrs_rx1})\n",
    "\n",
    "    # B) Rx5day\n",
    "    obs_rx5 = calc_rx5day(obs_series)\n",
    "    rdrs_rx5 = calc_rx5day(rdrs_series)\n",
    "    rx5_list.append({\"station_name\": st_name,\n",
    "                     \"obs_rx5day\": obs_rx5,\n",
    "                     \"rdrs_rx5day\": rdrs_rx5})\n",
    "\n",
    "    # C) CDD\n",
    "    obs_cdd_val = calc_cdd(obs_series)\n",
    "    rdrs_cdd_val = calc_cdd(rdrs_series)\n",
    "    cdd_list.append({\"station_name\": st_name,\n",
    "                     \"obs_cdd\": obs_cdd_val,\n",
    "                     \"rdrs_cdd\": rdrs_cdd_val})\n",
    "\n",
    "    # D) CWD\n",
    "    obs_cwd_val = calc_cwd(obs_series)\n",
    "    rdrs_cwd_val = calc_cwd(rdrs_series)\n",
    "    cwd_list.append({\"station_name\": st_name,\n",
    "                     \"obs_cwd\": obs_cwd_val,\n",
    "                     \"rdrs_cwd\": rdrs_cwd_val})\n",
    "\n",
    "    # E) R95 / R99\n",
    "    or95a, or95p, or99a, or99p = calc_r95p_r99p(obs_series)\n",
    "    er95a, er95p, er99a, er99p = calc_r95p_r99p(rdrs_series)\n",
    "    r95_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_r95amt\": or95a, \"obs_r95pct\": or95p,\n",
    "        \"rdrs_r95amt\": er95a, \"rdrs_r95pct\": er95p\n",
    "    })\n",
    "    r99_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_r99amt\": or99a, \"obs_r99pct\": or99p,\n",
    "        \"rdrs_r99amt\": er99a, \"rdrs_r99pct\": er99p\n",
    "    })\n",
    "\n",
    "    # F) wet/dry days\n",
    "    obs_wet5, obs_dry = calc_wetdays_drydays(obs_series)\n",
    "    rdrs_wet5, rdrs_dry = calc_wetdays_drydays(rdrs_series)\n",
    "    wet_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_wetdays5mm\": obs_wet5, \n",
    "        \"rdrs_wetdays5mm\": rdrs_wet5\n",
    "    })\n",
    "    dry_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_drydays\": obs_dry,\n",
    "        \"rdrs_drydays\": rdrs_dry\n",
    "    })\n",
    "\n",
    "print(\"Finished computing. Now merging lat/lon from physical file ...\")\n",
    "\n",
    "# Convert each list to DataFrame => merge lat,lon => save\n",
    "def attach_coords(df_in):\n",
    "    \"\"\"Attach lat, lon, elev from df_phys on station_name.\"\"\"\n",
    "    df_out = pd.merge(\n",
    "        df_in, \n",
    "        df_phys[[\"station_name\",\"lat\",\"lon\",\"elev\"]],\n",
    "        on=\"station_name\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "    return df_out\n",
    "\n",
    "df_rx1 = attach_coords(pd.DataFrame(rx1_list))\n",
    "df_rx5 = attach_coords(pd.DataFrame(rx5_list))\n",
    "df_cdd = attach_coords(pd.DataFrame(cdd_list))\n",
    "df_cwd = attach_coords(pd.DataFrame(cwd_list))\n",
    "df_r95 = attach_coords(pd.DataFrame(r95_list))\n",
    "df_r99 = attach_coords(pd.DataFrame(r99_list))\n",
    "df_wet = attach_coords(pd.DataFrame(wet_list))\n",
    "df_dry = attach_coords(pd.DataFrame(dry_list))\n",
    "\n",
    "###############################################################################\n",
    "# 6. SAVE OUTPUT (SAME FILE NAMES AS BEFORE)\n",
    "###############################################################################\n",
    "print(\"Saving index tables to Excel in:\", output_dir)\n",
    "df_rx1.to_excel(os.path.join(output_dir, \"rx1day.xlsx\"),  index=False)\n",
    "df_rx5.to_excel(os.path.join(output_dir, \"rx5day.xlsx\"),  index=False)\n",
    "df_cdd.to_excel(os.path.join(output_dir, \"cdd.xlsx\"),     index=False)\n",
    "df_cwd.to_excel(os.path.join(output_dir, \"cwd.xlsx\"),     index=False)\n",
    "df_r95.to_excel(os.path.join(output_dir, \"r95p.xlsx\"),    index=False)\n",
    "df_r99.to_excel(os.path.join(output_dir, \"r99p.xlsx\"),    index=False)\n",
    "df_wet.to_excel(os.path.join(output_dir, \"wetdays.xlsx\"), index=False)\n",
    "df_dry.to_excel(os.path.join(output_dir, \"drydays.xlsx\"), index=False)\n",
    "\n",
    "print(\"\\nAll precipitation-based indices have been saved to Excel with station_name (and lat/lon).\")\n",
    "\n",
    "###############################################################################\n",
    "# (OPTIONAL) QUICK MAP EXAMPLE (like obs_rx5day)\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nQuick map example for obs_rx5day ...\")\n",
    "    # Load shapefiles\n",
    "    gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "    gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        df_rx5,\n",
    "        geometry=gpd.points_from_xy(df_rx5[\"lon\"], df_rx5[\"lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "    for geom in gdf_basin.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='blue', linewidth=1)\n",
    "    for geom in gdf_lakes.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='cyan', linewidth=1)\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "    sc = ax.scatter(gdf_stations.geometry.x, gdf_stations.geometry.y,\n",
    "                    c=gdf_stations[\"obs_rx5day\"], cmap=\"Reds\", s=60,\n",
    "                    transform=ccrs.PlateCarree(), edgecolor=\"k\")\n",
    "    plt.colorbar(sc, ax=ax, label=\"Obs Rx5day (mm)\")\n",
    "\n",
    "    ax.set_extent([-95.5, -72, 38.5, 52.5])  # approximate bounding box\n",
    "    gl = ax.gridlines(draw_labels=True, linestyle='--', color='gray')\n",
    "    gl.right_labels = False\n",
    "    gl.top_labels   = False\n",
    "\n",
    "    plt.title(\"Obs Rx5day (from CSV daily data)\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Mapping step failed:\", e)\n",
    "\n",
    "print(\"\\n✅ Done computing precipitation-based indices from CSV, with station names included!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5633ebc-b41e-4825-9f29-928cb44a285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the climatic indices for RDRS v2.1 (temperature - Tmin/Tmax)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIG\n",
    "###############################################################################\n",
    "csv_file = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\RDRS v2.1_GLB_Temperature\\daily_loop\\rdrs_vs_stations_25km_LWR_1991_2012_tmin_tmax.csv\"\n",
    "physical_file = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation_Temperature.csv\"\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "output_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\RDRS v2.1_GLB_Temperature\\ClimaticIndices-8Nearest\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY CSV DATA\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data (obs, rdrs_lwr25_val) ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(f\"df_data shape = {df_data.shape}\")\n",
    "print(\"Columns:\", df_data.columns.tolist())\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. MERGE WITH PHYSICAL FILE TO GET LAT/LON\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS – FULL ETCCDI SET FOR Tmin / Tmax\n",
    "###############################################################################\n",
    "def absolute_extremes(series, kind):\n",
    "    \"\"\"Return the single-day absolute extreme.\"\"\"\n",
    "    if kind == \"max\":\n",
    "        return series.max(skipna=True)\n",
    "    else:                       # \"min\"\n",
    "        return series.min(skipna=True)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# percentile thresholds (5-day moving window, baseline = 1991-2012)\n",
    "# -------------------------------------------------------------------------\n",
    "BASE_START, BASE_END = \"1991-01-01\", \"2012-12-31\"\n",
    "\n",
    "def _climatology_percentiles(s, p):\n",
    "    \"\"\"Return a Series (index = 1…366) of the p-th percentile.\"\"\"\n",
    "    # drop 29 Feb so every year has 365 days\n",
    "    s = s[~((s.index.month == 2) & (s.index.day == 29))]\n",
    "    df = pd.DataFrame({\"val\": s, \"doy\": s.index.dayofyear})\n",
    "    climo = []\n",
    "    for d in range(1, 366):\n",
    "        win = list(range(d-2, d+3))                       # ±2-day window\n",
    "        win = [(x-1) % 365 + 1 for x in win]              # wrap around ends\n",
    "        vals = df.loc[df[\"doy\"].isin(win), \"val\"]\n",
    "        climo.append(np.nanpercentile(vals, p) if len(vals) else np.nan)\n",
    "    return pd.Series(climo, index=range(1, 366), name=f\"p{p}\")\n",
    "\n",
    "def percentile_flags(s, perc_series, side):\n",
    "    \"\"\"Return Boolean Series: True where value is < or > percentile.\"\"\"\n",
    "    doy = s.index.dayofyear\n",
    "    thr = perc_series.reindex(doy).values\n",
    "    if side == \"low\":\n",
    "        return s < thr\n",
    "    else:\n",
    "        return s > thr\n",
    "\n",
    "def spell_length(bool_series, min_run=6):\n",
    "    \"\"\"Total # days in spells of ≥ min_run consecutive Trues.\"\"\"\n",
    "    is_true = bool_series.fillna(False).values\n",
    "    # identify run lengths\n",
    "    run_ends = np.where(np.diff(np.concatenate(([0], is_true, [0]))))[0]\n",
    "    lengths  = run_ends[1::2] - run_ends[::2]\n",
    "    return lengths[lengths >= min_run].sum()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# absolute-threshold counters\n",
    "# -------------------------------------------------------------------------\n",
    "def count_threshold(series, op, thr):\n",
    "    if op == \"<\":\n",
    "        return (series < thr).sum()\n",
    "    else:\n",
    "        return (series > thr).sum()\n",
    "\n",
    "###############################################################################\n",
    "# 5.  ANNUAL ETCCDI INDICES  –  FORMAT COMPATIBLE WITH THE SEASONAL SCRIPT\n",
    "#     • keeps:   TXx  TNn  TX90p  TN10p  FD  WSDI  CSDI\n",
    "#     • drops:   TR, ID, SU, TXn, TNx\n",
    "###############################################################################\n",
    "rows = []\n",
    "\n",
    "print(\"→ computing *annual* indices …\")\n",
    "for st_name, st_grp in df_data.groupby(\"station_name\"):\n",
    "\n",
    "    st_grp = st_grp.set_index(\"time\").sort_index()\n",
    "\n",
    "    # build daily Series ................................................................\n",
    "    obs_max = st_grp.loc[st_grp[\"var\"] == \"tmax\", \"obs\"].asfreq(\"D\")\n",
    "    obs_min = st_grp.loc[st_grp[\"var\"] == \"tmin\", \"obs\"].asfreq(\"D\")\n",
    "    rdrs_max = st_grp.loc[st_grp[\"var\"] == \"tmax\", \"rdrs_lwr25_val\"].asfreq(\"D\")\n",
    "    rdrs_min = st_grp.loc[st_grp[\"var\"] == \"tmin\", \"rdrs_lwr25_val\"].asfreq(\"D\")\n",
    "    if obs_max.empty:          # station has no data at all\n",
    "        continue\n",
    "\n",
    "    # ── fixed-year climatology (1991-2012, 5-day moving window) ─────────────-\n",
    "    p90_TX_obs = _climatology_percentiles(obs_max[BASE_START:BASE_END], 90)\n",
    "    p10_TN_obs = _climatology_percentiles(obs_min[BASE_START:BASE_END], 10)\n",
    "    p90_TX_rdrs = _climatology_percentiles(rdrs_max[BASE_START:BASE_END], 90)\n",
    "    p10_TN_rdrs = _climatology_percentiles(rdrs_min[BASE_START:BASE_END], 10)\n",
    "\n",
    "    # flags for the *full* record (faster than recomputing year-by-year)\n",
    "    flags = {\n",
    "        \"obs_TX90\": obs_max >\n",
    "                     p90_TX_obs.reindex(obs_max.index.dayofyear).values,\n",
    "        \"rdrs_TX90\": rdrs_max >\n",
    "                     p90_TX_rdrs.reindex(rdrs_max.index.dayofyear).values,\n",
    "        \"obs_TN10\": obs_min <\n",
    "                     p10_TN_obs.reindex(obs_min.index.dayofyear).values,\n",
    "        \"rdrs_TN10\": rdrs_min <\n",
    "                     p10_TN_rdrs.reindex(rdrs_min.index.dayofyear).values,\n",
    "    }\n",
    "\n",
    "    # ── iterate over years (December belongs to the following DJF year) ──────\n",
    "    years = np.unique(obs_max.index.year)\n",
    "    for yr in years:\n",
    "        mask = obs_max.index.year == yr\n",
    "        if mask.sum() < 200:         # at least ~55 % of a year\n",
    "            continue\n",
    "\n",
    "        def _sel(s):          # helper to slice one year\n",
    "            return s[s.index.year == yr]\n",
    "\n",
    "        # intensity ....................................................................\n",
    "        TXx_obs = _sel(obs_max).max()\n",
    "        TXx_rdrs = _sel(rdrs_max).max()\n",
    "        TNn_obs = _sel(obs_min).min()\n",
    "        TNn_rdrs = _sel(rdrs_min).min()\n",
    "\n",
    "        # percentile frequencies (percentage of days)\n",
    "        TX90p_obs = flags[\"obs_TX90\"][mask].mean() * 100.0\n",
    "        TX90p_rdrs = flags[\"rdrs_TX90\"][mask].mean() * 100.0\n",
    "        TN10p_obs = flags[\"obs_TN10\"][mask].mean() * 100.0\n",
    "        TN10p_rdrs = flags[\"rdrs_TN10\"][mask].mean() * 100.0\n",
    "\n",
    "        # spell duration (≥6 consecutive days)\n",
    "        WSDI_obs = spell_length(flags[\"obs_TX90\"][mask], min_run=6)\n",
    "        WSDI_rdrs = spell_length(flags[\"rdrs_TX90\"][mask], min_run=6)\n",
    "        CSDI_obs = spell_length(flags[\"obs_TN10\"][mask], min_run=6)\n",
    "        CSDI_rdrs = spell_length(flags[\"rdrs_TN10\"][mask], min_run=6)\n",
    "\n",
    "        # absolute‐threshold count\n",
    "        FD_obs = (_sel(obs_min) < 0).sum()\n",
    "        FD_rdrs = (_sel(rdrs_min) < 0).sum()\n",
    "\n",
    "        # helper for ratios (avoid /0)\n",
    "        ratio = lambda o, e: np.nan if (o == 0 or np.isnan(o)) else e / o\n",
    "\n",
    "        rows.append(dict(\n",
    "            station_name=st_name, year=yr,\n",
    "            TXx_obs=TXx_obs,   TXx_rdrs=TXx_rdrs,   TXx_ratio=ratio(TXx_obs, TXx_rdrs),\n",
    "            TNn_obs=TNn_obs,   TNn_rdrs=TNn_rdrs,   TNn_ratio=ratio(TNn_obs, TNn_rdrs),\n",
    "            TX90p_obs=TX90p_obs, TX90p_rdrs=TX90p_rdrs,\n",
    "            TX90p_ratio=ratio(TX90p_obs, TX90p_rdrs),\n",
    "            TN10p_obs=TN10p_obs, TN10p_rdrs=TN10p_rdrs,\n",
    "            TN10p_ratio=ratio(TN10p_obs, TN10p_rdrs),\n",
    "            FD_obs=FD_obs,     FD_rdrs=FD_rdrs,     FD_ratio=ratio(FD_obs, FD_rdrs),\n",
    "            WSDI_obs=WSDI_obs, WSDI_rdrs=WSDI_rdrs,\n",
    "            WSDI_ratio=ratio(WSDI_obs, WSDI_rdrs),\n",
    "            CSDI_obs=CSDI_obs, CSDI_rdrs=CSDI_rdrs,\n",
    "            CSDI_ratio=ratio(CSDI_obs, CSDI_rdrs)\n",
    "        ))\n",
    "\n",
    "df_yr = (pd.DataFrame(rows)\n",
    "         .merge(df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "                on=\"station_name\", how=\"left\")\n",
    "         .sort_values([\"station_name\", \"year\"]))\n",
    "\n",
    "###############################################################################\n",
    "# 6.  SAVE OUTPUT  – ONE PARQUET ( + XLSX )  +  OPTIONAL per-index sheets\n",
    "###############################################################################\n",
    "annual_pq  = os.path.join(output_dir, \"Indices_Annual.parquet\")\n",
    "annual_xls = annual_pq.replace(\".parquet\", \".xlsx\")\n",
    "\n",
    "df_yr.to_parquet(annual_pq, index=False)\n",
    "df_yr.to_excel  (annual_xls, index=False)\n",
    "print(f\"✓ Annual indices saved → {annual_pq}\")\n",
    "print(f\"✓ …and also saved as   → {annual_xls}\")\n",
    "\n",
    "# OPTIONAL: write one Excel file per index in the familiar “wide” format\n",
    "# ---------------------------------------------------------------------------\n",
    "index_roots = [\"TXx\", \"TNn\", \"TX90p\", \"TN10p\", \"FD\", \"WSDI\", \"CSDI\"]\n",
    "def _wide(idx_root: str) -> pd.DataFrame:\n",
    "    return (df_yr\n",
    "            .pivot_table(index=\"station_name\",\n",
    "                         values=[f\"{idx_root}_obs\", f\"{idx_root}_rdrs\",\n",
    "                                 f\"{idx_root}_ratio\"])\n",
    "            .reset_index()\n",
    "            .merge(df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "                   on=\"station_name\", how=\"left\"))\n",
    "\n",
    "print(\"\\n(optional) individual workbooks …\")\n",
    "for idx in index_roots:\n",
    "    w = _wide(idx)\n",
    "    fp = os.path.join(output_dir, f\"{idx}.xlsx\")\n",
    "    w.to_excel(fp, index=False)\n",
    "    print(\"  •\", os.path.basename(fp))\n",
    "\n",
    "print(\"\\n✅  Annual-index workflow finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d11588-faa4-4089-86d8-689fa8e3149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal stratification of climatic indices RDRS v2.1 for having the seasonal indices\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIGURATION\n",
    "###############################################################################\n",
    "csv_file      = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\RDRS v2.1_GLB_Precipitation\\daily_loop\\rdrs_vs_stations_25km_LWR_1991_2012.csv\"\n",
    "physical_file = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv\"\n",
    "output_dir    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\RDRS v2.1_GLB_Precipitation\\ClimaticIndices-Seasonal\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY DATA & ADD TEMPORAL FIELDS\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# Standardize station_name\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Add month (1..12) and season (DJF, MAM, JJA, SON)\n",
    "df_data[\"month\"] = df_data[\"time\"].dt.month\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"DJF\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"MAM\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"JJA\"\n",
    "    else:\n",
    "        return \"SON\"\n",
    "\n",
    "df_data[\"season\"] = df_data[\"month\"].apply(get_season)\n",
    "\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. LOAD PHYSICAL FILE & MERGE COORDINATES\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "###############################################################################\n",
    "def calc_rx1day(series):\n",
    "    \"\"\"Max 1-day precipitation.\"\"\"\n",
    "    return series.max(skipna=True)\n",
    "\n",
    "def calc_rx5day(series):\n",
    "    \"\"\"Max 5-day running sum.\"\"\"\n",
    "    return series.rolling(5, min_periods=1).sum().max(skipna=True)\n",
    "\n",
    "def calc_cdd(series, dry_threshold=1.0):\n",
    "    \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "    is_dry = series < dry_threshold\n",
    "    max_run, current_run = 0, 0\n",
    "    for val in is_dry:\n",
    "        if val:\n",
    "            current_run += 1\n",
    "            max_run = max(max_run, current_run)\n",
    "        else:\n",
    "            current_run = 0\n",
    "    return max_run\n",
    "\n",
    "def calc_cwd(series, wet_threshold=1.0):\n",
    "    \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "    is_wet = series >= wet_threshold\n",
    "    max_run, current_run = 0, 0\n",
    "    for val in is_wet:\n",
    "        if val:\n",
    "            current_run += 1\n",
    "            max_run = max(max_run, current_run)\n",
    "        else:\n",
    "            current_run = 0\n",
    "    return max_run\n",
    "\n",
    "def calc_r95p_r99p(series, percentiles=(95,99)):\n",
    "    \"\"\"\n",
    "    r95amt, r95pct, r99amt, r99pct:\n",
    "    - r95amt = sum of daily prcp above 95th percentile\n",
    "    - r95pct = (r95amt / total) * 100\n",
    "    - similarly for 99th percentile\n",
    "    \"\"\"\n",
    "    wet = series[series >= 1.0]\n",
    "    if len(wet) < 5:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    p95 = np.percentile(wet, percentiles[0])\n",
    "    p99 = np.percentile(wet, percentiles[1])\n",
    "    r95_amt = wet[wet > p95].sum()\n",
    "    r99_amt = wet[wet > p99].sum()\n",
    "    total = series.sum(skipna=True)\n",
    "    r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "    r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "    return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "\n",
    "def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "    \"\"\"\n",
    "    wetdays = #days >= wet_thr\n",
    "    drydays = #days < dry_thr\n",
    "    \"\"\"\n",
    "    return (series >= wet_thr).sum(), (series < dry_thr).sum()\n",
    "\n",
    "###############################################################################\n",
    "# 5. FUNCTION TO COMPUTE INDICES FOR A GROUP (MONTHLY or SEASONAL)\n",
    "###############################################################################\n",
    "def compute_indices(df_group):\n",
    "    \"\"\"\n",
    "    For a subset of daily data (e.g. station+month, or station+season),\n",
    "    compute the climate indices for Obs vs RDRS, plus ratio columns.\n",
    "    \"\"\"\n",
    "    obs_series = df_group[\"obs\"].dropna().reset_index(drop=True)\n",
    "    rdrs_series = df_group[\"rdrs_val\"].dropna().reset_index(drop=True)\n",
    "    if len(obs_series) == 0 or len(rdrs_series) == 0:\n",
    "        return None\n",
    "    \n",
    "    res = {}\n",
    "    # Rx1day / Rx5day\n",
    "    res[\"rx1day_obs\"] = calc_rx1day(obs_series)\n",
    "    res[\"rx1day_rdrs\"] = calc_rx1day(rdrs_series)\n",
    "    res[\"rx5day_obs\"] = calc_rx5day(obs_series)\n",
    "    res[\"rx5day_rdrs\"] = calc_rx5day(rdrs_series)\n",
    "    \n",
    "    # CDD / CWD\n",
    "    res[\"cdd_obs\"] = calc_cdd(obs_series)\n",
    "    res[\"cdd_rdrs\"] = calc_cdd(rdrs_series)\n",
    "    res[\"cwd_obs\"] = calc_cwd(obs_series)\n",
    "    res[\"cwd_rdrs\"] = calc_cwd(rdrs_series)\n",
    "    \n",
    "    # R95 / R99\n",
    "    r95_obs = calc_r95p_r99p(obs_series)\n",
    "    r95_rdrs = calc_r95p_r99p(rdrs_series)\n",
    "    res[\"r95amt_obs\"] = r95_obs[0]\n",
    "    res[\"r95pct_obs\"] = r95_obs[1]\n",
    "    res[\"r95amt_rdrs\"] = r95_rdrs[0]\n",
    "    res[\"r95pct_rdrs\"] = r95_rdrs[1]\n",
    "    res[\"r99amt_obs\"] = r95_obs[2]\n",
    "    res[\"r99pct_obs\"] = r95_obs[3]\n",
    "    res[\"r99amt_rdrs\"] = r95_rdrs[2]\n",
    "    res[\"r99pct_rdrs\"] = r95_rdrs[3]\n",
    "    \n",
    "    # Wet / Dry days\n",
    "    wet_obs, dry_obs = calc_wetdays_drydays(obs_series)\n",
    "    wet_rdrs, dry_rdrs = calc_wetdays_drydays(rdrs_series)\n",
    "    res[\"wetdays_obs\"] = wet_obs\n",
    "    res[\"wetdays_rdrs\"] = wet_rdrs\n",
    "    res[\"drydays_obs\"] = dry_obs\n",
    "    res[\"drydays_rdrs\"] = dry_rdrs\n",
    "    \n",
    "    # Ratio columns: RDRS/OBS if obs != 0\n",
    "    if res[\"rx1day_obs\"]:\n",
    "        res[\"rx1day_ratio\"] = res[\"rx1day_rdrs\"] / res[\"rx1day_obs\"]\n",
    "    if res[\"rx5day_obs\"]:\n",
    "        res[\"rx5day_ratio\"] = res[\"rx5day_rdrs\"] / res[\"rx5day_obs\"]\n",
    "    if res[\"cdd_obs\"]:\n",
    "        res[\"cdd_ratio\"] = res[\"cdd_rdrs\"] / res[\"cdd_obs\"]\n",
    "    if res[\"cwd_obs\"]:\n",
    "        res[\"cwd_ratio\"] = res[\"cwd_rdrs\"] / res[\"cwd_obs\"]\n",
    "    if res[\"r95amt_obs\"]:\n",
    "        res[\"r95amt_ratio\"] = res[\"r95amt_rdrs\"] / res[\"r95amt_obs\"]\n",
    "    if res[\"r95pct_obs\"]:\n",
    "        res[\"r95pct_ratio\"] = res[\"r95pct_rdrs\"] / res[\"r95pct_obs\"]\n",
    "    if res[\"r99amt_obs\"]:\n",
    "        res[\"r99amt_ratio\"] = res[\"r99amt_rdrs\"] / res[\"r99amt_obs\"]\n",
    "    if res[\"r99pct_obs\"]:\n",
    "        res[\"r99pct_ratio\"] = res[\"r99pct_rdrs\"] / res[\"r99pct_obs\"]\n",
    "    if res[\"wetdays_obs\"]:\n",
    "        res[\"wetdays_ratio\"] = res[\"wetdays_rdrs\"] / res[\"wetdays_obs\"]\n",
    "    if res[\"drydays_obs\"]:\n",
    "        res[\"drydays_ratio\"] = res[\"drydays_rdrs\"] / res[\"drydays_obs\"]\n",
    "    \n",
    "    return res\n",
    "\n",
    "###############################################################################\n",
    "# 6. MONTHLY INDICES\n",
    "###############################################################################\n",
    "monthly_results = []\n",
    "group_month = df_data.groupby([\"station_name\", \"month\"])\n",
    "for (st_name, mon), group in group_month:\n",
    "    indices = compute_indices(group)\n",
    "    if indices is None:\n",
    "        continue\n",
    "    indices[\"station_name\"] = st_name\n",
    "    indices[\"month\"] = mon\n",
    "    monthly_results.append(indices)\n",
    "\n",
    "df_monthly = pd.DataFrame(monthly_results)\n",
    "df_monthly = pd.merge(\n",
    "    df_monthly,\n",
    "    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_monthly = df_monthly.sort_values([\"station_name\", \"month\"])\n",
    "monthly_out = os.path.join(output_dir, \"Indices_Monthly.xlsx\")\n",
    "df_monthly.to_excel(monthly_out, index=False)\n",
    "print(\"Monthly indices saved =>\", monthly_out)\n",
    "\n",
    "###############################################################################\n",
    "# 7. SEASONAL INDICES\n",
    "###############################################################################\n",
    "seasonal_results = []\n",
    "group_season = df_data.groupby([\"station_name\", \"season\"])\n",
    "for (st_name, seas), group in group_season:\n",
    "    indices = compute_indices(group)\n",
    "    if indices is None:\n",
    "        continue\n",
    "    indices[\"station_name\"] = st_name\n",
    "    indices[\"season\"] = seas\n",
    "    seasonal_results.append(indices)\n",
    "\n",
    "df_seasonal = pd.DataFrame(seasonal_results)\n",
    "df_seasonal = pd.merge(\n",
    "    df_seasonal,\n",
    "    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_seasonal = df_seasonal.sort_values([\"station_name\", \"season\"])\n",
    "seasonal_out = os.path.join(output_dir, \"Indices_Seasonal.xlsx\")\n",
    "df_seasonal.to_excel(seasonal_out, index=False)\n",
    "print(\"Seasonal indices saved =>\", seasonal_out)\n",
    "\n",
    "###############################################################################\n",
    "# 8. DONE\n",
    "###############################################################################\n",
    "print(\"\\nAll monthly and seasonal indices have been saved. (No extreme-event stratification.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638789f-ad70-4e6c-a7cf-0d1cb0dec424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DJF for RDRS v2.1\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import Point\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIG & PATHS\n",
    "###############################################################################\n",
    "indices_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\RDRS v2.1_GLB_Precipitation\\ClimaticIndices-Seasonal\"\n",
    "seasonal_file = os.path.join(indices_dir, \"Indices_Seasonal.xlsx\")  # single file\n",
    "output_plots  = os.path.join(indices_dir, \"AnalysisPlots_DJF\")\n",
    "os.makedirs(output_plots, exist_ok=True)\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "# Indices in your seasonal file\n",
    "index_list = [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"r95p\",\"r99p\",\"wetdays\",\"drydays\"]\n",
    "\n",
    "# For summary stats, define how to find obs vs. rdrs columns\n",
    "index_columns = {\n",
    "    \"rx1day\":  (\"rx1day_obs\",  \"rx1day_rdrs\"),\n",
    "    \"rx5day\":  (\"rx5day_obs\",  \"rx5day_rdrs\"),\n",
    "    \"cdd\":     (\"cdd_obs\",     \"cdd_rdrs\"),\n",
    "    \"cwd\":     (\"cwd_obs\",     \"cwd_rdrs\"),\n",
    "    \"r95p\":    ((\"r95amt_obs\",\"r95pct_obs\"), (\"r95amt_rdrs\",\"r95pct_rdrs\")),\n",
    "    \"r99p\":    ((\"r99amt_obs\",\"r99pct_obs\"), (\"r99amt_rdrs\",\"r99pct_rdrs\")),\n",
    "    \"wetdays\": (\"wetdays_obs\",\"wetdays_rdrs\"),\n",
    "    \"drydays\": (\"drydays_obs\",\"drydays_rdrs\"),\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD SEASONAL FILE & FILTER TO DJF\n",
    "###############################################################################\n",
    "df_season = pd.read_excel(seasonal_file)\n",
    "print(\"Loaded =>\", seasonal_file, \"| shape =\", df_season.shape)\n",
    "\n",
    "# Filter to DJF\n",
    "df_season = df_season[df_season[\"season\"]==\"DJF\"].copy()\n",
    "df_season = df_season.dropna(subset=[\"lat\",\"lon\"])  # ensure lat/lon exist\n",
    "print(\"After filtering to DJF => shape =\", df_season.shape)\n",
    "\n",
    "mdf = df_season.reset_index(drop=True)\n",
    "master_xlsx = os.path.join(output_plots, \"MasterTable_Seasonal_DJF.xlsx\")\n",
    "mdf.to_excel(master_xlsx, index=False)\n",
    "print(f\"\\n(A) Master table (DJF) saved => {master_xlsx}\")\n",
    "print(\"Columns:\", mdf.columns.tolist())\n",
    "\n",
    "###############################################################################\n",
    "# 3. SUMMARY TABLE (MBE, RMSE, STD, CC, d) for DJF\n",
    "###############################################################################\n",
    "def index_of_agreement(obs, model):\n",
    "    obs_mean = np.mean(obs)\n",
    "    num = np.sum((model - obs)**2)\n",
    "    den = np.sum((abs(model - obs_mean) + abs(obs - obs_mean))**2)\n",
    "    if den == 0:\n",
    "        return np.nan\n",
    "    return 1 - num/den\n",
    "\n",
    "def rmse(a, b):\n",
    "    return np.sqrt(np.mean((a-b)**2))\n",
    "\n",
    "def std_of_residuals(a, b):\n",
    "    return np.std(a-b, ddof=1)\n",
    "\n",
    "def mean_bias_error(a, b):\n",
    "    return np.mean(b-a)\n",
    "\n",
    "summary_rows = []\n",
    "for idx_name in index_list:\n",
    "    obs_cols = index_columns[idx_name][0]\n",
    "    rdrs_cols = index_columns[idx_name][1]\n",
    "\n",
    "    if isinstance(obs_cols, tuple):\n",
    "        # multiple columns\n",
    "        for oc, ec in zip(obs_cols, rdrs_cols):\n",
    "            valid = mdf[[oc, ec]].dropna()\n",
    "            if len(valid) < 2:\n",
    "                continue\n",
    "            obs_vals = valid[oc].values\n",
    "            rdrs_vals = valid[ec].values\n",
    "            MB  = mean_bias_error(obs_vals, rdrs_vals)\n",
    "            RM  = rmse(obs_vals, rdrs_vals)\n",
    "            SR  = std_of_residuals(obs_vals, rdrs_vals)\n",
    "            CC  = pearsonr(obs_vals, rdrs_vals)[0] if len(obs_vals) > 1 else np.nan\n",
    "            dd  = index_of_agreement(obs_vals, rdrs_vals)\n",
    "            idx_label = f\"{idx_name}_{oc.replace('_obs','')}\"\n",
    "            summary_rows.append({\n",
    "                \"Index\": idx_label,\n",
    "                \"Count\": len(valid),\n",
    "                \"MBE\": MB,\n",
    "                \"RMSE\": RM,\n",
    "                \"STDres\": SR,\n",
    "                \"CC\": CC,\n",
    "                \"d\": dd,\n",
    "            })\n",
    "    else:\n",
    "        oc = obs_cols\n",
    "        ec = rdrs_cols\n",
    "        valid = mdf[[oc, ec]].dropna()\n",
    "        if len(valid) < 2:\n",
    "            continue\n",
    "        obs_vals = valid[oc].values\n",
    "        rdrs_vals = valid[ec].values\n",
    "        MB = mean_bias_error(obs_vals, rdrs_vals)\n",
    "        RM = rmse(obs_vals, rdrs_vals)\n",
    "        SR = std_of_residuals(obs_vals, rdrs_vals)\n",
    "        CC = pearsonr(obs_vals, rdrs_vals)[0] if len(obs_vals) > 1 else np.nan\n",
    "        dd = index_of_agreement(obs_vals, rdrs_vals)\n",
    "        summary_rows.append({\n",
    "            \"Index\": idx_name,\n",
    "            \"Count\": len(valid),\n",
    "            \"MBE\": MB,\n",
    "            \"RMSE\": RM,\n",
    "            \"STDres\": SR,\n",
    "            \"CC\": CC,\n",
    "            \"d\": dd,\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_cols = [\"Index\",\"Count\",\"MBE\",\"RMSE\",\"STDres\",\"CC\",\"d\"]\n",
    "summary_df = summary_df[summary_cols]\n",
    "summary_xlsx = os.path.join(output_plots, \"SummaryTable_Extremes_DJF.xlsx\")\n",
    "summary_df.to_excel(summary_xlsx, index=False)\n",
    "print(f\"(B) Summary Table (DJF) => {summary_xlsx}\\n{summary_df}\")\n",
    "\n",
    "###############################################################################\n",
    "# 4. MAPPING: Combine Observed, RDRS v2.1, Ratio in One Figure\n",
    "###############################################################################\n",
    "gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "\n",
    "def add_basin_lakes(ax):\n",
    "    #ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    for geom in gdf_basin.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(),\n",
    "                          facecolor='none', edgecolor='black', linewidth=1)\n",
    "    for geom in gdf_lakes.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(),\n",
    "                          facecolor='none', edgecolor='cyan', linewidth=1)\n",
    "\n",
    "def plot_map_triptych(df, obs_col, rdrs_col, ratio_col, idx_name, out_png):\n",
    "    \"\"\"\n",
    "    Creates a single figure with 3 subplots (side-by-side):\n",
    "      1) Observed\n",
    "      2) RDRS v2.1\n",
    "      3) Ratio (RDRS/OBS)\n",
    "    Each subplot has a colorbar, a 90th-percentile hotspot circle, etc.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6),\n",
    "                             subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "\n",
    "    # We'll define a small helper to do each subplot\n",
    "    def scatter_map(ax, value_col, title):\n",
    "        ax.set_extent([-95.5, -72, 38.5, 52.5])\n",
    "        add_basin_lakes(ax)\n",
    "        sc = ax.scatter(df[\"lon\"], df[\"lat\"], c=df[value_col], cmap=\"viridis\",\n",
    "                        s=60, transform=ccrs.PlateCarree(), edgecolor=\"k\", zorder=10)\n",
    "        cb = plt.colorbar(sc, ax=ax, shrink=0.8)\n",
    "        cb.set_label(value_col)\n",
    "\n",
    "        # Hotspots => top 10%\n",
    "        vals = df[value_col].dropna().values\n",
    "        if len(vals) > 0:\n",
    "            thr = np.percentile(vals, 90)\n",
    "            is_hot = df[value_col]>=thr\n",
    "            ax.scatter(df.loc[is_hot,\"lon\"], df.loc[is_hot,\"lat\"],\n",
    "                       marker='o', facecolors='none', edgecolors='red', s=80,\n",
    "                       transform=ccrs.PlateCarree(), zorder=11,\n",
    "                       label=f\"Hotspot >= {thr:.2f}\")\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        gl = ax.gridlines(draw_labels=True, linestyle='--', color='gray')\n",
    "        gl.right_labels = False\n",
    "        gl.top_labels   = False\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    scatter_map(axes[0], obs_col,  f\"{idx_name} Observed (DJF)\")\n",
    "    scatter_map(axes[1], rdrs_col,  f\"{idx_name} RDRS (DJF)\")\n",
    "    scatter_map(axes[2], ratio_col,f\"{idx_name} (RDRS/OBS) (DJF)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved =>\", out_png)\n",
    "\n",
    "def get_map_cols(idx_name):\n",
    "    if idx_name in [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"drydays\"]:\n",
    "        obs  = f\"{idx_name}_obs\"\n",
    "        rdrs  = f\"{idx_name}_rdrs\"\n",
    "        ratio= f\"{idx_name}_ratio\"\n",
    "        return obs, rdrs, ratio\n",
    "    elif idx_name == \"wetdays\":\n",
    "        obs  = \"wetdays_obs\"\n",
    "        rdrs  = \"wetdays_rdrs\"\n",
    "        ratio= \"wetdays_ratio\"\n",
    "        return obs, rdrs, ratio\n",
    "    elif idx_name == \"r95p\":\n",
    "        obs  = \"r95amt_obs\"\n",
    "        rdrs  = \"r95amt_rdrs\"\n",
    "        ratio= \"r95amt_ratio\"\n",
    "        return obs, rdrs, ratio\n",
    "    elif idx_name == \"r99p\":\n",
    "        obs  = \"r99amt_obs\"\n",
    "        rdrs  = \"r99amt_rdrs\"\n",
    "        ratio= \"r99amt_ratio\"\n",
    "        return obs, rdrs, ratio\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "for idx_name in index_list:\n",
    "    obs_col, rdrs_col, ratio_col = get_map_cols(idx_name)\n",
    "    if obs_col is None:\n",
    "        continue\n",
    "\n",
    "    needed_cols = [obs_col, rdrs_col, ratio_col, \"lat\", \"lon\"]\n",
    "    if not all(c in mdf.columns for c in needed_cols):\n",
    "        print(f\"Skipping map for {idx_name} - missing columns.\")\n",
    "        continue\n",
    "\n",
    "    subdf = mdf.dropna(subset=[\"lat\",\"lon\"]).copy()\n",
    "    out_png = os.path.join(output_plots, f\"DJF_{idx_name}_MAP_3panel.png\")\n",
    "    plot_map_triptych(subdf, obs_col, rdrs_col, ratio_col, idx_name, out_png)\n",
    "\n",
    "###############################################################################\n",
    "# 5. DISTRIBUTION & BOX/CDF/Scatter in One Figure\n",
    "###############################################################################\n",
    "def plot_distribution_triptych(df, obs_col, rdrs_col, label, out_png):\n",
    "    \"\"\"\n",
    "    Creates a single figure with 3 subplots side-by-side:\n",
    "      1) Boxplot\n",
    "      2) CDF\n",
    "      3) Scatter\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,6))\n",
    "\n",
    "    # A) Boxplot\n",
    "    ax_box = axes[0]\n",
    "    data = pd.DataFrame({\"Obs\": df[obs_col], \"RDRS\": df[rdrs_col]}).melt(\n",
    "        var_name=\"Dataset\", value_name=label\n",
    "    )\n",
    "    sns.boxplot(data=data, x=\"Dataset\", y=label, ax=ax_box)\n",
    "    ax_box.set_title(f\"Boxplot: {label} (DJF)\")\n",
    "\n",
    "    # B) CDF\n",
    "    ax_cdf = axes[1]\n",
    "    obs_vals = df[obs_col].dropna()\n",
    "    rdrs_vals = df[rdrs_col].dropna()\n",
    "\n",
    "    def ecdf(x):\n",
    "        xs = np.sort(x)\n",
    "        ys = np.arange(1, len(xs)+1)/len(xs)\n",
    "        return xs, ys\n",
    "\n",
    "    if len(obs_vals)>=2 and len(rdrs_vals)>=2:\n",
    "        xs_o, ys_o = ecdf(obs_vals)\n",
    "        xs_e, ys_e = ecdf(rdrs_vals)\n",
    "        ax_cdf.plot(xs_o, ys_o, label=\"Obs\")\n",
    "        ax_cdf.plot(xs_e, ys_e, label=\"RDRS\")\n",
    "        ax_cdf.set_title(f\"CDF of {label} (DJF)\")\n",
    "        ax_cdf.set_xlabel(label)\n",
    "        ax_cdf.set_ylabel(\"Probability\")\n",
    "        ax_cdf.legend()\n",
    "    else:\n",
    "        ax_cdf.set_title(f\"CDF: not enough data ({label})\")\n",
    "\n",
    "    # C) Scatter\n",
    "    ax_scat = axes[2]\n",
    "    valid = df[[obs_col, rdrs_col]].dropna()\n",
    "    if len(valid)>=2:\n",
    "        x = valid[obs_col]\n",
    "        y = valid[rdrs_col]\n",
    "        cc, _ = pearsonr(x, y)\n",
    "        ax_scat.scatter(x, y, edgecolors='k', alpha=0.7)\n",
    "        mn, mx = np.nanmin([x.min(), y.min()]), np.nanmax([x.max(), y.max()])\n",
    "        ax_scat.plot([mn, mx],[mn, mx],'r--')\n",
    "        ax_scat.set_xlabel(f\"Obs {label} (DJF)\")\n",
    "        ax_scat.set_ylabel(f\"RDRS {label} (DJF)\")\n",
    "        ax_scat.set_title(f\"{label} (Corr={cc:.2f}, DJF)\")\n",
    "    else:\n",
    "        ax_scat.set_title(f\"Scatter: not enough data ({label})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved =>\", out_png)\n",
    "\n",
    "for idx_name in index_list:\n",
    "    # figure out obs, emd columns\n",
    "    if idx_name in [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"drydays\"]:\n",
    "        obs_col  = f\"{idx_name}_obs\"\n",
    "        rdrs_col  = f\"{idx_name}_rdrs\"\n",
    "    elif idx_name == \"wetdays\":\n",
    "        obs_col  = \"wetdays_obs\"\n",
    "        rdrs_col  = \"wetdays_rdrs\"\n",
    "    elif idx_name == \"r95p\":\n",
    "        obs_col  = \"r95amt_obs\"\n",
    "        rdrs_col  = \"r95amt_rdrs\"\n",
    "    elif idx_name == \"r99p\":\n",
    "        obs_col  = \"r99amt_obs\"\n",
    "        rdrs_col  = \"r99amt_rdrs\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    if obs_col not in mdf.columns or rdrs_col not in mdf.columns:\n",
    "        print(f\"Skipping distribution for {idx_name} - missing columns.\")\n",
    "        continue\n",
    "\n",
    "    subdf = mdf[[obs_col, rdrs_col]].dropna()\n",
    "    if len(subdf)<2:\n",
    "        print(f\"Skipping distribution for {idx_name} - not enough data.\")\n",
    "        continue\n",
    "\n",
    "    out_3panel = os.path.join(output_plots, f\"DJF_{idx_name}_Distribution_3panel.png\")\n",
    "    plot_distribution_triptych(subdf, obs_col, rdrs_col, idx_name, out_3panel)\n",
    "\n",
    "###############################################################################\n",
    "# 6. DONE\n",
    "###############################################################################\n",
    "print(\"\\nAll DJF steps completed! See outputs in:\", output_plots)\n",
    "\n",
    "## For the other seasons, just change any DJF to JJA, MAM, or SON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b0dfa-912c-48dc-9101-805a46701ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "                 #########################                                           ##############################\n",
    "                 #########################                  ERA5                     ##############################\n",
    "                 #########################                                           ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b80fdd-4cab-4adf-84ba-83b98768f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the climatic indices for ERA5 (prcp)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIG\n",
    "###############################################################################\n",
    "csv_file       = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Total_Precipitaion\\Modified\\daily_loop\\era5_vs_stations_8Nearest_LWR_1991_2012.csv\"\n",
    "physical_file  = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv\"\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "output_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Total_Precipitaion\\Modified\\ClimaticIndices-8Nearest\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY CSV DATA\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data (obs, era5_lwr8_val) ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "# parse 'time' as datetime if needed\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# unify station_name: remove leading/trailing spaces, uppercase\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(f\"df_data shape = {df_data.shape}\")\n",
    "print(\"Columns:\", df_data.columns.tolist())\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. (OPTIONAL) MERGE WITH PHYSICAL FILE TO GET LAT/LON\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "# unify station_name\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "###############################################################################\n",
    "def calc_rx1day(series):\n",
    "    \"\"\"Max 1-day precipitation.\"\"\"\n",
    "    return series.max(skipna=True)\n",
    "\n",
    "def calc_rx5day(series):\n",
    "    \"\"\"Max 5-day running sum.\"\"\"\n",
    "    roll_5 = series.rolling(5, min_periods=1).sum()\n",
    "    return roll_5.max(skipna=True)\n",
    "\n",
    "def calc_cdd(series, dry_threshold=1.0):\n",
    "    \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "    is_dry = series < dry_threshold\n",
    "    maxr, curr = 0, 0\n",
    "    for val in is_dry:\n",
    "        if val:\n",
    "            curr += 1\n",
    "            maxr = max(maxr, curr)\n",
    "        else:\n",
    "            curr = 0\n",
    "    return maxr\n",
    "\n",
    "def calc_cwd(series, wet_threshold=1.0):\n",
    "    \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "    is_wet = series >= wet_threshold\n",
    "    maxr, curr = 0, 0\n",
    "    for val in is_wet:\n",
    "        if val:\n",
    "            curr += 1\n",
    "            maxr = max(maxr, curr)\n",
    "        else:\n",
    "            curr = 0\n",
    "    return maxr\n",
    "\n",
    "def calc_r95p_r99p(series, percentile=(95,99)):\n",
    "    \"\"\"R95p, R99p TOT in mm, plus percentage of total.\"\"\"\n",
    "    # only wet days >=1 mm for percentile\n",
    "    wet = series[series >= 1.0]\n",
    "    if len(wet) < 5:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    p95 = np.percentile(wet, percentile[0])\n",
    "    p99 = np.percentile(wet, percentile[1])\n",
    "    r95_amt = wet[wet > p95].sum()\n",
    "    r99_amt = wet[wet > p99].sum()\n",
    "    total   = series.sum(skipna=True)\n",
    "    r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "    r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "    return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "\n",
    "def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "    \"\"\"Count #wet days >=5 mm, #dry days <1 mm.\"\"\"\n",
    "    w = (series >= wet_thr).sum()\n",
    "    d = (series <  dry_thr).sum()\n",
    "    return w, d\n",
    "\n",
    "###############################################################################\n",
    "# 5. COMPUTE INDICES FOR EACH STATION\n",
    "###############################################################################\n",
    "rx1_list, rx5_list = [], []\n",
    "cdd_list, cwd_list = [], []\n",
    "r95_list, r99_list = [], []\n",
    "wet_list, dry_list = [], []\n",
    "\n",
    "print(\"Computing indices for each station...\")\n",
    "\n",
    "grouped = df_data.groupby(\"station_name\", as_index=False)\n",
    "for st_name, grp in grouped:\n",
    "    # Sort by time (just in case)\n",
    "    grp = grp.sort_values(\"time\")\n",
    "\n",
    "    # daily obs/era5\n",
    "    obs_series = grp[\"obs\"].dropna().reset_index(drop=True)\n",
    "    era5_series = grp[\"era5_lwr8_val\"].dropna().reset_index(drop=True)  # <--- REFERENCE CHANGED HERE\n",
    "\n",
    "    # A) Rx1day\n",
    "    obs_rx1 = calc_rx1day(obs_series)\n",
    "    era5_rx1 = calc_rx1day(era5_series)\n",
    "    rx1_list.append({\"station_name\": st_name,\n",
    "                     \"obs_rx1day\": obs_rx1,\n",
    "                     \"era5_rx1day\": era5_rx1})\n",
    "\n",
    "    # B) Rx5day\n",
    "    obs_rx5 = calc_rx5day(obs_series)\n",
    "    era5_rx5 = calc_rx5day(era5_series)\n",
    "    rx5_list.append({\"station_name\": st_name,\n",
    "                     \"obs_rx5day\": obs_rx5,\n",
    "                     \"era5_rx5day\": era5_rx5})\n",
    "\n",
    "    # C) CDD\n",
    "    obs_cdd_val = calc_cdd(obs_series)\n",
    "    era5_cdd_val = calc_cdd(era5_series)\n",
    "    cdd_list.append({\"station_name\": st_name,\n",
    "                     \"obs_cdd\": obs_cdd_val,\n",
    "                     \"era5_cdd\": era5_cdd_val})\n",
    "\n",
    "    # D) CWD\n",
    "    obs_cwd_val = calc_cwd(obs_series)\n",
    "    era5_cwd_val = calc_cwd(era5_series)\n",
    "    cwd_list.append({\"station_name\": st_name,\n",
    "                     \"obs_cwd\": obs_cwd_val,\n",
    "                     \"era5_cwd\": era5_cwd_val})\n",
    "\n",
    "    # E) R95 / R99\n",
    "    or95a, or95p, or99a, or99p = calc_r95p_r99p(obs_series)\n",
    "    er95a, er95p, er99a, er99p = calc_r95p_r99p(era5_series)\n",
    "    r95_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_r95amt\": or95a, \"obs_r95pct\": or95p,\n",
    "        \"era5_r95amt\": er95a, \"era5_r95pct\": er95p\n",
    "    })\n",
    "    r99_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_r99amt\": or99a, \"obs_r99pct\": or99p,\n",
    "        \"era5_r99amt\": er99a, \"era5_r99pct\": er99p\n",
    "    })\n",
    "\n",
    "    # F) wet/dry days\n",
    "    obs_wet5, obs_dry = calc_wetdays_drydays(obs_series)\n",
    "    era5_wet5, era5_dry = calc_wetdays_drydays(era5_series)\n",
    "    wet_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_wetdays5mm\": obs_wet5, \n",
    "        \"era5_wetdays5mm\": era5_wet5\n",
    "    })\n",
    "    dry_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_drydays\": obs_dry,\n",
    "        \"era5_drydays\": era5_dry\n",
    "    })\n",
    "\n",
    "print(\"Finished computing. Now merging lat/lon from physical file ...\")\n",
    "\n",
    "def attach_coords(df_in):\n",
    "    \"\"\"Attach lat, lon, elev from df_phys on station_name.\"\"\"\n",
    "    df_out = pd.merge(\n",
    "        df_in, \n",
    "        df_phys[[\"station_name\",\"lat\",\"lon\",\"elev\"]],\n",
    "        on=\"station_name\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "    return df_out\n",
    "\n",
    "df_rx1 = attach_coords(pd.DataFrame(rx1_list))\n",
    "df_rx5 = attach_coords(pd.DataFrame(rx5_list))\n",
    "df_cdd = attach_coords(pd.DataFrame(cdd_list))\n",
    "df_cwd = attach_coords(pd.DataFrame(cwd_list))\n",
    "df_r95 = attach_coords(pd.DataFrame(r95_list))\n",
    "df_r99 = attach_coords(pd.DataFrame(r99_list))\n",
    "df_wet = attach_coords(pd.DataFrame(wet_list))\n",
    "df_dry = attach_coords(pd.DataFrame(dry_list))\n",
    "\n",
    "###############################################################################\n",
    "# 6. SAVE OUTPUT\n",
    "###############################################################################\n",
    "print(\"Saving index tables to Excel in:\", output_dir)\n",
    "df_rx1.to_excel(os.path.join(output_dir, \"rx1day.xlsx\"),  index=False)\n",
    "df_rx5.to_excel(os.path.join(output_dir, \"rx5day.xlsx\"),  index=False)\n",
    "df_cdd.to_excel(os.path.join(output_dir, \"cdd.xlsx\"),     index=False)\n",
    "df_cwd.to_excel(os.path.join(output_dir, \"cwd.xlsx\"),     index=False)\n",
    "df_r95.to_excel(os.path.join(output_dir, \"r95p.xlsx\"),    index=False)\n",
    "df_r99.to_excel(os.path.join(output_dir, \"r99p.xlsx\"),    index=False)\n",
    "df_wet.to_excel(os.path.join(output_dir, \"wetdays.xlsx\"), index=False)\n",
    "df_dry.to_excel(os.path.join(output_dir, \"drydays.xlsx\"), index=False)\n",
    "\n",
    "print(\"\\nAll precipitation-based indices have been saved to Excel.\")\n",
    "\n",
    "###############################################################################\n",
    "# (OPTIONAL) QUICK MAP EXAMPLE\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nQuick map example for obs_rx5day ...\")\n",
    "    # Load shapefiles\n",
    "    gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "    gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        df_rx5,\n",
    "        geometry=gpd.points_from_xy(df_rx5[\"lon\"], df_rx5[\"lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "    for geom in gdf_basin.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='blue', linewidth=1)\n",
    "    for geom in gdf_lakes.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='cyan', linewidth=1)\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "    sc = ax.scatter(gdf_stations.geometry.x, gdf_stations.geometry.y,\n",
    "                    c=gdf_stations[\"obs_rx5day\"], cmap=\"Reds\", s=60,\n",
    "                    transform=ccrs.PlateCarree(), edgecolor=\"k\")\n",
    "    plt.colorbar(sc, ax=ax, label=\"Obs Rx5day (mm)\")\n",
    "    ax.set_extent([-95.5, -72, 38.5, 52.5])  # approximate bounding box\n",
    "    gl = ax.gridlines(draw_labels=True, linestyle='--', color='gray')\n",
    "    gl.right_labels = False\n",
    "    gl.top_labels   = False\n",
    "\n",
    "    plt.title(\"Obs Rx5day (from CSV daily data)\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Mapping step failed:\", e)\n",
    "\n",
    "print(\"\\n✅ Done computing precipitation-based indices from 'era5_lwr8_val' column!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b186a-42d5-4369-83d6-c2ec91bbf346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the climatic indices for ERA5 (temperature (tmin-tmax))\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIG\n",
    "###############################################################################\n",
    "csv_file = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Temperature\\Temperature ERA5\\tmin-tmax\\daily_loop\\era5_vs_stations_8Nearest_LWR_1991_2012_tmin_tmax.csv\"\n",
    "physical_file = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation_Temperature.csv\"\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "output_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Temperature\\Temperature ERA5\\tmin-tmax\\ClimaticIndices-8Nearest\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY CSV DATA\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data (obs, era5_lwr8_val) ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(f\"df_data shape = {df_data.shape}\")\n",
    "print(\"Columns:\", df_data.columns.tolist())\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. MERGE WITH PHYSICAL FILE TO GET LAT/LON\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS – FULL ETCCDI SET FOR Tmin / Tmax\n",
    "###############################################################################\n",
    "def absolute_extremes(series, kind):\n",
    "    \"\"\"Return the single-day absolute extreme.\"\"\"\n",
    "    if kind == \"max\":\n",
    "        return series.max(skipna=True)\n",
    "    else:                       # \"min\"\n",
    "        return series.min(skipna=True)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# percentile thresholds (5-day moving window, baseline = 1991-2012)\n",
    "# -------------------------------------------------------------------------\n",
    "BASE_START, BASE_END = \"1991-01-01\", \"2012-12-31\"\n",
    "\n",
    "def _climatology_percentiles(s, p):\n",
    "    \"\"\"Return a Series (index = 1…366) of the p-th percentile.\"\"\"\n",
    "    # drop 29 Feb so every year has 365 days\n",
    "    s = s[~((s.index.month == 2) & (s.index.day == 29))]\n",
    "    df = pd.DataFrame({\"val\": s, \"doy\": s.index.dayofyear})\n",
    "    climo = []\n",
    "    for d in range(1, 366):\n",
    "        win = list(range(d-2, d+3))                       # ±2-day window\n",
    "        win = [(x-1) % 365 + 1 for x in win]              # wrap around ends\n",
    "        vals = df.loc[df[\"doy\"].isin(win), \"val\"]\n",
    "        climo.append(np.nanpercentile(vals, p) if len(vals) else np.nan)\n",
    "    return pd.Series(climo, index=range(1, 366), name=f\"p{p}\")\n",
    "\n",
    "def percentile_flags(s, perc_series, side):\n",
    "    \"\"\"Return Boolean Series: True where value is < or > percentile.\"\"\"\n",
    "    doy = s.index.dayofyear\n",
    "    thr = perc_series.reindex(doy).values\n",
    "    if side == \"low\":\n",
    "        return s < thr\n",
    "    else:\n",
    "        return s > thr\n",
    "\n",
    "def spell_length(bool_series, min_run=6):\n",
    "    \"\"\"Total # days in spells of ≥ min_run consecutive Trues.\"\"\"\n",
    "    is_true = bool_series.fillna(False).values\n",
    "    # identify run lengths\n",
    "    run_ends = np.where(np.diff(np.concatenate(([0], is_true, [0]))))[0]\n",
    "    lengths  = run_ends[1::2] - run_ends[::2]\n",
    "    return lengths[lengths >= min_run].sum()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# absolute-threshold counters\n",
    "# -------------------------------------------------------------------------\n",
    "def count_threshold(series, op, thr):\n",
    "    if op == \"<\":\n",
    "        return (series < thr).sum()\n",
    "    else:\n",
    "        return (series > thr).sum()\n",
    "\n",
    "###############################################################################\n",
    "# 5. COMPUTE INDICES FOR EACH STATION (OBS & ERA5 – Tmin / Tmax)\n",
    "###############################################################################\n",
    "indices_rows = []\n",
    "\n",
    "print(\"Computing climatic indices …\")\n",
    "for st_name, grp in df_data.groupby(\"station_name\"):\n",
    "\n",
    "    # split the long table into two wide daily Series --------------------------------\n",
    "    grp      = grp.set_index(\"time\")\n",
    "    obs_max  = grp.loc[grp[\"var\"]==\"tmax\", \"obs\"].asfreq(\"D\")\n",
    "    obs_min  = grp.loc[grp[\"var\"]==\"tmin\", \"obs\"].asfreq(\"D\")\n",
    "    era_max  = grp.loc[grp[\"var\"]==\"tmax\", \"era5_lwr8_val\"].asfreq(\"D\")\n",
    "    era_min  = grp.loc[grp[\"var\"]==\"tmin\", \"era5_lwr8_val\"].asfreq(\"D\")\n",
    "\n",
    "    for label, tmax, tmin in [(\"obs\",  obs_max,  obs_min),\n",
    "                              (\"era5\", era_max, era_min)]:\n",
    "\n",
    "        if tmax.empty or tmin.empty:\n",
    "            continue   # skip if station has no data\n",
    "\n",
    "        # --- absolute extremes ----------------------------------------------------\n",
    "        TXx = absolute_extremes(tmax, \"max\");  TNn = absolute_extremes(tmin, \"min\")\n",
    "        TXn = absolute_extremes(tmax, \"min\");  TNx = absolute_extremes(tmin, \"max\")\n",
    "\n",
    "        # --- percentile thresholds (baseline climatology) -------------------------\n",
    "        base_max = tmax[BASE_START:BASE_END];  base_min = tmin[BASE_START:BASE_END]\n",
    "        p90_TX   = _climatology_percentiles(base_max, 90)\n",
    "        p10_TX   = _climatology_percentiles(base_max, 10)\n",
    "        p90_TN   = _climatology_percentiles(base_min, 90)\n",
    "        p10_TN   = _climatology_percentiles(base_min, 10)\n",
    "\n",
    "        # flags for the whole record\n",
    "        TX90p_flag = percentile_flags(tmax, p90_TX, \"high\")\n",
    "        TX10p_flag = percentile_flags(tmax, p10_TX, \"low\")\n",
    "        TN90p_flag = percentile_flags(tmin, p90_TN, \"high\")\n",
    "        TN10p_flag = percentile_flags(tmin, p10_TN, \"low\")\n",
    "\n",
    "        TX90p = TX90p_flag.mean() * 100.0          # % of days\n",
    "        TX10p = TX10p_flag.mean() * 100.0\n",
    "        TN90p = TN90p_flag.mean() * 100.0\n",
    "        TN10p = TN10p_flag.mean() * 100.0\n",
    "\n",
    "        # --- warm / cold spell duration indices -----------------------------------\n",
    "        WSDI = spell_length(TX90p_flag, min_run=6)\n",
    "        CSDI = spell_length(TN10p_flag, min_run=6)\n",
    "\n",
    "        # --- absolute-threshold counters ------------------------------------------\n",
    "        FD = count_threshold(tmin, \"<\", 0.0)       # Frost days\n",
    "        ID = count_threshold(tmax, \"<\", 0.0)       # Ice days\n",
    "        SU = count_threshold(tmax, \">\", 25.0)      # Summer days\n",
    "        TR = count_threshold(tmin, \">\", 20.0)      # Tropical nights\n",
    "\n",
    "        # --- aggregate into one row ----------------------------------------------\n",
    "        indices_rows.append({\n",
    "            \"station_name\": st_name,\n",
    "            \"dataset\":      label,         # obs / era5\n",
    "            # intensity\n",
    "            \"TXx\": TXx, \"TNn\": TNn, \"TXn\": TXn, \"TNx\": TNx,\n",
    "            # percentile frequency\n",
    "            \"TX90p_%\": TX90p, \"TX10p_%\": TX10p,\n",
    "            \"TN90p_%\": TN90p, \"TN10p_%\": TN10p,\n",
    "            # spell duration\n",
    "            \"WSDI\": WSDI, \"CSDI\": CSDI,\n",
    "            # absolute threshold counts\n",
    "            \"FD\": FD, \"ID\": ID, \"SU\": SU, \"TR\": TR\n",
    "        })\n",
    "\n",
    "df_idx = pd.DataFrame(indices_rows)\n",
    "print(f\"Computed indices table shape: {df_idx.shape}\")\n",
    "\n",
    "###############################################################################\n",
    "# 6.  SAVE OUTPUT  (one sheet per dataset)\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "# 6.  SAVE OUTPUT  – ONE FILE PER INDEX  (wide table: obs_*, era5_*)\n",
    "###############################################################################\n",
    "# attach coordinates once (they will be the same for all indices)\n",
    "df_idx = pd.merge(\n",
    "    df_idx,\n",
    "    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# helper: build wide-format DataFrame for ONE index\n",
    "# ------------------------------------------------------------------\n",
    "def wide_table(index_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a wide table with columns:\n",
    "        station_name, obs_<idx>, era5_<idx>, lat, lon, elev\n",
    "    \"\"\"\n",
    "    wide = (\n",
    "        df_idx.pivot_table(index=\"station_name\",\n",
    "                           columns=\"dataset\",\n",
    "                           values=index_name)\n",
    "             .rename(columns={\"obs\": f\"obs_{index_name}\",\n",
    "                              \"era5\": f\"era5_{index_name}\"})\n",
    "             .reset_index()\n",
    "    )\n",
    "    # attach coords (unique per station)\n",
    "    wide = pd.merge(wide,\n",
    "                    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "                    on=\"station_name\", how=\"left\")\n",
    "    return wide\n",
    "\n",
    "# list of all index columns we computed\n",
    "index_cols = [\n",
    "    \"TXx\",\"TNn\",\"TXn\",\"TNx\",\n",
    "    \"TX90p_%\",\"TX10p_%\",\"TN90p_%\",\"TN10p_%\",\n",
    "    \"WSDI\",\"CSDI\",\"FD\",\"ID\",\"SU\",\"TR\"\n",
    "]\n",
    "\n",
    "print(f\"Writing one Excel file per index into\\n{output_dir}\")\n",
    "for idx in index_cols:\n",
    "    df_w = wide_table(idx)\n",
    "    out_xlsx = os.path.join(output_dir, f\"{idx}.xlsx\")\n",
    "    df_w.to_excel(out_xlsx, index=False)\n",
    "    print(\"  •\", os.path.basename(out_xlsx))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# OPTIONAL – also write a single Excel file containing *all* indices\n",
    "#            (each index in wide format, side-by-side)\n",
    "# ------------------------------------------------------------------\n",
    "wide_all = df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]].copy()\n",
    "for idx in index_cols:\n",
    "    w = wide_table(idx).drop(columns=[\"lat\",\"lon\",\"elev\"])\n",
    "    wide_all = pd.merge(wide_all, w, on=\"station_name\", how=\"left\")\n",
    "\n",
    "all_path = os.path.join(output_dir, \"Temperature_Indices_ALL.xlsx\")\n",
    "wide_all.to_excel(all_path, index=False)\n",
    "print(\"  •\", os.path.basename(all_path), \"(contains every index)\")\n",
    "\n",
    "print(\"✅  All index files written.\")\n",
    "\n",
    "###############################################################################\n",
    "# (OPTIONAL) QUICK MAP EXAMPLE  –  OBS-TXx (hottest daytime temperature)\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nQuick map example: OBS – TXx (°C)\")\n",
    "    \n",
    "    # base layers\n",
    "    gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "    gdf_lakes = gpd.read_file(lakes_shp     ).to_crs(epsg=4326)\n",
    "\n",
    "    # pick the OBS rows and build a GeoDataFrame\n",
    "    obs_tx = df_idx[df_idx[\"dataset\"] == \"obs\"].copy()\n",
    "    gdf_stn = gpd.GeoDataFrame(\n",
    "        obs_tx,\n",
    "        geometry=gpd.points_from_xy(obs_tx[\"lon\"], obs_tx[\"lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8),\n",
    "                            subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "    \n",
    "    # add basin + lakes outlines\n",
    "    ax.add_geometries(gdf_basin.geometry, ccrs.PlateCarree(),\n",
    "                      facecolor='none', edgecolor='black', linewidth=0.8)\n",
    "    ax.add_geometries(gdf_lakes.geometry, ccrs.PlateCarree(),\n",
    "                      facecolor='none', edgecolor='blue',  linewidth=0.8)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "    # scatter the station values\n",
    "    sc = ax.scatter(gdf_stn.geometry.x, gdf_stn.geometry.y,\n",
    "                    c=gdf_stn[\"TXx\"], cmap=\"hot_r\", s=60,\n",
    "                    edgecolor=\"k\", transform=ccrs.PlateCarree())\n",
    "    plt.colorbar(sc, ax=ax, label=\"TXx (°C)\")\n",
    "\n",
    "    ax.set_extent([-95.5, -72, 38.5, 52.5])   # Great Lakes frame\n",
    "    ax.set_title(\"Station hottest-day temperature (TXx)\\n1991-2012 – Observations\",\n",
    "                 fontsize=13)\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Mapping step failed:\", e)\n",
    "\n",
    "print(\"\\n✅  Finished: climatic indices computed & quick TXx map rendered.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6360f-ef9f-42f0-93cf-7acd82d30efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal stratification of climatic indices ERA5\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIGURATION\n",
    "###############################################################################\n",
    "csv_file      = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Total_Precipitaion\\Modified\\daily_loop\\era5_vs_stations_100km_LWR_1991_2012.csv\"\n",
    "physical_file = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv\"\n",
    "output_dir    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Total_Precipitaion\\Modified\\ClimaticIndices2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY DATA & ADD TEMPORAL FIELDS\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# Standardize station_name\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Add month (1..12) and season (DJF, MAM, JJA, SON)\n",
    "df_data[\"month\"] = df_data[\"time\"].dt.month\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"DJF\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"MAM\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"JJA\"\n",
    "    else:\n",
    "        return \"SON\"\n",
    "\n",
    "df_data[\"season\"] = df_data[\"month\"].apply(get_season)\n",
    "\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. LOAD PHYSICAL FILE & MERGE COORDINATES\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "###############################################################################\n",
    "def calc_rx1day(series):\n",
    "    \"\"\"Max 1-day precipitation.\"\"\"\n",
    "    return series.max(skipna=True)\n",
    "\n",
    "def calc_rx5day(series):\n",
    "    \"\"\"Max 5-day running sum.\"\"\"\n",
    "    return series.rolling(5, min_periods=1).sum().max(skipna=True)\n",
    "\n",
    "def calc_cdd(series, dry_threshold=1.0):\n",
    "    \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "    is_dry = series < dry_threshold\n",
    "    max_run, current_run = 0, 0\n",
    "    for val in is_dry:\n",
    "        if val:\n",
    "            current_run += 1\n",
    "            max_run = max(max_run, current_run)\n",
    "        else:\n",
    "            current_run = 0\n",
    "    return max_run\n",
    "\n",
    "def calc_cwd(series, wet_threshold=1.0):\n",
    "    \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "    is_wet = series >= wet_threshold\n",
    "    max_run, current_run = 0, 0\n",
    "    for val in is_wet:\n",
    "        if val:\n",
    "            current_run += 1\n",
    "            max_run = max(max_run, current_run)\n",
    "        else:\n",
    "            current_run = 0\n",
    "    return max_run\n",
    "\n",
    "def calc_r95p_r99p(series, percentiles=(95,99)):\n",
    "    \"\"\"\n",
    "    r95amt, r95pct, r99amt, r99pct:\n",
    "    - r95amt = sum of daily prcp above 95th percentile\n",
    "    - r95pct = (r95amt / total) * 100\n",
    "    - similarly for 99th percentile\n",
    "    \"\"\"\n",
    "    wet = series[series >= 1.0]\n",
    "    if len(wet) < 5:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    p95 = np.percentile(wet, percentiles[0])\n",
    "    p99 = np.percentile(wet, percentiles[1])\n",
    "    r95_amt = wet[wet > p95].sum()\n",
    "    r99_amt = wet[wet > p99].sum()\n",
    "    total = series.sum(skipna=True)\n",
    "    r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "    r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "    return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "\n",
    "def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "    \"\"\"\n",
    "    wetdays = #days >= wet_thr\n",
    "    drydays = #days < dry_thr\n",
    "    \"\"\"\n",
    "    return (series >= wet_thr).sum(), (series < dry_thr).sum()\n",
    "\n",
    "###############################################################################\n",
    "# 5. FUNCTION TO COMPUTE INDICES FOR A GROUP (MONTHLY or SEASONAL)\n",
    "###############################################################################\n",
    "def compute_indices(df_group):\n",
    "    \"\"\"\n",
    "    For a subset of daily data (e.g. station+month, or station+season),\n",
    "    compute the climate indices for Obs vs ERA5, plus ratio columns.\n",
    "    \"\"\"\n",
    "    obs_series = df_group[\"obs\"].dropna().reset_index(drop=True)\n",
    "    era5_series = df_group[\"era5_val\"].dropna().reset_index(drop=True)\n",
    "    if len(obs_series) == 0 or len(era5_series) == 0:\n",
    "        return None\n",
    "    \n",
    "    res = {}\n",
    "    # Rx1day / Rx5day\n",
    "    res[\"rx1day_obs\"] = calc_rx1day(obs_series)\n",
    "    res[\"rx1day_era5\"] = calc_rx1day(era5_series)\n",
    "    res[\"rx5day_obs\"] = calc_rx5day(obs_series)\n",
    "    res[\"rx5day_era5\"] = calc_rx5day(era5_series)\n",
    "    \n",
    "    # CDD / CWD\n",
    "    res[\"cdd_obs\"] = calc_cdd(obs_series)\n",
    "    res[\"cdd_era5\"] = calc_cdd(era5_series)\n",
    "    res[\"cwd_obs\"] = calc_cwd(obs_series)\n",
    "    res[\"cwd_era5\"] = calc_cwd(era5_series)\n",
    "    \n",
    "    # R95 / R99\n",
    "    r95_obs = calc_r95p_r99p(obs_series)\n",
    "    r95_era5 = calc_r95p_r99p(era5_series)\n",
    "    res[\"r95amt_obs\"] = r95_obs[0]\n",
    "    res[\"r95pct_obs\"] = r95_obs[1]\n",
    "    res[\"r95amt_era5\"] = r95_era5[0]\n",
    "    res[\"r95pct_era5\"] = r95_era5[1]\n",
    "    res[\"r99amt_obs\"] = r95_obs[2]\n",
    "    res[\"r99pct_obs\"] = r95_obs[3]\n",
    "    res[\"r99amt_era5\"] = r95_era5[2]\n",
    "    res[\"r99pct_era5\"] = r95_era5[3]\n",
    "    \n",
    "    # Wet / Dry days\n",
    "    wet_obs, dry_obs = calc_wetdays_drydays(obs_series)\n",
    "    wet_era5, dry_era5 = calc_wetdays_drydays(era5_series)\n",
    "    res[\"wetdays_obs\"] = wet_obs\n",
    "    res[\"wetdays_era5\"] = wet_era5\n",
    "    res[\"drydays_obs\"] = dry_obs\n",
    "    res[\"drydays_era5\"] = dry_era5\n",
    "    \n",
    "    # Ratio columns: EMD/OBS if obs != 0\n",
    "    if res[\"rx1day_obs\"]:\n",
    "        res[\"rx1day_ratio\"] = res[\"rx1day_era5\"] / res[\"rx1day_obs\"]\n",
    "    if res[\"rx5day_obs\"]:\n",
    "        res[\"rx5day_ratio\"] = res[\"rx5day_era5\"] / res[\"rx5day_obs\"]\n",
    "    if res[\"cdd_obs\"]:\n",
    "        res[\"cdd_ratio\"] = res[\"cdd_era5\"] / res[\"cdd_obs\"]\n",
    "    if res[\"cwd_obs\"]:\n",
    "        res[\"cwd_ratio\"] = res[\"cwd_era5\"] / res[\"cwd_obs\"]\n",
    "    if res[\"r95amt_obs\"]:\n",
    "        res[\"r95amt_ratio\"] = res[\"r95amt_era5\"] / res[\"r95amt_obs\"]\n",
    "    if res[\"r95pct_obs\"]:\n",
    "        res[\"r95pct_ratio\"] = res[\"r95pct_era5\"] / res[\"r95pct_obs\"]\n",
    "    if res[\"r99amt_obs\"]:\n",
    "        res[\"r99amt_ratio\"] = res[\"r99amt_era5\"] / res[\"r99amt_obs\"]\n",
    "    if res[\"r99pct_obs\"]:\n",
    "        res[\"r99pct_ratio\"] = res[\"r99pct_era5\"] / res[\"r99pct_obs\"]\n",
    "    if res[\"wetdays_obs\"]:\n",
    "        res[\"wetdays_ratio\"] = res[\"wetdays_era5\"] / res[\"wetdays_obs\"]\n",
    "    if res[\"drydays_obs\"]:\n",
    "        res[\"drydays_ratio\"] = res[\"drydays_era5\"] / res[\"drydays_obs\"]\n",
    "    \n",
    "    return res\n",
    "\n",
    "###############################################################################\n",
    "# 6. MONTHLY INDICES\n",
    "###############################################################################\n",
    "monthly_results = []\n",
    "group_month = df_data.groupby([\"station_name\", \"month\"])\n",
    "for (st_name, mon), group in group_month:\n",
    "    indices = compute_indices(group)\n",
    "    if indices is None:\n",
    "        continue\n",
    "    indices[\"station_name\"] = st_name\n",
    "    indices[\"month\"] = mon\n",
    "    monthly_results.append(indices)\n",
    "\n",
    "df_monthly = pd.DataFrame(monthly_results)\n",
    "df_monthly = pd.merge(\n",
    "    df_monthly,\n",
    "    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_monthly = df_monthly.sort_values([\"station_name\", \"month\"])\n",
    "monthly_out = os.path.join(output_dir, \"Indices_Monthly.xlsx\")\n",
    "df_monthly.to_excel(monthly_out, index=False)\n",
    "print(\"Monthly indices saved =>\", monthly_out)\n",
    "\n",
    "###############################################################################\n",
    "# 7. SEASONAL INDICES\n",
    "###############################################################################\n",
    "seasonal_results = []\n",
    "group_season = df_data.groupby([\"station_name\", \"season\"])\n",
    "for (st_name, seas), group in group_season:\n",
    "    indices = compute_indices(group)\n",
    "    if indices is None:\n",
    "        continue\n",
    "    indices[\"station_name\"] = st_name\n",
    "    indices[\"season\"] = seas\n",
    "    seasonal_results.append(indices)\n",
    "\n",
    "df_seasonal = pd.DataFrame(seasonal_results)\n",
    "df_seasonal = pd.merge(\n",
    "    df_seasonal,\n",
    "    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_seasonal = df_seasonal.sort_values([\"station_name\", \"season\"])\n",
    "seasonal_out = os.path.join(output_dir, \"Indices_Seasonal.xlsx\")\n",
    "df_seasonal.to_excel(seasonal_out, index=False)\n",
    "print(\"Seasonal indices saved =>\", seasonal_out)\n",
    "\n",
    "###############################################################################\n",
    "# 8. DONE\n",
    "###############################################################################\n",
    "print(\"\\nAll monthly and seasonal indices have been saved. (No extreme-event stratification.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d2253-2e0d-4449-8faa-0dd7ac092988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DJF for ERA5\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import Point\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIG & PATHS\n",
    "###############################################################################\n",
    "indices_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Total_Precipitaion\\Modified\\ClimaticIndices2\"\n",
    "seasonal_file = os.path.join(indices_dir, \"Indices_Seasonal.xlsx\")  # single file\n",
    "output_plots  = os.path.join(indices_dir, \"AnalysisPlots_DJF\")\n",
    "os.makedirs(output_plots, exist_ok=True)\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "# Indices in your seasonal file\n",
    "index_list = [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"r95p\",\"r99p\",\"wetdays\",\"drydays\"]\n",
    "\n",
    "# For summary stats, define how to find obs vs. emd columns\n",
    "index_columns = {\n",
    "    \"rx1day\":  (\"rx1day_obs\",  \"rx1day_era5\"),\n",
    "    \"rx5day\":  (\"rx5day_obs\",  \"rx5day_era5\"),\n",
    "    \"cdd\":     (\"cdd_obs\",     \"cdd_era5\"),\n",
    "    \"cwd\":     (\"cwd_obs\",     \"cwd_era5\"),\n",
    "    \"r95p\":    ((\"r95amt_obs\",\"r95pct_obs\"), (\"r95amt_era5\",\"r95pct_era5\")),\n",
    "    \"r99p\":    ((\"r99amt_obs\",\"r99pct_obs\"), (\"r99amt_era5\",\"r99pct_era5\")),\n",
    "    \"wetdays\": (\"wetdays_obs\",\"wetdays_era5\"),\n",
    "    \"drydays\": (\"drydays_obs\",\"drydays_era5\"),\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD SEASONAL FILE & FILTER TO DJF\n",
    "###############################################################################\n",
    "df_season = pd.read_excel(seasonal_file)\n",
    "print(\"Loaded =>\", seasonal_file, \"| shape =\", df_season.shape)\n",
    "\n",
    "# Filter to DJF\n",
    "df_season = df_season[df_season[\"season\"]==\"DJF\"].copy()\n",
    "df_season = df_season.dropna(subset=[\"lat\",\"lon\"])  # ensure lat/lon exist\n",
    "print(\"After filtering to DJF => shape =\", df_season.shape)\n",
    "\n",
    "mdf = df_season.reset_index(drop=True)\n",
    "master_xlsx = os.path.join(output_plots, \"MasterTable_Seasonal_DJF.xlsx\")\n",
    "mdf.to_excel(master_xlsx, index=False)\n",
    "print(f\"\\n(A) Master table (DJF) saved => {master_xlsx}\")\n",
    "print(\"Columns:\", mdf.columns.tolist())\n",
    "\n",
    "###############################################################################\n",
    "# 3. SUMMARY TABLE (MBE, RMSE, STD, CC, d) for DJF\n",
    "###############################################################################\n",
    "def index_of_agreement(obs, model):\n",
    "    obs_mean = np.mean(obs)\n",
    "    num = np.sum((model - obs)**2)\n",
    "    den = np.sum((abs(model - obs_mean) + abs(obs - obs_mean))**2)\n",
    "    if den == 0:\n",
    "        return np.nan\n",
    "    return 1 - num/den\n",
    "\n",
    "def rmse(a, b):\n",
    "    return np.sqrt(np.mean((a-b)**2))\n",
    "\n",
    "def std_of_residuals(a, b):\n",
    "    return np.std(a-b, ddof=1)\n",
    "\n",
    "def mean_bias_error(a, b):\n",
    "    return np.mean(b-a)\n",
    "\n",
    "summary_rows = []\n",
    "for idx_name in index_list:\n",
    "    obs_cols = index_columns[idx_name][0]\n",
    "    era5_cols = index_columns[idx_name][1]\n",
    "\n",
    "    if isinstance(obs_cols, tuple):\n",
    "        # multiple columns\n",
    "        for oc, ec in zip(obs_cols, era5_cols):\n",
    "            valid = mdf[[oc, ec]].dropna()\n",
    "            if len(valid) < 2:\n",
    "                continue\n",
    "            obs_vals = valid[oc].values\n",
    "            era5_vals = valid[ec].values\n",
    "            MB  = mean_bias_error(obs_vals, era5_vals)\n",
    "            RM  = rmse(obs_vals, era5_vals)\n",
    "            SR  = std_of_residuals(obs_vals, era5_vals)\n",
    "            CC  = pearsonr(obs_vals, era5_vals)[0] if len(obs_vals) > 1 else np.nan\n",
    "            dd  = index_of_agreement(obs_vals, era5_vals)\n",
    "            idx_label = f\"{idx_name}_{oc.replace('_obs','')}\"\n",
    "            summary_rows.append({\n",
    "                \"Index\": idx_label,\n",
    "                \"Count\": len(valid),\n",
    "                \"MBE\": MB,\n",
    "                \"RMSE\": RM,\n",
    "                \"STDres\": SR,\n",
    "                \"CC\": CC,\n",
    "                \"d\": dd,\n",
    "            })\n",
    "    else:\n",
    "        oc = obs_cols\n",
    "        ec = era5_cols\n",
    "        valid = mdf[[oc, ec]].dropna()\n",
    "        if len(valid) < 2:\n",
    "            continue\n",
    "        obs_vals = valid[oc].values\n",
    "        era5_vals = valid[ec].values\n",
    "        MB = mean_bias_error(obs_vals, era5_vals)\n",
    "        RM = rmse(obs_vals, era5_vals)\n",
    "        SR = std_of_residuals(obs_vals, era5_vals)\n",
    "        CC = pearsonr(obs_vals, era5_vals)[0] if len(obs_vals) > 1 else np.nan\n",
    "        dd = index_of_agreement(obs_vals, era5_vals)\n",
    "        summary_rows.append({\n",
    "            \"Index\": idx_name,\n",
    "            \"Count\": len(valid),\n",
    "            \"MBE\": MB,\n",
    "            \"RMSE\": RM,\n",
    "            \"STDres\": SR,\n",
    "            \"CC\": CC,\n",
    "            \"d\": dd,\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_cols = [\"Index\",\"Count\",\"MBE\",\"RMSE\",\"STDres\",\"CC\",\"d\"]\n",
    "summary_df = summary_df[summary_cols]\n",
    "summary_xlsx = os.path.join(output_plots, \"SummaryTable_Extremes_DJF.xlsx\")\n",
    "summary_df.to_excel(summary_xlsx, index=False)\n",
    "print(f\"(B) Summary Table (DJF) => {summary_xlsx}\\n{summary_df}\")\n",
    "\n",
    "###############################################################################\n",
    "# 4. MAPPING: Combine Observed, EMD, Ratio in One Figure\n",
    "###############################################################################\n",
    "gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "\n",
    "def add_basin_lakes(ax):\n",
    "    #ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    for geom in gdf_basin.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(),\n",
    "                          facecolor='none', edgecolor='black', linewidth=1)\n",
    "    for geom in gdf_lakes.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(),\n",
    "                          facecolor='none', edgecolor='cyan', linewidth=1)\n",
    "\n",
    "def plot_map_triptych(df, obs_col, emd_col, ratio_col, idx_name, out_png):\n",
    "    \"\"\"\n",
    "    Creates a single figure with 3 subplots (side-by-side):\n",
    "      1) Observed\n",
    "      2) ERA5\n",
    "      3) Ratio (ERA5/OBS)\n",
    "    Each subplot has a colorbar, a 90th-percentile hotspot circle, etc.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6),\n",
    "                             subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "\n",
    "    # We'll define a small helper to do each subplot\n",
    "    def scatter_map(ax, value_col, title):\n",
    "        ax.set_extent([-95.5, -72, 38.5, 52.5])\n",
    "        add_basin_lakes(ax)\n",
    "        sc = ax.scatter(df[\"lon\"], df[\"lat\"], c=df[value_col], cmap=\"viridis\",\n",
    "                        s=60, transform=ccrs.PlateCarree(), edgecolor=\"k\", zorder=10)\n",
    "        cb = plt.colorbar(sc, ax=ax, shrink=0.8)\n",
    "        cb.set_label(value_col)\n",
    "\n",
    "        # Hotspots => top 10%\n",
    "        vals = df[value_col].dropna().values\n",
    "        if len(vals) > 0:\n",
    "            thr = np.percentile(vals, 90)\n",
    "            is_hot = df[value_col]>=thr\n",
    "            ax.scatter(df.loc[is_hot,\"lon\"], df.loc[is_hot,\"lat\"],\n",
    "                       marker='o', facecolors='none', edgecolors='red', s=80,\n",
    "                       transform=ccrs.PlateCarree(), zorder=11,\n",
    "                       label=f\"Hotspot >= {thr:.2f}\")\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        gl = ax.gridlines(draw_labels=True, linestyle='--', color='gray')\n",
    "        gl.right_labels = False\n",
    "        gl.top_labels   = False\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    scatter_map(axes[0], obs_col,  f\"{idx_name} Observed (DJF)\")\n",
    "    scatter_map(axes[1], era5_col,  f\"{idx_name} ERA5 (DJF)\")\n",
    "    scatter_map(axes[2], ratio_col,f\"{idx_name} (ERA5/OBS) (DJF)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved =>\", out_png)\n",
    "\n",
    "def get_map_cols(idx_name):\n",
    "    if idx_name in [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"drydays\"]:\n",
    "        obs  = f\"{idx_name}_obs\"\n",
    "        era5  = f\"{idx_name}_era5\"\n",
    "        ratio= f\"{idx_name}_ratio\"\n",
    "        return obs, era5, ratio\n",
    "    elif idx_name == \"wetdays\":\n",
    "        obs  = \"wetdays_obs\"\n",
    "        era5  = \"wetdays_era5\"\n",
    "        ratio= \"wetdays_ratio\"\n",
    "        return obs, era5, ratio\n",
    "    elif idx_name == \"r95p\":\n",
    "        obs  = \"r95amt_obs\"\n",
    "        era5  = \"r95amt_era5\"\n",
    "        ratio= \"r95amt_ratio\"\n",
    "        return obs, era5, ratio\n",
    "    elif idx_name == \"r99p\":\n",
    "        obs  = \"r99amt_obs\"\n",
    "        era5  = \"r99amt_era5\"\n",
    "        ratio= \"r99amt_ratio\"\n",
    "        return obs, era5, ratio\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "for idx_name in index_list:\n",
    "    obs_col, era5_col, ratio_col = get_map_cols(idx_name)\n",
    "    if obs_col is None:\n",
    "        continue\n",
    "\n",
    "    needed_cols = [obs_col, era5_col, ratio_col, \"lat\", \"lon\"]\n",
    "    if not all(c in mdf.columns for c in needed_cols):\n",
    "        print(f\"Skipping map for {idx_name} - missing columns.\")\n",
    "        continue\n",
    "\n",
    "    subdf = mdf.dropna(subset=[\"lat\",\"lon\"]).copy()\n",
    "    out_png = os.path.join(output_plots, f\"DJF_{idx_name}_MAP_3panel.png\")\n",
    "    plot_map_triptych(subdf, obs_col, era5_col, ratio_col, idx_name, out_png)\n",
    "\n",
    "###############################################################################\n",
    "# 5. DISTRIBUTION & BOX/CDF/Scatter in One Figure\n",
    "###############################################################################\n",
    "def plot_distribution_triptych(df, obs_col, era5_col, label, out_png):\n",
    "    \"\"\"\n",
    "    Creates a single figure with 3 subplots side-by-side:\n",
    "      1) Boxplot\n",
    "      2) CDF\n",
    "      3) Scatter\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,6))\n",
    "\n",
    "    # A) Boxplot\n",
    "    ax_box = axes[0]\n",
    "    data = pd.DataFrame({\"Obs\": df[obs_col], \"ERA5\": df[era5_col]}).melt(\n",
    "        var_name=\"Dataset\", value_name=label\n",
    "    )\n",
    "    sns.boxplot(data=data, x=\"Dataset\", y=label, ax=ax_box)\n",
    "    ax_box.set_title(f\"Boxplot: {label} (DJF)\")\n",
    "\n",
    "    # B) CDF\n",
    "    ax_cdf = axes[1]\n",
    "    obs_vals = df[obs_col].dropna()\n",
    "    era5_vals = df[era5_col].dropna()\n",
    "\n",
    "    def ecdf(x):\n",
    "        xs = np.sort(x)\n",
    "        ys = np.arange(1, len(xs)+1)/len(xs)\n",
    "        return xs, ys\n",
    "\n",
    "    if len(obs_vals)>=2 and len(era5_vals)>=2:\n",
    "        xs_o, ys_o = ecdf(obs_vals)\n",
    "        xs_e, ys_e = ecdf(era5_vals)\n",
    "        ax_cdf.plot(xs_o, ys_o, label=\"Obs\")\n",
    "        ax_cdf.plot(xs_e, ys_e, label=\"ERA5\")\n",
    "        ax_cdf.set_title(f\"CDF of {label} (DJF)\")\n",
    "        ax_cdf.set_xlabel(label)\n",
    "        ax_cdf.set_ylabel(\"Probability\")\n",
    "        ax_cdf.legend()\n",
    "    else:\n",
    "        ax_cdf.set_title(f\"CDF: not enough data ({label})\")\n",
    "\n",
    "    # C) Scatter\n",
    "    ax_scat = axes[2]\n",
    "    valid = df[[obs_col, era5_col]].dropna()\n",
    "    if len(valid)>=2:\n",
    "        x = valid[obs_col]\n",
    "        y = valid[era5_col]\n",
    "        cc, _ = pearsonr(x, y)\n",
    "        ax_scat.scatter(x, y, edgecolors='k', alpha=0.7)\n",
    "        mn, mx = np.nanmin([x.min(), y.min()]), np.nanmax([x.max(), y.max()])\n",
    "        ax_scat.plot([mn, mx],[mn, mx],'r--')\n",
    "        ax_scat.set_xlabel(f\"Obs {label} (DJF)\")\n",
    "        ax_scat.set_ylabel(f\"ERA5 {label} (DJF)\")\n",
    "        ax_scat.set_title(f\"{label} (Corr={cc:.2f}, DJF)\")\n",
    "    else:\n",
    "        ax_scat.set_title(f\"Scatter: not enough data ({label})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved =>\", out_png)\n",
    "\n",
    "for idx_name in index_list:\n",
    "    # figure out obs, emd columns\n",
    "    if idx_name in [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"drydays\"]:\n",
    "        obs_col  = f\"{idx_name}_obs\"\n",
    "        era5_col  = f\"{idx_name}_era5\"\n",
    "    elif idx_name == \"wetdays\":\n",
    "        obs_col  = \"wetdays_obs\"\n",
    "        era5_col  = \"wetdays_era5\"\n",
    "    elif idx_name == \"r95p\":\n",
    "        obs_col  = \"r95amt_obs\"\n",
    "        era5_col  = \"r95amt_era5\"\n",
    "    elif idx_name == \"r99p\":\n",
    "        obs_col  = \"r99amt_obs\"\n",
    "        era5_col  = \"r99amt_era5\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    if obs_col not in mdf.columns or era5_col not in mdf.columns:\n",
    "        print(f\"Skipping distribution for {idx_name} - missing columns.\")\n",
    "        continue\n",
    "\n",
    "    subdf = mdf[[obs_col, era5_col]].dropna()\n",
    "    if len(subdf)<2:\n",
    "        print(f\"Skipping distribution for {idx_name} - not enough data.\")\n",
    "        continue\n",
    "\n",
    "    out_3panel = os.path.join(output_plots, f\"DJF_{idx_name}_Distribution_3panel.png\")\n",
    "    plot_distribution_triptych(subdf, obs_col, era5_col, idx_name, out_3panel)\n",
    "\n",
    "###############################################################################\n",
    "# 6. DONE\n",
    "###############################################################################\n",
    "print(\"\\nAll DJF steps completed! See outputs in:\", output_plots)\n",
    "\n",
    "# For the other seasons, just change any DJF to JJA, MAM, or SON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77c2d0-a9a8-408b-a1fc-0414f4aaba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "                 #########################                                           ##############################\n",
    "                 #########################                  MERRA-2                  ##############################\n",
    "                 #########################                                           ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae35a9-bda7-4e9c-a4bd-d1ff660c454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the climatic indices for MERRA2 (prcp)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIG\n",
    "###############################################################################\n",
    "csv_file       = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\MERRA2_GLB_Precipitation\\daily_loop\\merra2_vs_stations_12Nearest_LWR_1991_2012.csv\"\n",
    "physical_file  = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv\"\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "output_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\MERRA2_GLB_Precipitation\\ClimaticIndices-12Nearest\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY CSV DATA\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data (obs, merra2_lwr12_val) ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "# parse 'time' as datetime if needed\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# unify station_name: remove leading/trailing spaces, uppercase\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(f\"df_data shape = {df_data.shape}\")\n",
    "print(\"Columns:\", df_data.columns.tolist())\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. (OPTIONAL) MERGE WITH PHYSICAL FILE TO GET LAT/LON\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "# unify station_name\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "###############################################################################\n",
    "def calc_rx1day(series):\n",
    "    \"\"\"Max 1-day precipitation.\"\"\"\n",
    "    return series.max(skipna=True)\n",
    "\n",
    "def calc_rx5day(series):\n",
    "    \"\"\"Max 5-day running sum.\"\"\"\n",
    "    roll_5 = series.rolling(5, min_periods=1).sum()\n",
    "    return roll_5.max(skipna=True)\n",
    "\n",
    "def calc_cdd(series, dry_threshold=1.0):\n",
    "    \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "    is_dry = series < dry_threshold\n",
    "    maxr, curr = 0, 0\n",
    "    for val in is_dry:\n",
    "        if val:\n",
    "            curr += 1\n",
    "            maxr = max(maxr, curr)\n",
    "        else:\n",
    "            curr = 0\n",
    "    return maxr\n",
    "\n",
    "def calc_cwd(series, wet_threshold=1.0):\n",
    "    \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "    is_wet = series >= wet_threshold\n",
    "    maxr, curr = 0, 0\n",
    "    for val in is_wet:\n",
    "        if val:\n",
    "            curr += 1\n",
    "            maxr = max(maxr, curr)\n",
    "        else:\n",
    "            curr = 0\n",
    "    return maxr\n",
    "\n",
    "def calc_r95p_r99p(series, percentile=(95,99)):\n",
    "    \"\"\"R95p, R99p TOT in mm, plus percentage of total.\"\"\"\n",
    "    # only wet days >=1 mm for percentile\n",
    "    wet = series[series >= 1.0]\n",
    "    if len(wet) < 5:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    p95 = np.percentile(wet, percentile[0])\n",
    "    p99 = np.percentile(wet, percentile[1])\n",
    "    r95_amt = wet[wet > p95].sum()\n",
    "    r99_amt = wet[wet > p99].sum()\n",
    "    total   = series.sum(skipna=True)\n",
    "    r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "    r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "    return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "\n",
    "def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "    \"\"\"Count #wet days >=5 mm, #dry days <1 mm.\"\"\"\n",
    "    w = (series >= wet_thr).sum()\n",
    "    d = (series <  dry_thr).sum()\n",
    "    return w, d\n",
    "\n",
    "###############################################################################\n",
    "# 5. COMPUTE INDICES FOR EACH STATION\n",
    "###############################################################################\n",
    "rx1_list, rx5_list = [], []\n",
    "cdd_list, cwd_list = [], []\n",
    "r95_list, r99_list = [], []\n",
    "wet_list, dry_list = [], []\n",
    "\n",
    "print(\"Computing indices for each station...\")\n",
    "\n",
    "grouped = df_data.groupby(\"station_name\", as_index=False)\n",
    "for st_name, grp in grouped:\n",
    "    # Sort by time (just in case)\n",
    "    grp = grp.sort_values(\"time\")\n",
    "\n",
    "    # daily obs/era5\n",
    "    obs_series = grp[\"obs\"].dropna().reset_index(drop=True)\n",
    "    merra2_series = grp[\"merra2_lwr12_val\"].dropna().reset_index(drop=True)  # <--- REFERENCE CHANGED HERE\n",
    "\n",
    "    # A) Rx1day\n",
    "    obs_rx1 = calc_rx1day(obs_series)\n",
    "    merra2_rx1 = calc_rx1day(merra2_series)\n",
    "    rx1_list.append({\"station_name\": st_name,\n",
    "                     \"obs_rx1day\": obs_rx1,\n",
    "                     \"merra2_rx1day\": merra2_rx1})\n",
    "\n",
    "    # B) Rx5day\n",
    "    obs_rx5 = calc_rx5day(obs_series)\n",
    "    merra2_rx5 = calc_rx5day(merra2_series)\n",
    "    rx5_list.append({\"station_name\": st_name,\n",
    "                     \"obs_rx5day\": obs_rx5,\n",
    "                     \"merra2_rx5day\": merra2_rx5})\n",
    "\n",
    "    # C) CDD\n",
    "    obs_cdd_val = calc_cdd(obs_series)\n",
    "    merra2_cdd_val = calc_cdd(merra2_series)\n",
    "    cdd_list.append({\"station_name\": st_name,\n",
    "                     \"obs_cdd\": obs_cdd_val,\n",
    "                     \"merra2_cdd\": merra2_cdd_val})\n",
    "\n",
    "    # D) CWD\n",
    "    obs_cwd_val = calc_cwd(obs_series)\n",
    "    merra2_cwd_val = calc_cwd(merra2_series)\n",
    "    cwd_list.append({\"station_name\": st_name,\n",
    "                     \"obs_cwd\": obs_cwd_val,\n",
    "                     \"merra2_cwd\": merra2_cwd_val})\n",
    "\n",
    "    # E) R95 / R99\n",
    "    or95a, or95p, or99a, or99p = calc_r95p_r99p(obs_series)\n",
    "    er95a, er95p, er99a, er99p = calc_r95p_r99p(merra2_series)\n",
    "    r95_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_r95amt\": or95a, \"obs_r95pct\": or95p,\n",
    "        \"merra2_r95amt\": er95a, \"merra2_r95pct\": er95p\n",
    "    })\n",
    "    r99_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_r99amt\": or99a, \"obs_r99pct\": or99p,\n",
    "        \"merra2_r99amt\": er99a, \"merra2_r99pct\": er99p\n",
    "    })\n",
    "\n",
    "    # F) wet/dry days\n",
    "    obs_wet5, obs_dry = calc_wetdays_drydays(obs_series)\n",
    "    merra2_wet5, merra2_dry = calc_wetdays_drydays(merra2_series)\n",
    "    wet_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_wetdays5mm\": obs_wet5, \n",
    "        \"merra2_wetdays5mm\": merra2_wet5\n",
    "    })\n",
    "    dry_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_drydays\": obs_dry,\n",
    "        \"merra2_drydays\": merra2_dry\n",
    "    })\n",
    "\n",
    "print(\"Finished computing. Now merging lat/lon from physical file ...\")\n",
    "\n",
    "def attach_coords(df_in):\n",
    "    \"\"\"Attach lat, lon, elev from df_phys on station_name.\"\"\"\n",
    "    df_out = pd.merge(\n",
    "        df_in, \n",
    "        df_phys[[\"station_name\",\"lat\",\"lon\",\"elev\"]],\n",
    "        on=\"station_name\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "    return df_out\n",
    "\n",
    "df_rx1 = attach_coords(pd.DataFrame(rx1_list))\n",
    "df_rx5 = attach_coords(pd.DataFrame(rx5_list))\n",
    "df_cdd = attach_coords(pd.DataFrame(cdd_list))\n",
    "df_cwd = attach_coords(pd.DataFrame(cwd_list))\n",
    "df_r95 = attach_coords(pd.DataFrame(r95_list))\n",
    "df_r99 = attach_coords(pd.DataFrame(r99_list))\n",
    "df_wet = attach_coords(pd.DataFrame(wet_list))\n",
    "df_dry = attach_coords(pd.DataFrame(dry_list))\n",
    "\n",
    "###############################################################################\n",
    "# 6. SAVE OUTPUT\n",
    "###############################################################################\n",
    "print(\"Saving index tables to Excel in:\", output_dir)\n",
    "df_rx1.to_excel(os.path.join(output_dir, \"rx1day.xlsx\"),  index=False)\n",
    "df_rx5.to_excel(os.path.join(output_dir, \"rx5day.xlsx\"),  index=False)\n",
    "df_cdd.to_excel(os.path.join(output_dir, \"cdd.xlsx\"),     index=False)\n",
    "df_cwd.to_excel(os.path.join(output_dir, \"cwd.xlsx\"),     index=False)\n",
    "df_r95.to_excel(os.path.join(output_dir, \"r95p.xlsx\"),    index=False)\n",
    "df_r99.to_excel(os.path.join(output_dir, \"r99p.xlsx\"),    index=False)\n",
    "df_wet.to_excel(os.path.join(output_dir, \"wetdays.xlsx\"), index=False)\n",
    "df_dry.to_excel(os.path.join(output_dir, \"drydays.xlsx\"), index=False)\n",
    "\n",
    "print(\"\\nAll precipitation-based indices have been saved to Excel.\")\n",
    "\n",
    "###############################################################################\n",
    "# (OPTIONAL) QUICK MAP EXAMPLE\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nQuick map example for obs_rx5day ...\")\n",
    "    # Load shapefiles\n",
    "    gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "    gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        df_rx5,\n",
    "        geometry=gpd.points_from_xy(df_rx5[\"lon\"], df_rx5[\"lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "    for geom in gdf_basin.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='blue', linewidth=1)\n",
    "    for geom in gdf_lakes.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='cyan', linewidth=1)\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "    sc = ax.scatter(gdf_stations.geometry.x, gdf_stations.geometry.y,\n",
    "                    c=gdf_stations[\"obs_rx5day\"], cmap=\"Reds\", s=60,\n",
    "                    transform=ccrs.PlateCarree(), edgecolor=\"k\")\n",
    "    plt.colorbar(sc, ax=ax, label=\"Obs Rx5day (mm)\")\n",
    "    ax.set_extent([-95.5, -72, 38.5, 52.5])  # approximate bounding box\n",
    "    gl = ax.gridlines(draw_labels=True, linestyle='--', color='gray')\n",
    "    gl.right_labels = False\n",
    "    gl.top_labels   = False\n",
    "\n",
    "    plt.title(\"Obs Rx5day (from CSV daily data)\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Mapping step failed:\", e)\n",
    "\n",
    "print(\"\\n✅ Done computing precipitation-based indices from 'merra2_lwr12_val' column!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f527401-434d-46e1-8545-625145100354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the temperature (tmin-tmax) climatic indices for MERRA2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIG\n",
    "###############################################################################\n",
    "csv_file = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\MERRA2_GLB_Temperature\\daily_loop\\merra2_vs_stations_12Nearest_LWR_1991_2012.csv\"\n",
    "physical_file = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation_Temperature.csv\"\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "output_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\MERRA2_GLB_Temperature\\ClimaticIndices-12Nearest\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY CSV DATA\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data (obs, merra2_lwr12_val) ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(f\"df_data shape = {df_data.shape}\")\n",
    "print(\"Columns:\", df_data.columns.tolist())\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. MERGE WITH PHYSICAL FILE TO GET LAT/LON\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS – FULL ETCCDI SET FOR Tmin / Tmax\n",
    "###############################################################################\n",
    "def absolute_extremes(series, kind):\n",
    "    \"\"\"Return the single-day absolute extreme.\"\"\"\n",
    "    if kind == \"max\":\n",
    "        return series.max(skipna=True)\n",
    "    else:                       # \"min\"\n",
    "        return series.min(skipna=True)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# percentile thresholds (5-day moving window, baseline = 1991-2012)\n",
    "# -------------------------------------------------------------------------\n",
    "BASE_START, BASE_END = \"1991-01-01\", \"2012-12-31\"\n",
    "\n",
    "def _climatology_percentiles(s, p):\n",
    "    \"\"\"Return a Series (index = 1…366) of the p-th percentile.\"\"\"\n",
    "    # drop 29 Feb so every year has 365 days\n",
    "    s = s[~((s.index.month == 2) & (s.index.day == 29))]\n",
    "    df = pd.DataFrame({\"val\": s, \"doy\": s.index.dayofyear})\n",
    "    climo = []\n",
    "    for d in range(1, 366):\n",
    "        win = list(range(d-2, d+3))                       # ±2-day window\n",
    "        win = [(x-1) % 365 + 1 for x in win]              # wrap around ends\n",
    "        vals = df.loc[df[\"doy\"].isin(win), \"val\"]\n",
    "        climo.append(np.nanpercentile(vals, p) if len(vals) else np.nan)\n",
    "    return pd.Series(climo, index=range(1, 366), name=f\"p{p}\")\n",
    "\n",
    "def percentile_flags(s, perc_series, side):\n",
    "    \"\"\"Return Boolean Series: True where value is < or > percentile.\"\"\"\n",
    "    doy = s.index.dayofyear\n",
    "    thr = perc_series.reindex(doy).values\n",
    "    if side == \"low\":\n",
    "        return s < thr\n",
    "    else:\n",
    "        return s > thr\n",
    "\n",
    "def spell_length(bool_series, min_run=6):\n",
    "    \"\"\"Total # days in spells of ≥ min_run consecutive Trues.\"\"\"\n",
    "    is_true = bool_series.fillna(False).values\n",
    "    # identify run lengths\n",
    "    run_ends = np.where(np.diff(np.concatenate(([0], is_true, [0]))))[0]\n",
    "    lengths  = run_ends[1::2] - run_ends[::2]\n",
    "    return lengths[lengths >= min_run].sum()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# absolute-threshold counters\n",
    "# -------------------------------------------------------------------------\n",
    "def count_threshold(series, op, thr):\n",
    "    if op == \"<\":\n",
    "        return (series < thr).sum()\n",
    "    else:\n",
    "        return (series > thr).sum()\n",
    "\n",
    "###############################################################################\n",
    "# 5.  ANNUAL ETCCDI INDICES  –  FORMAT COMPATIBLE WITH THE SEASONAL SCRIPT\n",
    "#     • keeps:   TXx  TNn  TX90p  TN10p  FD  WSDI  CSDI\n",
    "#     • drops:   TR, ID, SU, TXn, TNx\n",
    "###############################################################################\n",
    "rows = []\n",
    "\n",
    "print(\"→ computing *annual* indices …\")\n",
    "for st_name, st_grp in df_data.groupby(\"station_name\"):\n",
    "\n",
    "    st_grp = st_grp.set_index(\"time\").sort_index()\n",
    "\n",
    "    # build daily Series ................................................................\n",
    "    obs_max = st_grp.loc[st_grp[\"var\"] == \"tmax\", \"obs\"].asfreq(\"D\")\n",
    "    obs_min = st_grp.loc[st_grp[\"var\"] == \"tmin\", \"obs\"].asfreq(\"D\")\n",
    "    merra2_max = st_grp.loc[st_grp[\"var\"] == \"tmax\", \"merra2_lwr12_val\"].asfreq(\"D\")\n",
    "    merra2_min = st_grp.loc[st_grp[\"var\"] == \"tmin\", \"merra2_lwr12_val\"].asfreq(\"D\")\n",
    "    if obs_max.empty:          # station has no data at all\n",
    "        continue\n",
    "\n",
    "    # ── fixed-year climatology (1991-2012, 5-day moving window) ─────────────-\n",
    "    p90_TX_obs = _climatology_percentiles(obs_max[BASE_START:BASE_END], 90)\n",
    "    p10_TN_obs = _climatology_percentiles(obs_min[BASE_START:BASE_END], 10)\n",
    "    p90_TX_merra2 = _climatology_percentiles(merra2_max[BASE_START:BASE_END], 90)\n",
    "    p10_TN_merra2 = _climatology_percentiles(merra2_min[BASE_START:BASE_END], 10)\n",
    "\n",
    "    # flags for the *full* record (faster than recomputing year-by-year)\n",
    "    flags = {\n",
    "        \"obs_TX90\": obs_max >\n",
    "                     p90_TX_obs.reindex(obs_max.index.dayofyear).values,\n",
    "        \"merra2_TX90\": merra2_max >\n",
    "                     p90_TX_merra2.reindex(merra2_max.index.dayofyear).values,\n",
    "        \"obs_TN10\": obs_min <\n",
    "                     p10_TN_obs.reindex(obs_min.index.dayofyear).values,\n",
    "        \"merra2_TN10\": merra2_min <\n",
    "                     p10_TN_merra2.reindex(merra2_min.index.dayofyear).values,\n",
    "    }\n",
    "\n",
    "    # ── iterate over years (December belongs to the following DJF year) ──────\n",
    "    years = np.unique(obs_max.index.year)\n",
    "    for yr in years:\n",
    "        mask = obs_max.index.year == yr\n",
    "        if mask.sum() < 200:         # at least ~55 % of a year\n",
    "            continue\n",
    "\n",
    "        def _sel(s):          # helper to slice one year\n",
    "            return s[s.index.year == yr]\n",
    "\n",
    "        # intensity ....................................................................\n",
    "        TXx_obs = _sel(obs_max).max()\n",
    "        TXx_merra2 = _sel(merra2_max).max()\n",
    "        TNn_obs = _sel(obs_min).min()\n",
    "        TNn_merra2 = _sel(merra2_min).min()\n",
    "\n",
    "        # percentile frequencies (percentage of days)\n",
    "        TX90p_obs = flags[\"obs_TX90\"][mask].mean() * 100.0\n",
    "        TX90p_merra2 = flags[\"merra2_TX90\"][mask].mean() * 100.0\n",
    "        TN10p_obs = flags[\"obs_TN10\"][mask].mean() * 100.0\n",
    "        TN10p_merra2 = flags[\"merra2_TN10\"][mask].mean() * 100.0\n",
    "\n",
    "        # spell duration (≥6 consecutive days)\n",
    "        WSDI_obs = spell_length(flags[\"obs_TX90\"][mask], min_run=6)\n",
    "        WSDI_merra2 = spell_length(flags[\"merra2_TX90\"][mask], min_run=6)\n",
    "        CSDI_obs = spell_length(flags[\"obs_TN10\"][mask], min_run=6)\n",
    "        CSDI_merra2 = spell_length(flags[\"merra2_TN10\"][mask], min_run=6)\n",
    "\n",
    "        # absolute‐threshold count\n",
    "        FD_obs = (_sel(obs_min) < 0).sum()\n",
    "        FD_merra2 = (_sel(merra2_min) < 0).sum()\n",
    "\n",
    "        # helper for ratios (avoid /0)\n",
    "        ratio = lambda o, e: np.nan if (o == 0 or np.isnan(o)) else e / o\n",
    "\n",
    "        rows.append(dict(\n",
    "            station_name=st_name, year=yr,\n",
    "            TXx_obs=TXx_obs,   TXx_merra2=TXx_merra2,   TXx_ratio=ratio(TXx_obs, TXx_merra2),\n",
    "            TNn_obs=TNn_obs,   TNn_merra2=TNn_merra2,   TNn_ratio=ratio(TNn_obs, TNn_merra2),\n",
    "            TX90p_obs=TX90p_obs, TX90p_merra2=TX90p_merra2,\n",
    "            TX90p_ratio=ratio(TX90p_obs, TX90p_merra2),\n",
    "            TN10p_obs=TN10p_obs, TN10p_merra2=TN10p_merra2,\n",
    "            TN10p_ratio=ratio(TN10p_obs, TN10p_merra2),\n",
    "            FD_obs=FD_obs,     FD_merra2=FD_merra2,     FD_ratio=ratio(FD_obs, FD_merra2),\n",
    "            WSDI_obs=WSDI_obs, WSDI_merra2=WSDI_merra2,\n",
    "            WSDI_ratio=ratio(WSDI_obs, WSDI_merra2),\n",
    "            CSDI_obs=CSDI_obs, CSDI_merra2=CSDI_merra2,\n",
    "            CSDI_ratio=ratio(CSDI_obs, CSDI_merra2)\n",
    "        ))\n",
    "\n",
    "df_yr = (pd.DataFrame(rows)\n",
    "         .merge(df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "                on=\"station_name\", how=\"left\")\n",
    "         .sort_values([\"station_name\", \"year\"]))\n",
    "\n",
    "###############################################################################\n",
    "# 6.  SAVE OUTPUT  – ONE PARQUET ( + XLSX )  +  OPTIONAL per-index sheets\n",
    "###############################################################################\n",
    "annual_pq  = os.path.join(output_dir, \"Indices_Annual.parquet\")\n",
    "annual_xls = annual_pq.replace(\".parquet\", \".xlsx\")\n",
    "\n",
    "df_yr.to_parquet(annual_pq, index=False)\n",
    "df_yr.to_excel  (annual_xls, index=False)\n",
    "print(f\"✓ Annual indices saved → {annual_pq}\")\n",
    "print(f\"✓ …and also saved as   → {annual_xls}\")\n",
    "\n",
    "# OPTIONAL: write one Excel file per index in the familiar “wide” format\n",
    "# ---------------------------------------------------------------------------\n",
    "index_roots = [\"TXx\", \"TNn\", \"TX90p\", \"TN10p\", \"FD\", \"WSDI\", \"CSDI\"]\n",
    "def _wide(idx_root: str) -> pd.DataFrame:\n",
    "    return (df_yr\n",
    "            .pivot_table(index=\"station_name\",\n",
    "                         values=[f\"{idx_root}_obs\", f\"{idx_root}_merra2\",\n",
    "                                 f\"{idx_root}_ratio\"])\n",
    "            .reset_index()\n",
    "            .merge(df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "                   on=\"station_name\", how=\"left\"))\n",
    "\n",
    "print(\"\\n(optional) individual workbooks …\")\n",
    "for idx in index_roots:\n",
    "    w = _wide(idx)\n",
    "    fp = os.path.join(output_dir, f\"{idx}.xlsx\")\n",
    "    w.to_excel(fp, index=False)\n",
    "    print(\"  •\", os.path.basename(fp))\n",
    "\n",
    "print(\"\\n✅  Annual-index workflow finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca275a-8ae1-4e46-82da-6b4a4b2dc955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal stratification of climatic indices MERRA2 for having the seasonal indices\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIGURATION\n",
    "###############################################################################\n",
    "csv_file      = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\MERRA2_GLB_Precipitation\\daily_loop\\merra2_vs_stations_12Nearest_LWR_1991_2012.csv\"\n",
    "physical_file = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv\"\n",
    "output_dir    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\MERRA2_GLB_Precipitation\\ClimaticIndices-Seasonal\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY DATA & ADD TEMPORAL FIELDS\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# Standardize station_name\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Add month (1..12) and season (DJF, MAM, JJA, SON)\n",
    "df_data[\"month\"] = df_data[\"time\"].dt.month\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"DJF\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"MAM\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"JJA\"\n",
    "    else:\n",
    "        return \"SON\"\n",
    "\n",
    "df_data[\"season\"] = df_data[\"month\"].apply(get_season)\n",
    "\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. LOAD PHYSICAL FILE & MERGE COORDINATES\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "###############################################################################\n",
    "def calc_rx1day(series):\n",
    "    \"\"\"Max 1-day precipitation.\"\"\"\n",
    "    return series.max(skipna=True)\n",
    "\n",
    "def calc_rx5day(series):\n",
    "    \"\"\"Max 5-day running sum.\"\"\"\n",
    "    return series.rolling(5, min_periods=1).sum().max(skipna=True)\n",
    "\n",
    "def calc_cdd(series, dry_threshold=1.0):\n",
    "    \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "    is_dry = series < dry_threshold\n",
    "    max_run, current_run = 0, 0\n",
    "    for val in is_dry:\n",
    "        if val:\n",
    "            current_run += 1\n",
    "            max_run = max(max_run, current_run)\n",
    "        else:\n",
    "            current_run = 0\n",
    "    return max_run\n",
    "\n",
    "def calc_cwd(series, wet_threshold=1.0):\n",
    "    \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "    is_wet = series >= wet_threshold\n",
    "    max_run, current_run = 0, 0\n",
    "    for val in is_wet:\n",
    "        if val:\n",
    "            current_run += 1\n",
    "            max_run = max(max_run, current_run)\n",
    "        else:\n",
    "            current_run = 0\n",
    "    return max_run\n",
    "\n",
    "def calc_r95p_r99p(series, percentiles=(95,99)):\n",
    "    \"\"\"\n",
    "    r95amt, r95pct, r99amt, r99pct:\n",
    "    - r95amt = sum of daily prcp above 95th percentile\n",
    "    - r95pct = (r95amt / total) * 100\n",
    "    - similarly for 99th percentile\n",
    "    \"\"\"\n",
    "    wet = series[series >= 1.0]\n",
    "    if len(wet) < 5:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    p95 = np.percentile(wet, percentiles[0])\n",
    "    p99 = np.percentile(wet, percentiles[1])\n",
    "    r95_amt = wet[wet > p95].sum()\n",
    "    r99_amt = wet[wet > p99].sum()\n",
    "    total = series.sum(skipna=True)\n",
    "    r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "    r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "    return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "\n",
    "def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "    \"\"\"\n",
    "    wetdays = #days >= wet_thr\n",
    "    drydays = #days < dry_thr\n",
    "    \"\"\"\n",
    "    return (series >= wet_thr).sum(), (series < dry_thr).sum()\n",
    "\n",
    "###############################################################################\n",
    "# 5. FUNCTION TO COMPUTE INDICES FOR A GROUP (MONTHLY or SEASONAL)\n",
    "###############################################################################\n",
    "def compute_indices(df_group):\n",
    "    \"\"\"\n",
    "    For a subset of daily data (e.g. station+month, or station+season),\n",
    "    compute the climate indices for Obs vs MERRA2, plus ratio columns.\n",
    "    \"\"\"\n",
    "    obs_series = df_group[\"obs\"].dropna().reset_index(drop=True)\n",
    "    merra2_series = df_group[\"merra2_lwr12_val\"].dropna().reset_index(drop=True)\n",
    "    if len(obs_series) == 0 or len(merra2_series) == 0:\n",
    "        return None\n",
    "    \n",
    "    res = {}\n",
    "    # Rx1day / Rx5day\n",
    "    res[\"rx1day_obs\"] = calc_rx1day(obs_series)\n",
    "    res[\"rx1day_merra2\"] = calc_rx1day(merra2_series)\n",
    "    res[\"rx5day_obs\"] = calc_rx5day(obs_series)\n",
    "    res[\"rx5day_merra2\"] = calc_rx5day(merra2_series)\n",
    "    \n",
    "    # CDD / CWD\n",
    "    res[\"cdd_obs\"] = calc_cdd(obs_series)\n",
    "    res[\"cdd_merra2\"] = calc_cdd(merra2_series)\n",
    "    res[\"cwd_obs\"] = calc_cwd(obs_series)\n",
    "    res[\"cwd_merra2\"] = calc_cwd(merra2_series)\n",
    "    \n",
    "    # R95 / R99\n",
    "    r95_obs = calc_r95p_r99p(obs_series)\n",
    "    r95_merra2 = calc_r95p_r99p(merra2_series)\n",
    "    res[\"r95amt_obs\"] = r95_obs[0]\n",
    "    res[\"r95pct_obs\"] = r95_obs[1]\n",
    "    res[\"r95amt_merra2\"] = r95_merra2[0]\n",
    "    res[\"r95pct_merra2\"] = r95_merra2[1]\n",
    "    res[\"r99amt_obs\"] = r95_obs[2]\n",
    "    res[\"r99pct_obs\"] = r95_obs[3]\n",
    "    res[\"r99amt_merra2\"] = r95_merra2[2]\n",
    "    res[\"r99pct_merra2\"] = r95_merra2[3]\n",
    "    \n",
    "    # Wet / Dry days\n",
    "    wet_obs, dry_obs = calc_wetdays_drydays(obs_series)\n",
    "    wet_merra2, dry_merra2 = calc_wetdays_drydays(merra2_series)\n",
    "    res[\"wetdays_obs\"] = wet_obs\n",
    "    res[\"wetdays_merra2\"] = wet_merra2\n",
    "    res[\"drydays_obs\"] = dry_obs\n",
    "    res[\"drydays_merra2\"] = dry_merra2\n",
    "    \n",
    "    # Ratio columns: MERRA2/OBS if obs != 0\n",
    "    if res[\"rx1day_obs\"]:\n",
    "        res[\"rx1day_ratio\"] = res[\"rx1day_merra2\"] / res[\"rx1day_obs\"]\n",
    "    if res[\"rx5day_obs\"]:\n",
    "        res[\"rx5day_ratio\"] = res[\"rx5day_merra2\"] / res[\"rx5day_obs\"]\n",
    "    if res[\"cdd_obs\"]:\n",
    "        res[\"cdd_ratio\"] = res[\"cdd_merra2\"] / res[\"cdd_obs\"]\n",
    "    if res[\"cwd_obs\"]:\n",
    "        res[\"cwd_ratio\"] = res[\"cwd_merra2\"] / res[\"cwd_obs\"]\n",
    "    if res[\"r95amt_obs\"]:\n",
    "        res[\"r95amt_ratio\"] = res[\"r95amt_merra2\"] / res[\"r95amt_obs\"]\n",
    "    if res[\"r95pct_obs\"]:\n",
    "        res[\"r95pct_ratio\"] = res[\"r95pct_merra2\"] / res[\"r95pct_obs\"]\n",
    "    if res[\"r99amt_obs\"]:\n",
    "        res[\"r99amt_ratio\"] = res[\"r99amt_merra2\"] / res[\"r99amt_obs\"]\n",
    "    if res[\"r99pct_obs\"]:\n",
    "        res[\"r99pct_ratio\"] = res[\"r99pct_merra2\"] / res[\"r99pct_obs\"]\n",
    "    if res[\"wetdays_obs\"]:\n",
    "        res[\"wetdays_ratio\"] = res[\"wetdays_merra2\"] / res[\"wetdays_obs\"]\n",
    "    if res[\"drydays_obs\"]:\n",
    "        res[\"drydays_ratio\"] = res[\"drydays_merra2\"] / res[\"drydays_obs\"]\n",
    "    \n",
    "    return res\n",
    "\n",
    "###############################################################################\n",
    "# 6. MONTHLY INDICES\n",
    "###############################################################################\n",
    "monthly_results = []\n",
    "group_month = df_data.groupby([\"station_name\", \"month\"])\n",
    "for (st_name, mon), group in group_month:\n",
    "    indices = compute_indices(group)\n",
    "    if indices is None:\n",
    "        continue\n",
    "    indices[\"station_name\"] = st_name\n",
    "    indices[\"month\"] = mon\n",
    "    monthly_results.append(indices)\n",
    "\n",
    "df_monthly = pd.DataFrame(monthly_results)\n",
    "df_monthly = pd.merge(\n",
    "    df_monthly,\n",
    "    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_monthly = df_monthly.sort_values([\"station_name\", \"month\"])\n",
    "monthly_out = os.path.join(output_dir, \"Indices_Monthly.xlsx\")\n",
    "df_monthly.to_excel(monthly_out, index=False)\n",
    "print(\"Monthly indices saved =>\", monthly_out)\n",
    "\n",
    "###############################################################################\n",
    "# 7. SEASONAL INDICES\n",
    "###############################################################################\n",
    "seasonal_results = []\n",
    "group_season = df_data.groupby([\"station_name\", \"season\"])\n",
    "for (st_name, seas), group in group_season:\n",
    "    indices = compute_indices(group)\n",
    "    if indices is None:\n",
    "        continue\n",
    "    indices[\"station_name\"] = st_name\n",
    "    indices[\"season\"] = seas\n",
    "    seasonal_results.append(indices)\n",
    "\n",
    "df_seasonal = pd.DataFrame(seasonal_results)\n",
    "df_seasonal = pd.merge(\n",
    "    df_seasonal,\n",
    "    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_seasonal = df_seasonal.sort_values([\"station_name\", \"season\"])\n",
    "seasonal_out = os.path.join(output_dir, \"Indices_Seasonal.xlsx\")\n",
    "df_seasonal.to_excel(seasonal_out, index=False)\n",
    "print(\"Seasonal indices saved =>\", seasonal_out)\n",
    "\n",
    "###############################################################################\n",
    "# 8. DONE\n",
    "###############################################################################\n",
    "print(\"\\nAll monthly and seasonal indices have been saved. (No extreme-event stratification.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73218415-cf61-49d4-98b4-4aecc1d41dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DJF for MERRA2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import Point\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIG & PATHS\n",
    "###############################################################################\n",
    "indices_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\MERRA2_GLB_Precipitation\\ClimaticIndices-Seasonal\"\n",
    "seasonal_file = os.path.join(indices_dir, \"Indices_Seasonal.xlsx\")  # single file\n",
    "output_plots  = os.path.join(indices_dir, \"AnalysisPlots_DJF\")\n",
    "os.makedirs(output_plots, exist_ok=True)\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "# Indices in your seasonal file\n",
    "index_list = [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"r95p\",\"r99p\",\"wetdays\",\"drydays\"]\n",
    "\n",
    "# For summary stats, define how to find obs vs. emd columns\n",
    "index_columns = {\n",
    "    \"rx1day\":  (\"rx1day_obs\",  \"rx1day_merra2\"),\n",
    "    \"rx5day\":  (\"rx5day_obs\",  \"rx5day_merra2\"),\n",
    "    \"cdd\":     (\"cdd_obs\",     \"cdd_merra2\"),\n",
    "    \"cwd\":     (\"cwd_obs\",     \"cwd_merra2\"),\n",
    "    \"r95p\":    ((\"r95amt_obs\",\"r95pct_obs\"), (\"r95amt_merra2\",\"r95pct_merra2\")),\n",
    "    \"r99p\":    ((\"r99amt_obs\",\"r99pct_obs\"), (\"r99amt_merra2\",\"r99pct_merra2\")),\n",
    "    \"wetdays\": (\"wetdays_obs\",\"wetdays_merra2\"),\n",
    "    \"drydays\": (\"drydays_obs\",\"drydays_merra2\"),\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD SEASONAL FILE & FILTER TO DJF\n",
    "###############################################################################\n",
    "df_season = pd.read_excel(seasonal_file)\n",
    "print(\"Loaded =>\", seasonal_file, \"| shape =\", df_season.shape)\n",
    "\n",
    "# Filter to DJF\n",
    "df_season = df_season[df_season[\"season\"]==\"DJF\"].copy()\n",
    "df_season = df_season.dropna(subset=[\"lat\",\"lon\"])  # ensure lat/lon exist\n",
    "print(\"After filtering to DJF => shape =\", df_season.shape)\n",
    "\n",
    "mdf = df_season.reset_index(drop=True)\n",
    "master_xlsx = os.path.join(output_plots, \"MasterTable_Seasonal_DJF.xlsx\")\n",
    "mdf.to_excel(master_xlsx, index=False)\n",
    "print(f\"\\n(A) Master table (DJF) saved => {master_xlsx}\")\n",
    "print(\"Columns:\", mdf.columns.tolist())\n",
    "\n",
    "###############################################################################\n",
    "# 3. SUMMARY TABLE (MBE, RMSE, STD, CC, d) for DJF\n",
    "###############################################################################\n",
    "def index_of_agreement(obs, model):\n",
    "    obs_mean = np.mean(obs)\n",
    "    num = np.sum((model - obs)**2)\n",
    "    den = np.sum((abs(model - obs_mean) + abs(obs - obs_mean))**2)\n",
    "    if den == 0:\n",
    "        return np.nan\n",
    "    return 1 - num/den\n",
    "\n",
    "def rmse(a, b):\n",
    "    return np.sqrt(np.mean((a-b)**2))\n",
    "\n",
    "def std_of_residuals(a, b):\n",
    "    return np.std(a-b, ddof=1)\n",
    "\n",
    "def mean_bias_error(a, b):\n",
    "    return np.mean(b-a)\n",
    "\n",
    "summary_rows = []\n",
    "for idx_name in index_list:\n",
    "    obs_cols = index_columns[idx_name][0]\n",
    "    merra2_cols = index_columns[idx_name][1]\n",
    "\n",
    "    if isinstance(obs_cols, tuple):\n",
    "        # multiple columns\n",
    "        for oc, ec in zip(obs_cols, merra2_cols):\n",
    "            valid = mdf[[oc, ec]].dropna()\n",
    "            if len(valid) < 2:\n",
    "                continue\n",
    "            obs_vals = valid[oc].values\n",
    "            merra2_vals = valid[ec].values\n",
    "            MB  = mean_bias_error(obs_vals, merra2_vals)\n",
    "            RM  = rmse(obs_vals, merra2_vals)\n",
    "            SR  = std_of_residuals(obs_vals, merra2_vals)\n",
    "            CC  = pearsonr(obs_vals, merra2_vals)[0] if len(obs_vals) > 1 else np.nan\n",
    "            dd  = index_of_agreement(obs_vals, merra2_vals)\n",
    "            idx_label = f\"{idx_name}_{oc.replace('_obs','')}\"\n",
    "            summary_rows.append({\n",
    "                \"Index\": idx_label,\n",
    "                \"Count\": len(valid),\n",
    "                \"MBE\": MB,\n",
    "                \"RMSE\": RM,\n",
    "                \"STDres\": SR,\n",
    "                \"CC\": CC,\n",
    "                \"d\": dd,\n",
    "            })\n",
    "    else:\n",
    "        oc = obs_cols\n",
    "        ec = merra2_cols\n",
    "        valid = mdf[[oc, ec]].dropna()\n",
    "        if len(valid) < 2:\n",
    "            continue\n",
    "        obs_vals = valid[oc].values\n",
    "        merra2_vals = valid[ec].values\n",
    "        MB = mean_bias_error(obs_vals, merra2_vals)\n",
    "        RM = rmse(obs_vals, merra2_vals)\n",
    "        SR = std_of_residuals(obs_vals, merra2_vals)\n",
    "        CC = pearsonr(obs_vals, merra2_vals)[0] if len(obs_vals) > 1 else np.nan\n",
    "        dd = index_of_agreement(obs_vals, merra2_vals)\n",
    "        summary_rows.append({\n",
    "            \"Index\": idx_name,\n",
    "            \"Count\": len(valid),\n",
    "            \"MBE\": MB,\n",
    "            \"RMSE\": RM,\n",
    "            \"STDres\": SR,\n",
    "            \"CC\": CC,\n",
    "            \"d\": dd,\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_cols = [\"Index\",\"Count\",\"MBE\",\"RMSE\",\"STDres\",\"CC\",\"d\"]\n",
    "summary_df = summary_df[summary_cols]\n",
    "summary_xlsx = os.path.join(output_plots, \"SummaryTable_Extremes_DJF.xlsx\")\n",
    "summary_df.to_excel(summary_xlsx, index=False)\n",
    "print(f\"(B) Summary Table (DJF) => {summary_xlsx}\\n{summary_df}\")\n",
    "\n",
    "###############################################################################\n",
    "# 4. MAPPING: Combine Observed, MERRA2, Ratio in One Figure\n",
    "###############################################################################\n",
    "gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "\n",
    "def add_basin_lakes(ax):\n",
    "    #ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    for geom in gdf_basin.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(),\n",
    "                          facecolor='none', edgecolor='black', linewidth=1)\n",
    "    for geom in gdf_lakes.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(),\n",
    "                          facecolor='none', edgecolor='cyan', linewidth=1)\n",
    "\n",
    "def plot_map_triptych(df, obs_col, merra2_col, ratio_col, idx_name, out_png):\n",
    "    \"\"\"\n",
    "    Creates a single figure with 3 subplots (side-by-side):\n",
    "      1) Observed\n",
    "      2) MERRA2\n",
    "      3) Ratio (MERRA2/OBS)\n",
    "    Each subplot has a colorbar, a 90th-percentile hotspot circle, etc.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6),\n",
    "                             subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "\n",
    "    # We'll define a small helper to do each subplot\n",
    "    def scatter_map(ax, value_col, title):\n",
    "        ax.set_extent([-95.5, -72, 38.5, 52.5])\n",
    "        add_basin_lakes(ax)\n",
    "        sc = ax.scatter(df[\"lon\"], df[\"lat\"], c=df[value_col], cmap=\"viridis\",\n",
    "                        s=60, transform=ccrs.PlateCarree(), edgecolor=\"k\", zorder=10)\n",
    "        cb = plt.colorbar(sc, ax=ax, shrink=0.8)\n",
    "        cb.set_label(value_col)\n",
    "\n",
    "        # Hotspots => top 10%\n",
    "        vals = df[value_col].dropna().values\n",
    "        if len(vals) > 0:\n",
    "            thr = np.percentile(vals, 90)\n",
    "            is_hot = df[value_col]>=thr\n",
    "            ax.scatter(df.loc[is_hot,\"lon\"], df.loc[is_hot,\"lat\"],\n",
    "                       marker='o', facecolors='none', edgecolors='red', s=80,\n",
    "                       transform=ccrs.PlateCarree(), zorder=11,\n",
    "                       label=f\"Hotspot >= {thr:.2f}\")\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        gl = ax.gridlines(draw_labels=True, linestyle='--', color='gray')\n",
    "        gl.right_labels = False\n",
    "        gl.top_labels   = False\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    scatter_map(axes[0], obs_col,  f\"{idx_name} Observed (DJF)\")\n",
    "    scatter_map(axes[1], merra2_col,  f\"{idx_name} MERRA2 (DJF)\")\n",
    "    scatter_map(axes[2], ratio_col,f\"{idx_name} (MERRA2/OBS) (DJF)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved =>\", out_png)\n",
    "\n",
    "def get_map_cols(idx_name):\n",
    "    if idx_name in [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"drydays\"]:\n",
    "        obs  = f\"{idx_name}_obs\"\n",
    "        merra2  = f\"{idx_name}_merra2\"\n",
    "        ratio= f\"{idx_name}_ratio\"\n",
    "        return obs, merra2, ratio\n",
    "    elif idx_name == \"wetdays\":\n",
    "        obs  = \"wetdays_obs\"\n",
    "        merra2  = \"wetdays_merra2\"\n",
    "        ratio= \"wetdays_ratio\"\n",
    "        return obs, merra2, ratio\n",
    "    elif idx_name == \"r95p\":\n",
    "        obs  = \"r95amt_obs\"\n",
    "        merra2  = \"r95amt_merra2\"\n",
    "        ratio= \"r95amt_ratio\"\n",
    "        return obs, merra2, ratio\n",
    "    elif idx_name == \"r99p\":\n",
    "        obs  = \"r99amt_obs\"\n",
    "        merra2  = \"r99amt_merra2\"\n",
    "        ratio= \"r99amt_ratio\"\n",
    "        return obs, merra2, ratio\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "for idx_name in index_list:\n",
    "    obs_col, merra2_col, ratio_col = get_map_cols(idx_name)\n",
    "    if obs_col is None:\n",
    "        continue\n",
    "\n",
    "    needed_cols = [obs_col, merra2_col, ratio_col, \"lat\", \"lon\"]\n",
    "    if not all(c in mdf.columns for c in needed_cols):\n",
    "        print(f\"Skipping map for {idx_name} - missing columns.\")\n",
    "        continue\n",
    "\n",
    "    subdf = mdf.dropna(subset=[\"lat\",\"lon\"]).copy()\n",
    "    out_png = os.path.join(output_plots, f\"DJF_{idx_name}_MAP_3panel.png\")\n",
    "    plot_map_triptych(subdf, obs_col, merra2_col, ratio_col, idx_name, out_png)\n",
    "\n",
    "###############################################################################\n",
    "# 5. DISTRIBUTION & BOX/CDF/Scatter in One Figure\n",
    "###############################################################################\n",
    "def plot_distribution_triptych(df, obs_col, merra2_col, label, out_png):\n",
    "    \"\"\"\n",
    "    Creates a single figure with 3 subplots side-by-side:\n",
    "      1) Boxplot\n",
    "      2) CDF\n",
    "      3) Scatter\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,6))\n",
    "\n",
    "    # A) Boxplot\n",
    "    ax_box = axes[0]\n",
    "    data = pd.DataFrame({\"Obs\": df[obs_col], \"MERRA2\": df[merra2_col]}).melt(\n",
    "        var_name=\"Dataset\", value_name=label\n",
    "    )\n",
    "    sns.boxplot(data=data, x=\"Dataset\", y=label, ax=ax_box)\n",
    "    ax_box.set_title(f\"Boxplot: {label} (DJF)\")\n",
    "\n",
    "    # B) CDF\n",
    "    ax_cdf = axes[1]\n",
    "    obs_vals = df[obs_col].dropna()\n",
    "    merra2_vals = df[merra2_col].dropna()\n",
    "\n",
    "    def ecdf(x):\n",
    "        xs = np.sort(x)\n",
    "        ys = np.arange(1, len(xs)+1)/len(xs)\n",
    "        return xs, ys\n",
    "\n",
    "    if len(obs_vals)>=2 and len(merra2_vals)>=2:\n",
    "        xs_o, ys_o = ecdf(obs_vals)\n",
    "        xs_e, ys_e = ecdf(merra2_vals)\n",
    "        ax_cdf.plot(xs_o, ys_o, label=\"Obs\")\n",
    "        ax_cdf.plot(xs_e, ys_e, label=\"MERRA2\")\n",
    "        ax_cdf.set_title(f\"CDF of {label} (DJF)\")\n",
    "        ax_cdf.set_xlabel(label)\n",
    "        ax_cdf.set_ylabel(\"Probability\")\n",
    "        ax_cdf.legend()\n",
    "    else:\n",
    "        ax_cdf.set_title(f\"CDF: not enough data ({label})\")\n",
    "\n",
    "    # C) Scatter\n",
    "    ax_scat = axes[2]\n",
    "    valid = df[[obs_col, merra2_col]].dropna()\n",
    "    if len(valid)>=2:\n",
    "        x = valid[obs_col]\n",
    "        y = valid[merra2_col]\n",
    "        cc, _ = pearsonr(x, y)\n",
    "        ax_scat.scatter(x, y, edgecolors='k', alpha=0.7)\n",
    "        mn, mx = np.nanmin([x.min(), y.min()]), np.nanmax([x.max(), y.max()])\n",
    "        ax_scat.plot([mn, mx],[mn, mx],'r--')\n",
    "        ax_scat.set_xlabel(f\"Obs {label} (DJF)\")\n",
    "        ax_scat.set_ylabel(f\"MERRA2 {label} (DJF)\")\n",
    "        ax_scat.set_title(f\"{label} (Corr={cc:.2f}, DJF)\")\n",
    "    else:\n",
    "        ax_scat.set_title(f\"Scatter: not enough data ({label})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved =>\", out_png)\n",
    "\n",
    "for idx_name in index_list:\n",
    "    # figure out obs, emd columns\n",
    "    if idx_name in [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"drydays\"]:\n",
    "        obs_col  = f\"{idx_name}_obs\"\n",
    "        merra2_col  = f\"{idx_name}_merra2\"\n",
    "    elif idx_name == \"wetdays\":\n",
    "        obs_col  = \"wetdays_obs\"\n",
    "        merra2_col  = \"wetdays_merra2\"\n",
    "    elif idx_name == \"r95p\":\n",
    "        obs_col  = \"r95amt_obs\"\n",
    "        merra2_col  = \"r95amt_merra2\"\n",
    "    elif idx_name == \"r99p\":\n",
    "        obs_col  = \"r99amt_obs\"\n",
    "        merra2_col  = \"r99amt_merra2\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    if obs_col not in mdf.columns or merra2_col not in mdf.columns:\n",
    "        print(f\"Skipping distribution for {idx_name} - missing columns.\")\n",
    "        continue\n",
    "\n",
    "    subdf = mdf[[obs_col, merra2_col]].dropna()\n",
    "    if len(subdf)<2:\n",
    "        print(f\"Skipping distribution for {idx_name} - not enough data.\")\n",
    "        continue\n",
    "\n",
    "    out_3panel = os.path.join(output_plots, f\"DJF_{idx_name}_Distribution_3panel.png\")\n",
    "    plot_distribution_triptych(subdf, obs_col, merra2_col, idx_name, out_3panel)\n",
    "\n",
    "###############################################################################\n",
    "# 6. DONE\n",
    "###############################################################################\n",
    "print(\"\\nAll DJF steps completed! See outputs in:\", output_plots)\n",
    "\n",
    "# For the other seasons, just change any DJF to JJA, MAM, or SON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc713e-c1cf-41f1-a197-0d27ffd7e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "                 #########################                                           ##############################\n",
    "                 #########################                  CHIRPS                   ##############################\n",
    "                 #########################                                           ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e48af-517d-4764-afa6-c2edafd88488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the climatic indices for CHIRPS \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIG\n",
    "###############################################################################\n",
    "csv_file       = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\CHIRPS_GLB_Precipitation\\Masked to GLB\\daily_loop\\chirps_vs_stations_12Nearest_LWR_1991_2012.csv\"\n",
    "physical_file  = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv\"\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "output_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\CHIRPS_GLB_Precipitation\\Masked to GLB\\ClimaticIndices-12Nearest\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY CSV DATA\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data (obs, chirps_lwr12_val) ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "# parse 'time' as datetime if needed\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# Unify station_name: remove leading/trailing spaces, uppercase\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(f\"df_data shape = {df_data.shape}\")\n",
    "print(\"Columns:\", df_data.columns.tolist())\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. (OPTIONAL) MERGE WITH PHYSICAL FILE TO GET LAT/LON\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "# Unify station_name\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "###############################################################################\n",
    "def calc_rx1day(series):\n",
    "    \"\"\"Max 1-day precipitation.\"\"\"\n",
    "    return series.max(skipna=True)\n",
    "\n",
    "def calc_rx5day(series):\n",
    "    \"\"\"Max 5-day running sum.\"\"\"\n",
    "    roll_5 = series.rolling(5, min_periods=1).sum()\n",
    "    return roll_5.max(skipna=True)\n",
    "\n",
    "def calc_cdd(series, dry_threshold=1.0):\n",
    "    \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "    is_dry = series < dry_threshold\n",
    "    maxr, curr = 0, 0\n",
    "    for val in is_dry:\n",
    "        if val:\n",
    "            curr += 1\n",
    "            maxr = max(maxr, curr)\n",
    "        else:\n",
    "            curr = 0\n",
    "    return maxr\n",
    "\n",
    "def calc_cwd(series, wet_threshold=1.0):\n",
    "    \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "    is_wet = series >= wet_threshold\n",
    "    maxr, curr = 0, 0\n",
    "    for val in is_wet:\n",
    "        if val:\n",
    "            curr += 1\n",
    "            maxr = max(maxr, curr)\n",
    "        else:\n",
    "            curr = 0\n",
    "    return maxr\n",
    "\n",
    "def calc_r95p_r99p(series, percentile=(95,99)):\n",
    "    \"\"\"R95p, R99p total in mm, plus percentage of total precipitation.\"\"\"\n",
    "    wet = series[series >= 1.0]\n",
    "    if len(wet) < 5:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    p95 = np.percentile(wet, percentile[0])\n",
    "    p99 = np.percentile(wet, percentile[1])\n",
    "    r95_amt = wet[wet > p95].sum()\n",
    "    r99_amt = wet[wet > p99].sum()\n",
    "    total   = series.sum(skipna=True)\n",
    "    r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "    r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "    return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "\n",
    "def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "    \"\"\"Count number of wet days (>=5 mm) and dry days (<1 mm).\"\"\"\n",
    "    w = (series >= wet_thr).sum()\n",
    "    d = (series <  dry_thr).sum()\n",
    "    return w, d\n",
    "\n",
    "###############################################################################\n",
    "# 5. COMPUTE INDICES FOR EACH STATION\n",
    "###############################################################################\n",
    "rx1_list, rx5_list = [], []\n",
    "cdd_list, cwd_list = [], []\n",
    "r95_list, r99_list = [], []\n",
    "wet_list, dry_list = [], []\n",
    "\n",
    "print(\"Computing indices for each station...\")\n",
    "\n",
    "grouped = df_data.groupby(\"station_name\", as_index=False)\n",
    "for st_name, grp in grouped:\n",
    "    # Sort by time\n",
    "    grp = grp.sort_values(\"time\")\n",
    "    \n",
    "    # Daily series: observed and CHIRPS LWR interpolation\n",
    "    obs_series = grp[\"obs\"].dropna().reset_index(drop=True)\n",
    "    chirps_series = grp[\"chirps_lwr12_val\"].dropna().reset_index(drop=True)\n",
    "    \n",
    "    # A) Rx1day\n",
    "    obs_rx1 = calc_rx1day(obs_series)\n",
    "    chirps_rx1 = calc_rx1day(chirps_series)\n",
    "    rx1_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_rx1day\": obs_rx1,\n",
    "        \"chirps_rx1day\": chirps_rx1\n",
    "    })\n",
    "    \n",
    "    # B) Rx5day\n",
    "    obs_rx5 = calc_rx5day(obs_series)\n",
    "    chirps_rx5 = calc_rx5day(chirps_series)\n",
    "    rx5_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_rx5day\": obs_rx5,\n",
    "        \"chirps_rx5day\": chirps_rx5\n",
    "    })\n",
    "    \n",
    "    # C) CDD\n",
    "    obs_cdd_val = calc_cdd(obs_series)\n",
    "    chirps_cdd_val = calc_cdd(chirps_series)\n",
    "    cdd_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_cdd\": obs_cdd_val,\n",
    "        \"chirps_cdd\": chirps_cdd_val\n",
    "    })\n",
    "    \n",
    "    # D) CWD\n",
    "    obs_cwd_val = calc_cwd(obs_series)\n",
    "    chirps_cwd_val = calc_cwd(chirps_series)\n",
    "    cwd_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_cwd\": obs_cwd_val,\n",
    "        \"chirps_cwd\": chirps_cwd_val\n",
    "    })\n",
    "    \n",
    "    # E) R95 / R99\n",
    "    or95a, or95p, or99a, or99p = calc_r95p_r99p(obs_series)\n",
    "    cr95a, cr95p, cr99a, cr99p = calc_r95p_r99p(chirps_series)\n",
    "    r95_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_r95amt\": or95a,\n",
    "        \"obs_r95pct\": or95p,\n",
    "        \"chirps_r95amt\": cr95a,\n",
    "        \"chirps_r95pct\": cr95p\n",
    "    })\n",
    "    r99_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_r99amt\": or99a,\n",
    "        \"obs_r99pct\": or99p,\n",
    "        \"chirps_r99amt\": cr99a,\n",
    "        \"chirps_r99pct\": cr99p\n",
    "    })\n",
    "    \n",
    "    # F) Wet/Dry days\n",
    "    obs_wet5, obs_dry = calc_wetdays_drydays(obs_series)\n",
    "    chirps_wet5, chirps_dry = calc_wetdays_drydays(chirps_series)\n",
    "    wet_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_wetdays5mm\": obs_wet5,\n",
    "        \"chirps_wetdays5mm\": chirps_wet5\n",
    "    })\n",
    "    dry_list.append({\n",
    "        \"station_name\": st_name,\n",
    "        \"obs_drydays\": obs_dry,\n",
    "        \"chirps_drydays\": chirps_dry\n",
    "    })\n",
    "\n",
    "print(\"Finished computing. Now merging lat/lon from physical file ...\")\n",
    "\n",
    "def attach_coords(df_in):\n",
    "    \"\"\"Attach lat, lon, and elevation from physical file on station_name.\"\"\"\n",
    "    df_out = pd.merge(\n",
    "        df_in,\n",
    "        df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "        on=\"station_name\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    return df_out\n",
    "\n",
    "df_rx1 = attach_coords(pd.DataFrame(rx1_list))\n",
    "df_rx5 = attach_coords(pd.DataFrame(rx5_list))\n",
    "df_cdd = attach_coords(pd.DataFrame(cdd_list))\n",
    "df_cwd = attach_coords(pd.DataFrame(cwd_list))\n",
    "df_r95 = attach_coords(pd.DataFrame(r95_list))\n",
    "df_r99 = attach_coords(pd.DataFrame(r99_list))\n",
    "df_wet = attach_coords(pd.DataFrame(wet_list))\n",
    "df_dry = attach_coords(pd.DataFrame(dry_list))\n",
    "\n",
    "###############################################################################\n",
    "# 6. SAVE OUTPUT\n",
    "###############################################################################\n",
    "print(\"Saving index tables to Excel in:\", output_dir)\n",
    "df_rx1.to_excel(os.path.join(output_dir, \"rx1day.xlsx\"),  index=False)\n",
    "df_rx5.to_excel(os.path.join(output_dir, \"rx5day.xlsx\"),  index=False)\n",
    "df_cdd.to_excel(os.path.join(output_dir, \"cdd.xlsx\"),     index=False)\n",
    "df_cwd.to_excel(os.path.join(output_dir, \"cwd.xlsx\"),     index=False)\n",
    "df_r95.to_excel(os.path.join(output_dir, \"r95p.xlsx\"),    index=False)\n",
    "df_r99.to_excel(os.path.join(output_dir, \"r99p.xlsx\"),    index=False)\n",
    "df_wet.to_excel(os.path.join(output_dir, \"wetdays.xlsx\"), index=False)\n",
    "df_dry.to_excel(os.path.join(output_dir, \"drydays.xlsx\"), index=False)\n",
    "\n",
    "print(\"\\nAll precipitation-based indices have been saved to Excel.\")\n",
    "\n",
    "###############################################################################\n",
    "# (OPTIONAL) QUICK MAP EXAMPLE\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nQuick map example for obs_rx5day ...\")\n",
    "    # Load shapefiles\n",
    "    gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "    gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        df_rx5,\n",
    "        geometry=gpd.points_from_xy(df_rx5[\"lon\"], df_rx5[\"lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "    for geom in gdf_basin.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='blue', linewidth=1)\n",
    "    for geom in gdf_lakes.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(), facecolor='none', edgecolor='cyan', linewidth=1)\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "    sc = ax.scatter(gdf_stations.geometry.x, gdf_stations.geometry.y,\n",
    "                    c=gdf_stations[\"obs_rx5day\"], cmap=\"Reds\", s=60,\n",
    "                    transform=ccrs.PlateCarree(), edgecolor=\"k\")\n",
    "    plt.colorbar(sc, ax=ax, label=\"Obs Rx5day (mm)\")\n",
    "    ax.set_extent([-95.5, -72, 38.5, 52.5])\n",
    "    gl = ax.gridlines(draw_labels=True, linestyle='--', color='gray')\n",
    "    gl.right_labels = False\n",
    "    gl.top_labels   = False\n",
    "\n",
    "    plt.title(\"Obs Rx5day (from CSV daily data)\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Mapping step failed:\", e)\n",
    "\n",
    "print(\"\\n✅ Done computing precipitation-based indices from 'chirps_lwr12_val' column!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03753c5-5992-4a13-b960-4697eb249cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal stratification of climatic indices CHIRPS for having the seasonal indices\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILE PATHS & CONFIGURATION\n",
    "###############################################################################\n",
    "csv_file      = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\CHIRPS_GLB_Precipitation\\Masked to GLB\\daily_loop\\chirps_vs_stations_25km_LWR_1991_2012.csv\"\n",
    "physical_file = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv\"\n",
    "output_dir    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\CHIRPS_GLB_Precipitation\\Masked to GLB\\ClimaticIndices-Seasonal\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD DAILY DATA & ADD TEMPORAL FIELDS\n",
    "###############################################################################\n",
    "print(\"Loading daily CSV data ...\")\n",
    "df_data = pd.read_csv(csv_file)\n",
    "df_data[\"time\"] = pd.to_datetime(df_data[\"time\"], errors=\"coerce\")\n",
    "\n",
    "# Standardize station_name\n",
    "df_data[\"station_name\"] = df_data[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Add month (1..12) and season (DJF, MAM, JJA, SON)\n",
    "df_data[\"month\"] = df_data[\"time\"].dt.month\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"DJF\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"MAM\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"JJA\"\n",
    "    else:\n",
    "        return \"SON\"\n",
    "\n",
    "df_data[\"season\"] = df_data[\"month\"].apply(get_season)\n",
    "\n",
    "print(\"Time range:\", df_data[\"time\"].min(), \"to\", df_data[\"time\"].max())\n",
    "\n",
    "###############################################################################\n",
    "# 3. LOAD PHYSICAL FILE & MERGE COORDINATES\n",
    "###############################################################################\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_phys = df_phys.rename(columns={\n",
    "    \"NAME\": \"station_name\",\n",
    "    \"LATITUDE\": \"lat\",\n",
    "    \"LONGITUDE\": \"lon\",\n",
    "    \"Elevation\": \"elev\"\n",
    "})\n",
    "df_phys[\"station_name\"] = df_phys[\"station_name\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "###############################################################################\n",
    "# 4. HELPER FUNCTIONS FOR CLIMATE INDICES\n",
    "###############################################################################\n",
    "def calc_rx1day(series):\n",
    "    \"\"\"Max 1-day precipitation.\"\"\"\n",
    "    return series.max(skipna=True)\n",
    "\n",
    "def calc_rx5day(series):\n",
    "    \"\"\"Max 5-day running sum.\"\"\"\n",
    "    return series.rolling(5, min_periods=1).sum().max(skipna=True)\n",
    "\n",
    "def calc_cdd(series, dry_threshold=1.0):\n",
    "    \"\"\"Longest run of days < dry_threshold.\"\"\"\n",
    "    is_dry = series < dry_threshold\n",
    "    max_run, current_run = 0, 0\n",
    "    for val in is_dry:\n",
    "        if val:\n",
    "            current_run += 1\n",
    "            max_run = max(max_run, current_run)\n",
    "        else:\n",
    "            current_run = 0\n",
    "    return max_run\n",
    "\n",
    "def calc_cwd(series, wet_threshold=1.0):\n",
    "    \"\"\"Longest run of days >= wet_threshold.\"\"\"\n",
    "    is_wet = series >= wet_threshold\n",
    "    max_run, current_run = 0, 0\n",
    "    for val in is_wet:\n",
    "        if val:\n",
    "            current_run += 1\n",
    "            max_run = max(max_run, current_run)\n",
    "        else:\n",
    "            current_run = 0\n",
    "    return max_run\n",
    "\n",
    "def calc_r95p_r99p(series, percentiles=(95,99)):\n",
    "    \"\"\"\n",
    "    r95amt, r95pct, r99amt, r99pct:\n",
    "    - r95amt = sum of daily prcp above 95th percentile\n",
    "    - r95pct = (r95amt / total) * 100\n",
    "    - similarly for 99th percentile\n",
    "    \"\"\"\n",
    "    wet = series[series >= 1.0]\n",
    "    if len(wet) < 5:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    p95 = np.percentile(wet, percentiles[0])\n",
    "    p99 = np.percentile(wet, percentiles[1])\n",
    "    r95_amt = wet[wet > p95].sum()\n",
    "    r99_amt = wet[wet > p99].sum()\n",
    "    total = series.sum(skipna=True)\n",
    "    r95_pct = (r95_amt / total) * 100 if total > 0 else np.nan\n",
    "    r99_pct = (r99_amt / total) * 100 if total > 0 else np.nan\n",
    "    return r95_amt, r95_pct, r99_amt, r99_pct\n",
    "\n",
    "def calc_wetdays_drydays(series, wet_thr=5.0, dry_thr=1.0):\n",
    "    \"\"\"\n",
    "    wetdays = #days >= wet_thr\n",
    "    drydays = #days < dry_thr\n",
    "    \"\"\"\n",
    "    return (series >= wet_thr).sum(), (series < dry_thr).sum()\n",
    "\n",
    "###############################################################################\n",
    "# 5. FUNCTION TO COMPUTE INDICES FOR A GROUP (MONTHLY or SEASONAL)\n",
    "###############################################################################\n",
    "def compute_indices(df_group):\n",
    "    \"\"\"\n",
    "    For a subset of daily data (e.g. station+month, or station+season),\n",
    "    compute the climate indices for Obs vs CHIRPS, plus ratio columns.\n",
    "    \"\"\"\n",
    "    obs_series = df_group[\"obs\"].dropna().reset_index(drop=True)\n",
    "    chirps_series = df_group[\"chirps_val\"].dropna().reset_index(drop=True)\n",
    "    if len(obs_series) == 0 or len(chirps_series) == 0:\n",
    "        return None\n",
    "    \n",
    "    res = {}\n",
    "    # Rx1day / Rx5day\n",
    "    res[\"rx1day_obs\"] = calc_rx1day(obs_series)\n",
    "    res[\"rx1day_chirps\"] = calc_rx1day(chirps_series)\n",
    "    res[\"rx5day_obs\"] = calc_rx5day(obs_series)\n",
    "    res[\"rx5day_chirps\"] = calc_rx5day(chirps_series)\n",
    "    \n",
    "    # CDD / CWD\n",
    "    res[\"cdd_obs\"] = calc_cdd(obs_series)\n",
    "    res[\"cdd_chirps\"] = calc_cdd(chirps_series)\n",
    "    res[\"cwd_obs\"] = calc_cwd(obs_series)\n",
    "    res[\"cwd_chirps\"] = calc_cwd(chirps_series)\n",
    "    \n",
    "    # R95 / R99\n",
    "    r95_obs = calc_r95p_r99p(obs_series)\n",
    "    r95_chirps = calc_r95p_r99p(chirps_series)\n",
    "    res[\"r95amt_obs\"] = r95_obs[0]\n",
    "    res[\"r95pct_obs\"] = r95_obs[1]\n",
    "    res[\"r95amt_chirps\"] = r95_chirps[0]\n",
    "    res[\"r95pct_chirps\"] = r95_chirps[1]\n",
    "    res[\"r99amt_obs\"] = r95_obs[2]\n",
    "    res[\"r99pct_obs\"] = r95_obs[3]\n",
    "    res[\"r99amt_chirps\"] = r95_chirps[2]\n",
    "    res[\"r99pct_chirps\"] = r95_chirps[3]\n",
    "    \n",
    "    # Wet / Dry days\n",
    "    wet_obs, dry_obs = calc_wetdays_drydays(obs_series)\n",
    "    wet_chirps, dry_chirps = calc_wetdays_drydays(chirps_series)\n",
    "    res[\"wetdays_obs\"] = wet_obs\n",
    "    res[\"wetdays_chirps\"] = wet_chirps\n",
    "    res[\"drydays_obs\"] = dry_obs\n",
    "    res[\"drydays_chirps\"] = dry_chirps\n",
    "    \n",
    "    # Ratio columns: CHIRPS/OBS if obs != 0\n",
    "    if res[\"rx1day_obs\"]:\n",
    "        res[\"rx1day_ratio\"] = res[\"rx1day_chirps\"] / res[\"rx1day_obs\"]\n",
    "    if res[\"rx5day_obs\"]:\n",
    "        res[\"rx5day_ratio\"] = res[\"rx5day_chirps\"] / res[\"rx5day_obs\"]\n",
    "    if res[\"cdd_obs\"]:\n",
    "        res[\"cdd_ratio\"] = res[\"cdd_chirps\"] / res[\"cdd_obs\"]\n",
    "    if res[\"cwd_obs\"]:\n",
    "        res[\"cwd_ratio\"] = res[\"cwd_chirps\"] / res[\"cwd_obs\"]\n",
    "    if res[\"r95amt_obs\"]:\n",
    "        res[\"r95amt_ratio\"] = res[\"r95amt_chirps\"] / res[\"r95amt_obs\"]\n",
    "    if res[\"r95pct_obs\"]:\n",
    "        res[\"r95pct_ratio\"] = res[\"r95pct_chirps\"] / res[\"r95pct_obs\"]\n",
    "    if res[\"r99amt_obs\"]:\n",
    "        res[\"r99amt_ratio\"] = res[\"r99amt_chirps\"] / res[\"r99amt_obs\"]\n",
    "    if res[\"r99pct_obs\"]:\n",
    "        res[\"r99pct_ratio\"] = res[\"r99pct_chirps\"] / res[\"r99pct_obs\"]\n",
    "    if res[\"wetdays_obs\"]:\n",
    "        res[\"wetdays_ratio\"] = res[\"wetdays_chirps\"] / res[\"wetdays_obs\"]\n",
    "    if res[\"drydays_obs\"]:\n",
    "        res[\"drydays_ratio\"] = res[\"drydays_chirps\"] / res[\"drydays_obs\"]\n",
    "    \n",
    "    return res\n",
    "\n",
    "###############################################################################\n",
    "# 6. MONTHLY INDICES\n",
    "###############################################################################\n",
    "monthly_results = []\n",
    "group_month = df_data.groupby([\"station_name\", \"month\"])\n",
    "for (st_name, mon), group in group_month:\n",
    "    indices = compute_indices(group)\n",
    "    if indices is None:\n",
    "        continue\n",
    "    indices[\"station_name\"] = st_name\n",
    "    indices[\"month\"] = mon\n",
    "    monthly_results.append(indices)\n",
    "\n",
    "df_monthly = pd.DataFrame(monthly_results)\n",
    "df_monthly = pd.merge(\n",
    "    df_monthly,\n",
    "    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_monthly = df_monthly.sort_values([\"station_name\", \"month\"])\n",
    "monthly_out = os.path.join(output_dir, \"Indices_Monthly.xlsx\")\n",
    "df_monthly.to_excel(monthly_out, index=False)\n",
    "print(\"Monthly indices saved =>\", monthly_out)\n",
    "\n",
    "###############################################################################\n",
    "# 7. SEASONAL INDICES\n",
    "###############################################################################\n",
    "seasonal_results = []\n",
    "group_season = df_data.groupby([\"station_name\", \"season\"])\n",
    "for (st_name, seas), group in group_season:\n",
    "    indices = compute_indices(group)\n",
    "    if indices is None:\n",
    "        continue\n",
    "    indices[\"station_name\"] = st_name\n",
    "    indices[\"season\"] = seas\n",
    "    seasonal_results.append(indices)\n",
    "\n",
    "df_seasonal = pd.DataFrame(seasonal_results)\n",
    "df_seasonal = pd.merge(\n",
    "    df_seasonal,\n",
    "    df_phys[[\"station_name\", \"lat\", \"lon\", \"elev\"]],\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_seasonal = df_seasonal.sort_values([\"station_name\", \"season\"])\n",
    "seasonal_out = os.path.join(output_dir, \"Indices_Seasonal.xlsx\")\n",
    "df_seasonal.to_excel(seasonal_out, index=False)\n",
    "print(\"Seasonal indices saved =>\", seasonal_out)\n",
    "\n",
    "###############################################################################\n",
    "# 8. DONE\n",
    "###############################################################################\n",
    "print(\"\\nAll monthly and seasonal indices have been saved. (No extreme-event stratification.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51ebeb-473a-4d7f-9145-1786c47f73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DJF for CHIRPS\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import Point\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIG & PATHS\n",
    "###############################################################################\n",
    "indices_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\CHIRPS_GLB_Precipitation\\Masked to GLB\\ClimaticIndices-Seasonal\"\n",
    "seasonal_file = os.path.join(indices_dir, \"Indices_Seasonal.xlsx\")  # single file\n",
    "output_plots  = os.path.join(indices_dir, \"AnalysisPlots_DJF\")\n",
    "os.makedirs(output_plots, exist_ok=True)\n",
    "\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "\n",
    "# Indices in your seasonal file\n",
    "index_list = [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"r95p\",\"r99p\",\"wetdays\",\"drydays\"]\n",
    "\n",
    "# For summary stats, define how to find obs vs. chirps columns\n",
    "index_columns = {\n",
    "    \"rx1day\":  (\"rx1day_obs\",  \"rx1day_chirps\"),\n",
    "    \"rx5day\":  (\"rx5day_obs\",  \"rx5day_chirps\"),\n",
    "    \"cdd\":     (\"cdd_obs\",     \"cdd_chirps\"),\n",
    "    \"cwd\":     (\"cwd_obs\",     \"cwd_chirps\"),\n",
    "    \"r95p\":    ((\"r95amt_obs\",\"r95pct_obs\"), (\"r95amt_chirps\",\"r95pct_chirps\")),\n",
    "    \"r99p\":    ((\"r99amt_obs\",\"r99pct_obs\"), (\"r99amt_chirps\",\"r99pct_chirps\")),\n",
    "    \"wetdays\": (\"wetdays_obs\",\"wetdays_chirps\"),\n",
    "    \"drydays\": (\"drydays_obs\",\"drydays_chirps\"),\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 2. LOAD SEASONAL FILE & FILTER TO DJF\n",
    "###############################################################################\n",
    "df_season = pd.read_excel(seasonal_file)\n",
    "print(\"Loaded =>\", seasonal_file, \"| shape =\", df_season.shape)\n",
    "\n",
    "# Filter to DJF\n",
    "df_season = df_season[df_season[\"season\"]==\"DJF\"].copy()\n",
    "df_season = df_season.dropna(subset=[\"lat\",\"lon\"])  # ensure lat/lon exist\n",
    "print(\"After filtering to DJF => shape =\", df_season.shape)\n",
    "\n",
    "mdf = df_season.reset_index(drop=True)\n",
    "master_xlsx = os.path.join(output_plots, \"MasterTable_Seasonal_DJF.xlsx\")\n",
    "mdf.to_excel(master_xlsx, index=False)\n",
    "print(f\"\\n(A) Master table (DJF) saved => {master_xlsx}\")\n",
    "print(\"Columns:\", mdf.columns.tolist())\n",
    "\n",
    "###############################################################################\n",
    "# 3. SUMMARY TABLE (MBE, RMSE, STD, CC, d) for DJF\n",
    "###############################################################################\n",
    "def index_of_agreement(obs, model):\n",
    "    obs_mean = np.mean(obs)\n",
    "    num = np.sum((model - obs)**2)\n",
    "    den = np.sum((abs(model - obs_mean) + abs(obs - obs_mean))**2)\n",
    "    if den == 0:\n",
    "        return np.nan\n",
    "    return 1 - num/den\n",
    "\n",
    "def rmse(a, b):\n",
    "    return np.sqrt(np.mean((a-b)**2))\n",
    "\n",
    "def std_of_residuals(a, b):\n",
    "    return np.std(a-b, ddof=1)\n",
    "\n",
    "def mean_bias_error(a, b):\n",
    "    return np.mean(b-a)\n",
    "\n",
    "summary_rows = []\n",
    "for idx_name in index_list:\n",
    "    obs_cols = index_columns[idx_name][0]\n",
    "    chirps_cols = index_columns[idx_name][1]\n",
    "\n",
    "    if isinstance(obs_cols, tuple):\n",
    "        # multiple columns\n",
    "        for oc, ec in zip(obs_cols, chirps_cols):\n",
    "            valid = mdf[[oc, ec]].dropna()\n",
    "            if len(valid) < 2:\n",
    "                continue\n",
    "            obs_vals = valid[oc].values\n",
    "            chirps_vals = valid[ec].values\n",
    "            MB  = mean_bias_error(obs_vals, chirps_vals)\n",
    "            RM  = rmse(obs_vals, chirps_vals)\n",
    "            SR  = std_of_residuals(obs_vals, chirps_vals)\n",
    "            CC  = pearsonr(obs_vals, chirps_vals)[0] if len(obs_vals) > 1 else np.nan\n",
    "            dd  = index_of_agreement(obs_vals, chirps_vals)\n",
    "            idx_label = f\"{idx_name}_{oc.replace('_obs','')}\"\n",
    "            summary_rows.append({\n",
    "                \"Index\": idx_label,\n",
    "                \"Count\": len(valid),\n",
    "                \"MBE\": MB,\n",
    "                \"RMSE\": RM,\n",
    "                \"STDres\": SR,\n",
    "                \"CC\": CC,\n",
    "                \"d\": dd,\n",
    "            })\n",
    "    else:\n",
    "        oc = obs_cols\n",
    "        ec = chirps_cols\n",
    "        valid = mdf[[oc, ec]].dropna()\n",
    "        if len(valid) < 2:\n",
    "            continue\n",
    "        obs_vals = valid[oc].values\n",
    "        chirps_vals = valid[ec].values\n",
    "        MB = mean_bias_error(obs_vals, chirps_vals)\n",
    "        RM = rmse(obs_vals, chirps_vals)\n",
    "        SR = std_of_residuals(obs_vals, chirps_vals)\n",
    "        CC = pearsonr(obs_vals, chirps_vals)[0] if len(obs_vals) > 1 else np.nan\n",
    "        dd = index_of_agreement(obs_vals, chirps_vals)\n",
    "        summary_rows.append({\n",
    "            \"Index\": idx_name,\n",
    "            \"Count\": len(valid),\n",
    "            \"MBE\": MB,\n",
    "            \"RMSE\": RM,\n",
    "            \"STDres\": SR,\n",
    "            \"CC\": CC,\n",
    "            \"d\": dd,\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_cols = [\"Index\",\"Count\",\"MBE\",\"RMSE\",\"STDres\",\"CC\",\"d\"]\n",
    "summary_df = summary_df[summary_cols]\n",
    "summary_xlsx = os.path.join(output_plots, \"SummaryTable_Extremes_DJF.xlsx\")\n",
    "summary_df.to_excel(summary_xlsx, index=False)\n",
    "print(f\"(B) Summary Table (DJF) => {summary_xlsx}\\n{summary_df}\")\n",
    "\n",
    "###############################################################################\n",
    "# 4. MAPPING: Combine Observed, CHIRPS, Ratio in One Figure\n",
    "###############################################################################\n",
    "gdf_basin = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
    "gdf_lakes = gpd.read_file(lakes_shp).to_crs(epsg=4326)\n",
    "\n",
    "def add_basin_lakes(ax):\n",
    "    #ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    for geom in gdf_basin.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(),\n",
    "                          facecolor='none', edgecolor='black', linewidth=1)\n",
    "    for geom in gdf_lakes.geometry:\n",
    "        ax.add_geometries([geom], ccrs.PlateCarree(),\n",
    "                          facecolor='none', edgecolor='cyan', linewidth=1)\n",
    "\n",
    "def plot_map_triptych(df, obs_col, chirps_col, ratio_col, idx_name, out_png):\n",
    "    \"\"\"\n",
    "    Creates a single figure with 3 subplots (side-by-side):\n",
    "      1) Observed\n",
    "      2) CHIRPS v2.1\n",
    "      3) Ratio (CHIRPS/OBS)\n",
    "    Each subplot has a colorbar, a 90th-percentile hotspot circle, etc.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6),\n",
    "                             subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "\n",
    "    # We'll define a small helper to do each subplot\n",
    "    def scatter_map(ax, value_col, title):\n",
    "        ax.set_extent([-95.5, -72, 38.5, 52.5])\n",
    "        add_basin_lakes(ax)\n",
    "        sc = ax.scatter(df[\"lon\"], df[\"lat\"], c=df[value_col], cmap=\"viridis\",\n",
    "                        s=60, transform=ccrs.PlateCarree(), edgecolor=\"k\", zorder=10)\n",
    "        cb = plt.colorbar(sc, ax=ax, shrink=0.8)\n",
    "        cb.set_label(value_col)\n",
    "\n",
    "        # Hotspots => top 10%\n",
    "        vals = df[value_col].dropna().values\n",
    "        if len(vals) > 0:\n",
    "            thr = np.percentile(vals, 90)\n",
    "            is_hot = df[value_col]>=thr\n",
    "            ax.scatter(df.loc[is_hot,\"lon\"], df.loc[is_hot,\"lat\"],\n",
    "                       marker='o', facecolors='none', edgecolors='red', s=80,\n",
    "                       transform=ccrs.PlateCarree(), zorder=11,\n",
    "                       label=f\"Hotspot >= {thr:.2f}\")\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        gl = ax.gridlines(draw_labels=True, linestyle='--', color='gray')\n",
    "        gl.right_labels = False\n",
    "        gl.top_labels   = False\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    scatter_map(axes[0], obs_col,  f\"{idx_name} Observed (DJF)\")\n",
    "    scatter_map(axes[1], chirps_col,  f\"{idx_name} CHIRPS (DJF)\")\n",
    "    scatter_map(axes[2], ratio_col,f\"{idx_name} (CHIRPS/OBS) (DJF)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved =>\", out_png)\n",
    "\n",
    "def get_map_cols(idx_name):\n",
    "    if idx_name in [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"drydays\"]:\n",
    "        obs  = f\"{idx_name}_obs\"\n",
    "        chirps  = f\"{idx_name}_chirps\"\n",
    "        ratio= f\"{idx_name}_ratio\"\n",
    "        return obs, chirps, ratio\n",
    "    elif idx_name == \"wetdays\":\n",
    "        obs  = \"wetdays_obs\"\n",
    "        chirps  = \"wetdays_chirps\"\n",
    "        ratio= \"wetdays_ratio\"\n",
    "        return obs, chirps, ratio\n",
    "    elif idx_name == \"r95p\":\n",
    "        obs  = \"r95amt_obs\"\n",
    "        chirps  = \"r95amt_chirps\"\n",
    "        ratio= \"r95amt_ratio\"\n",
    "        return obs, chirps, ratio\n",
    "    elif idx_name == \"r99p\":\n",
    "        obs  = \"r99amt_obs\"\n",
    "        chirps  = \"r99amt_chirps\"\n",
    "        ratio= \"r99amt_ratio\"\n",
    "        return obs, chirps, ratio\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "for idx_name in index_list:\n",
    "    obs_col, chirps_col, ratio_col = get_map_cols(idx_name)\n",
    "    if obs_col is None:\n",
    "        continue\n",
    "\n",
    "    needed_cols = [obs_col, chirps_col, ratio_col, \"lat\", \"lon\"]\n",
    "    if not all(c in mdf.columns for c in needed_cols):\n",
    "        print(f\"Skipping map for {idx_name} - missing columns.\")\n",
    "        continue\n",
    "\n",
    "    subdf = mdf.dropna(subset=[\"lat\",\"lon\"]).copy()\n",
    "    out_png = os.path.join(output_plots, f\"DJF_{idx_name}_MAP_3panel.png\")\n",
    "    plot_map_triptych(subdf, obs_col, chirps_col, ratio_col, idx_name, out_png)\n",
    "\n",
    "###############################################################################\n",
    "# 5. DISTRIBUTION & BOX/CDF/Scatter in One Figure\n",
    "###############################################################################\n",
    "def plot_distribution_triptych(df, obs_col, chirps_col, label, out_png):\n",
    "    \"\"\"\n",
    "    Creates a single figure with 3 subplots side-by-side:\n",
    "      1) Boxplot\n",
    "      2) CDF\n",
    "      3) Scatter\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,6))\n",
    "\n",
    "    # A) Boxplot\n",
    "    ax_box = axes[0]\n",
    "    data = pd.DataFrame({\"Obs\": df[obs_col], \"CHIRPS\": df[chirps_col]}).melt(\n",
    "        var_name=\"Dataset\", value_name=label\n",
    "    )\n",
    "    sns.boxplot(data=data, x=\"Dataset\", y=label, ax=ax_box)\n",
    "    ax_box.set_title(f\"Boxplot: {label} (DJF)\")\n",
    "\n",
    "    # B) CDF\n",
    "    ax_cdf = axes[1]\n",
    "    obs_vals = df[obs_col].dropna()\n",
    "    chirps_vals = df[chirps_col].dropna()\n",
    "\n",
    "    def ecdf(x):\n",
    "        xs = np.sort(x)\n",
    "        ys = np.arange(1, len(xs)+1)/len(xs)\n",
    "        return xs, ys\n",
    "\n",
    "    if len(obs_vals)>=2 and len(chirps_vals)>=2:\n",
    "        xs_o, ys_o = ecdf(obs_vals)\n",
    "        xs_e, ys_e = ecdf(chirps_vals)\n",
    "        ax_cdf.plot(xs_o, ys_o, label=\"Obs\")\n",
    "        ax_cdf.plot(xs_e, ys_e, label=\"CHIRPS\")\n",
    "        ax_cdf.set_title(f\"CDF of {label} (DJF)\")\n",
    "        ax_cdf.set_xlabel(label)\n",
    "        ax_cdf.set_ylabel(\"Probability\")\n",
    "        ax_cdf.legend()\n",
    "    else:\n",
    "        ax_cdf.set_title(f\"CDF: not enough data ({label})\")\n",
    "\n",
    "    # C) Scatter\n",
    "    ax_scat = axes[2]\n",
    "    valid = df[[obs_col, chirps_col]].dropna()\n",
    "    if len(valid)>=2:\n",
    "        x = valid[obs_col]\n",
    "        y = valid[chirps_col]\n",
    "        cc, _ = pearsonr(x, y)\n",
    "        ax_scat.scatter(x, y, edgecolors='k', alpha=0.7)\n",
    "        mn, mx = np.nanmin([x.min(), y.min()]), np.nanmax([x.max(), y.max()])\n",
    "        ax_scat.plot([mn, mx],[mn, mx],'r--')\n",
    "        ax_scat.set_xlabel(f\"Obs {label} (DJF)\")\n",
    "        ax_scat.set_ylabel(f\"CHIRPS {label} (DJF)\")\n",
    "        ax_scat.set_title(f\"{label} (Corr={cc:.2f}, DJF)\")\n",
    "    else:\n",
    "        ax_scat.set_title(f\"Scatter: not enough data ({label})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved =>\", out_png)\n",
    "\n",
    "for idx_name in index_list:\n",
    "    # figure out obs, emd columns\n",
    "    if idx_name in [\"rx1day\",\"rx5day\",\"cdd\",\"cwd\",\"drydays\"]:\n",
    "        obs_col  = f\"{idx_name}_obs\"\n",
    "        chirps_col  = f\"{idx_name}_chirps\"\n",
    "    elif idx_name == \"wetdays\":\n",
    "        obs_col  = \"wetdays_obs\"\n",
    "        chirps_col  = \"wetdays_chirps\"\n",
    "    elif idx_name == \"r95p\":\n",
    "        obs_col  = \"r95amt_obs\"\n",
    "        chirps_col  = \"r95amt_chirps\"\n",
    "    elif idx_name == \"r99p\":\n",
    "        obs_col  = \"r99amt_obs\"\n",
    "        chirps_col  = \"r99amt_chirps\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    if obs_col not in mdf.columns or chirps_col not in mdf.columns:\n",
    "        print(f\"Skipping distribution for {idx_name} - missing columns.\")\n",
    "        continue\n",
    "\n",
    "    subdf = mdf[[obs_col, chirps_col]].dropna()\n",
    "    if len(subdf)<2:\n",
    "        print(f\"Skipping distribution for {idx_name} - not enough data.\")\n",
    "        continue\n",
    "\n",
    "    out_3panel = os.path.join(output_plots, f\"DJF_{idx_name}_Distribution_3panel.png\")\n",
    "    plot_distribution_triptych(subdf, obs_col, chirps_col, idx_name, out_3panel)\n",
    "\n",
    "###############################################################################\n",
    "# 6. DONE\n",
    "###############################################################################\n",
    "print(\"\\nAll DJF steps completed! See outputs in:\", output_plots)\n",
    "\n",
    "# For the other seasons, just change any DJF to JJA, MAM, or SON"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
