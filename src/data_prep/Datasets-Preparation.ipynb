{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc24cec-8271-488a-90d2-253bfe55a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                            #######################\n",
    "                                                            ######## PRISM ########\n",
    "                                                            #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f25225-1195-48bb-9f48-9130a7cc3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mask PRISM dataset to a box over the Great Lakes Basin and reorder dimensions\n",
    "Varibale can be prcp, tmin or tmax\n",
    "--------------------------------------------------------\n",
    "input : D:\\PRISM_t\\tmax\\nc file 1991-2013\\PRISM_tmax_4km_1991_2013.nc\n",
    "output: D:\\PRISM_t\\tmax\\nc file 1991-2013\\PRISM_tmax_4km_GLB_1991_2013.nc\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "in_nc  = Path(r\"D:\\PRISM_t\\tmin\\nc file 1991-2013\\PRISM_tmin_4km_1991_2013.nc\") # Choose your own input path\n",
    "out_nc = Path(r\"D:\\PRISM_t\\tmin\\nc file 1991-2013\\PRISM_tmin_4km_GLB_1991_2013_mod.nc\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1) open, rename coordinates\n",
    "# ----------------------------------------------------------------------\n",
    "ds = xr.open_dataset(in_nc)\n",
    "\n",
    "ds = ds.rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2) build boolean mask for requested window\n",
    "#    (use .where(..., drop=True) so non-matching rows/cols disappear)\n",
    "# ----------------------------------------------------------------------\n",
    "mask = (\n",
    "    (ds.lon >= -95.5) & (ds.lon <= -72.0) &\n",
    "    (ds.lat >=  38.5) & (ds.lat <=  50)\n",
    ")\n",
    "\n",
    "# Apply mask to every variable that has lat & lon dims\n",
    "ds = ds.where(mask, drop=True)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3) reorder dimensions to (lat, lon, time)\n",
    "# ----------------------------------------------------------------------\n",
    "# If lat is descending, leave it that way â€“ CF doesnâ€™t care about order.\n",
    "wanted_order = (\"lat\", \"lon\", \"time\")\n",
    "ds = ds.transpose(*wanted_order)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4) write NetCDF  (keep compression & nodata from source file)\n",
    "# ----------------------------------------------------------------------\n",
    "enc = {\n",
    "    \"tmin\": dict(zlib=True, complevel=4, dtype=\"float32\"),\n",
    "    \"time\": dict(dtype=\"int32\", zlib=True, complevel=4, _FillValue=None),\n",
    "}\n",
    "\n",
    "ds.to_netcdf(\n",
    "    out_nc,\n",
    "    mode=\"w\",\n",
    "    format=\"NETCDF4\",\n",
    "    engine=\"netcdf4\",\n",
    "    encoding=enc,\n",
    ")\n",
    "\n",
    "print(\"âœ… masked cube saved â†’\", out_nc)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f01f7-68e1-4a0e-97c9-f9b375b8e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ merge_PRISM_tmin_tmax â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Merge the Great-Lakes-masked PRISM Tmin + Tmax cubes into ONE NetCDF\n",
    "#   â€¢ input 1 : PRISM_tmin_4km_GLB_1991_2013_mod.nc   (has *only* Tmin)\n",
    "#   â€¢ input 2 : PRISM_tmax_4km_GLB_1991_2013_mod.nc   (has *only* Tmax)\n",
    "#   â€¢ output  : PRISM_tmin_tmax_4km_GLB_1991_2013.nc  (both variables)\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "# â”€â”€â”€ paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "tmin_nc = Path(r\"D:\\PRISM_t\\tmin\\nc file 1991-2013\\PRISM_tmin_4km_GLB_1991_2013_mod.nc\")  # Choose your own input path\n",
    "tmax_nc = Path(r\"D:\\PRISM_t\\tmax\\nc file 1991-2013\\PRISM_tmax_4km_GLB_1991_2013_mod.nc\")\n",
    "\n",
    "out_nc  = Path(r\"D:\\PRISM_t\\tmin-max\\nc file 1991-2013\\PRISM_tmin_tmax_4km_GLB_1991_2013.nc\")\n",
    "out_nc.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€ open & rename variables to canonical names â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ds_min = xr.open_dataset(tmin_nc)            # expect single var inside\n",
    "ds_max = xr.open_dataset(tmax_nc)\n",
    "\n",
    "# whatever they are called, rename to â€œtminâ€ / â€œtmaxâ€\n",
    "min_var = next(iter(ds_min.data_vars))\n",
    "max_var = next(iter(ds_max.data_vars))\n",
    "ds_min = ds_min.rename({min_var: \"tmin\"})\n",
    "ds_max = ds_max.rename({max_var: \"tmax\"})\n",
    "\n",
    "# â”€â”€â”€ make sure grids & calendars coincide  (should, but be safe) â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ds_min, ds_max = xr.align(ds_min, ds_max, join=\"exact\")\n",
    "\n",
    "# â”€â”€â”€ merge & copy some global attributes  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ds_merged = xr.merge([ds_min[\"tmin\"], ds_max[\"tmax\"]])\n",
    "ds_merged.attrs = {**ds_min.attrs, **ds_max.attrs}\n",
    "ds_merged.attrs[\"history\"] = (\n",
    "    ds_merged.attrs.get(\"history\", \"\") +\n",
    "    \" | merged tmin & tmax (PRISM Great-Lakes crop)\"\n",
    ")\n",
    "\n",
    "# â”€â”€â”€ compression / encoding  -------------------------------------------\n",
    "comp = dict(zlib=True, complevel=4)\n",
    "enc  = {\n",
    "    \"tmin\": comp,\n",
    "    \"tmax\": comp,\n",
    "    \"time\": dict(dtype=\"int32\", zlib=True, complevel=4, _FillValue=None),\n",
    "}\n",
    "\n",
    "# â”€â”€â”€ write out  ---------------------------------------------------------\n",
    "ds_merged.to_netcdf(out_nc, format=\"NETCDF4\", engine=\"netcdf4\", encoding=enc)\n",
    "print(\"âœ…  merged cube written â†’\", out_nc)\n",
    "print(ds_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26611179-61de-4c41-bd84-4bdf8e25be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling MERIT DEM (90 m) to PRISM 4-km grid (0.041666667Â° Ã— 0.041666667Â°) - \"it can be resampled to any other spatial resolution for other datasets\"\n",
    "\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "# Paths to input and output DEM files\n",
    "dem_file   = r\"D:\\PhD\\GLB\\DEM\\Both\\Merged_all_ Clipped.tif\"\n",
    "output_dem = r\"D:\\PhD\\GLB\\DEM\\Both\\GreatLakes_MERIT_PRISM_0.0416667deg.tif\"\n",
    "\n",
    "# Open original DEM\n",
    "with rasterio.open(dem_file) as src:\n",
    "    src_crs       = src.crs\n",
    "    left, bottom, right, top = src.bounds\n",
    "    src_transform = src.transform\n",
    "\n",
    "    # --- PRISM spacing: 0.041666667Â° lon, 0.041666667Â° lat ---\n",
    "    xres = yres = 0.041666667   # 1/24 degree  â‰ˆ 4 km\n",
    "\n",
    "    # Compute transformation for resampling\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src_crs, src_crs,\n",
    "        src.width, src.height,\n",
    "        left, bottom, right, top,\n",
    "        resolution=(xres, yres)\n",
    "    )\n",
    "\n",
    "    # Update metadata for the new DEM\n",
    "    new_meta = src.meta.copy()\n",
    "    new_meta.update({\n",
    "        \"driver\":    \"GTiff\",\n",
    "        \"height\":    height,\n",
    "        \"width\":     width,\n",
    "        \"transform\": transform,\n",
    "        \"nodata\":    -9999\n",
    "    })\n",
    "\n",
    "    # Create an empty array for the new DEM\n",
    "    new_dem_data = np.empty((height, width), dtype=np.float32)\n",
    "\n",
    "    # Resample and write to new file\n",
    "    with rasterio.open(output_dem, \"w\", **new_meta) as dst:\n",
    "        reproject(\n",
    "            source        = rasterio.band(src, 1),\n",
    "            destination   = new_dem_data,\n",
    "            src_transform = src_transform,\n",
    "            dst_transform = transform,\n",
    "            src_crs       = src_crs,\n",
    "            dst_crs       = src_crs,\n",
    "            resampling    = Resampling.bilinear\n",
    "        )\n",
    "        dst.write(new_dem_data, 1)\n",
    "\n",
    "print(f\"âœ… Resampled DEM (0.0416667Â°Ã—0.0416667Â°) saved as: {output_dem}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e8d2e9-c84e-4adb-afa7-325f2d1a6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Elevation from the resampled MERIT DEM to the merged nc files of prcp 1991-2013 (Here for PRISM)\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.interpolate import griddata\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "resampled_dem       = r\"D:\\PhD\\GLB\\DEM\\Both\\GreatLakes_MERIT_PRISM_0.0416667deg.tif\"\n",
    "PRISM_file            = r\"D:\\PRISM_ppt_daily\\nc file 1991-2013\\PRISM_ppt_4km_GLB_1991_2013.nc\"\n",
    "updated_PRISM_file    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\PRISM_GLB_Precipitation\\PRISM_prcp_GLB_1991-2013_with_Elevation.nc\"\n",
    "\n",
    "print(\"ðŸ”¹ Opening PRISM datasetâ€¦\")\n",
    "PRISM_ds   = xr.open_dataset(PRISM_file)\n",
    "PRISM_lats = PRISM_ds[\"lat\"].values\n",
    "PRISM_lons = PRISM_ds[\"lon\"].values\n",
    "\n",
    "print(\"ðŸ”¹ Reading DEMâ€¦\")\n",
    "with rasterio.open(resampled_dem) as src:\n",
    "    dem_data  = src.read(1)\n",
    "    transform = src.transform\n",
    "    height, width = dem_data.shape\n",
    "    dem_lons = np.linspace(src.bounds.left,  src.bounds.right, width)\n",
    "    dem_lats = np.linspace(src.bounds.top,   src.bounds.bottom, height)\n",
    "    dem_lon2d, dem_lat2d = np.meshgrid(dem_lons, dem_lats)\n",
    "    dem_points = np.column_stack((dem_lon2d.ravel(), dem_lat2d.ravel()))\n",
    "    dem_values = dem_data.ravel()\n",
    "\n",
    "# Prepare output elevation grid\n",
    "grid_elev = np.full((len(PRISM_lats), len(PRISM_lons)), np.nan, dtype=np.float32)\n",
    "\n",
    "print(\"ðŸ”¹ Interpolating elevation onto PRISM gridâ€¦\")\n",
    "for i, lat in enumerate(tqdm(PRISM_lats, desc=\"  Rows\")):\n",
    "    # build query points for this row\n",
    "    query_pts = np.column_stack((PRISM_lons, np.full_like(PRISM_lons, lat)))\n",
    "    # nearestâ€neighbor for entire row at once\n",
    "    row_vals = griddata(dem_points, dem_values, query_pts, method=\"nearest\")\n",
    "    grid_elev[i, :] = row_vals\n",
    "\n",
    "print(\"ðŸ”¹ Assigning elevation to datasetâ€¦\")\n",
    "PRISM_ds[\"elevation\"] = ((\"lat\", \"lon\"), grid_elev)\n",
    "PRISM_ds.elevation.attrs.update(\n",
    "    long_name=\"Surface elevation\",\n",
    "    units=\"m\",\n",
    ")\n",
    "\n",
    "print(\"ðŸ”¹ Writing updated NetCDFâ€¦\")\n",
    "PRISM_ds.to_netcdf(updated_PRISM_file)\n",
    "print(f\"âœ… Done! Saved with elevation: {updated_PRISM_file}\")\n",
    "\n",
    "# Up to here, we have a netcdf file for PRISM (prcp or tmin/tmax) that has this variable, lat, lon, and elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de3726-796a-4942-b90d-62ada4413fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                            #######################\n",
    "                                                            ######## EMDNA ########\n",
    "                                                            #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9dc48-a3c2-4f8f-a606-f9770068d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting prcp from the masked original EMDNA files\n",
    "\n",
    "\"\"\"\n",
    "Extract precipitation (prcp) from **masked** EMDNA ensemble files.\n",
    "\n",
    "For every ensemble ID in ENSEMBLES the script:\n",
    "  â€¢ scans  ROOT_DIR/<ensemble>/Masked/   for *.nc4\n",
    "  â€¢ keeps only the variable 'trange'\n",
    "  â€¢ writes each file to  ROOT_DIR/<ensemble>/Masked/trange/<original>_prcp.nc4\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ input settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT_DIR  = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\"\n",
    "ENSEMBLES = [1, 11, 21, 31, 41, 51, 61, 66, 71, 76, 81, 86, 91, 96, 100] # Any choice of ensemble members can be specified\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "for ens in ENSEMBLES:\n",
    "    input_folder  = os.path.join(ROOT_DIR, str(ens), \"Masked\")\n",
    "    output_folder = os.path.join(input_folder, \"prcp\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    file_list = glob.glob(os.path.join(input_folder, \"*.nc4\"))\n",
    "    print(f\"\\nEnsemble {ens}: found {len(file_list)} files\")\n",
    "\n",
    "    for file_path in file_list:\n",
    "        base_name   = os.path.basename(file_path)\n",
    "        output_file = os.path.join(\n",
    "            output_folder,\n",
    "            base_name.replace(\".nc4\", \"_prcp.nc4\")\n",
    "        )\n",
    "\n",
    "        print(f\"  â†’ {base_name}\")\n",
    "\n",
    "        # open, subset, save\n",
    "        with xr.open_dataset(file_path) as ds:\n",
    "            ds_prcp = ds[['prcp']]\n",
    "            ds_prcp.to_netcdf(\n",
    "                output_file,\n",
    "                encoding={var: {\"zlib\": True, \"complevel\": 4}\n",
    "                          for var in ds_prcp.data_vars}\n",
    "            )\n",
    "\n",
    "print(\"\\nâœ… All ensembles processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cb305-4cb2-4166-a0d5-7a07210365c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting trange from the masked original EMDNA files\n",
    "\n",
    "\"\"\"\n",
    "Extracting (trange) from **masked** EMDNA ensemble files.\n",
    "\n",
    "For every ensemble ID in ENSEMBLES the script:\n",
    "  â€¢ scans  ROOT_DIR/<ensemble>/Masked/   for *.nc4\n",
    "  â€¢ keeps only the variable 'trange'\n",
    "  â€¢ writes each file to  ROOT_DIR/<ensemble>/Masked/trange/<original>_trange.nc4\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ input settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT_DIR  = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\"\n",
    "ENSEMBLES = [1, 11, 21, 31, 41, 51, 61, 66, 71, 76, 81, 86, 91, 96, 100] # Any choice of ensemble members can be specified\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "for ens in ENSEMBLES:\n",
    "    input_folder  = os.path.join(ROOT_DIR, str(ens), \"Masked\")\n",
    "    output_folder = os.path.join(input_folder, \"trange\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    file_list = glob.glob(os.path.join(input_folder, \"*.nc4\"))\n",
    "    print(f\"\\nEnsemble {ens}: found {len(file_list)} files\")\n",
    "\n",
    "    for file_path in file_list:\n",
    "        base_name   = os.path.basename(file_path)\n",
    "        output_file = os.path.join(\n",
    "            output_folder,\n",
    "            base_name.replace(\".nc4\", \"_trange.nc4\")\n",
    "        )\n",
    "\n",
    "        print(f\"  â†’ {base_name}\")\n",
    "\n",
    "        # open, subset, save\n",
    "        with xr.open_dataset(file_path) as ds:\n",
    "            ds_trange = ds[['trange']]\n",
    "            ds_trange.to_netcdf(\n",
    "                output_file,\n",
    "                encoding={var: {\"zlib\": True, \"complevel\": 4}\n",
    "                          for var in ds_trange.data_vars}\n",
    "            )\n",
    "\n",
    "print(\"\\nâœ… All ensembles processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18770ff0-7518-4054-9cb5-38eabc3938b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting tmean from the masked original EMDNA files\n",
    "\n",
    "\"\"\"\n",
    "Extract daily-mean temperature (tmean) from **masked** EMDNA ensemble files.\n",
    "\n",
    "For every ensemble ID in ENSEMBLES the script:\n",
    "  â€¢ scans  ROOT_DIR/<ensemble>/Masked/   for *.nc4\n",
    "  â€¢ keeps only the variable 'tmean'\n",
    "  â€¢ writes each file to  ROOT_DIR/<ensemble>/Masked/tmean/<original>_tmean.nc4\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ USER SETTINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT_DIR  = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\"\n",
    "ENSEMBLES = [1, 11, 21, 31, 41, 51, 61, 66, 71, 76, 81, 86, 91, 96, 100]\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "for ens in ENSEMBLES:\n",
    "    input_folder  = os.path.join(ROOT_DIR, str(ens), \"Masked\")\n",
    "    output_folder = os.path.join(input_folder, \"tmean\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    file_list = glob.glob(os.path.join(input_folder, \"*.nc4\"))\n",
    "    print(f\"\\nEnsemble {ens}: found {len(file_list)} files\")\n",
    "\n",
    "    for file_path in file_list:\n",
    "        base_name   = os.path.basename(file_path)\n",
    "        output_file = os.path.join(\n",
    "            output_folder,\n",
    "            base_name.replace(\".nc4\", \"_tmean.nc4\")\n",
    "        )\n",
    "\n",
    "        print(f\"  â†’ {base_name}\")\n",
    "\n",
    "        # open, subset, save\n",
    "        with xr.open_dataset(file_path) as ds:\n",
    "            ds_tmean = ds[['tmean']]\n",
    "            ds_tmean.to_netcdf(\n",
    "                output_file,\n",
    "                encoding={var: {\"zlib\": True, \"complevel\": 4}\n",
    "                          for var in ds_tmean.data_vars}\n",
    "            )\n",
    "\n",
    "print(\"\\nâœ… All ensembles processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dad9c3-7dee-4a67-930f-b0bba4384579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ build_emdna_tmin_tmax_only â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Making min & max temperature from trange and tmean\n",
    "\"\"\"\n",
    "Create daily Tmin / Tmax files from EMDNA Tmean + Trange pairs.\n",
    "Each output NetCDF holds *only* the variables `tmin` and `tmax`.\n",
    "\n",
    "Folder layout expected\n",
    "ROOT/<ens>/Masked/\n",
    "    â”œâ”€ tmean\\   â€¦ *_tmean.nc4\n",
    "    â””â”€ trange\\  â€¦ *_trange.nc4\n",
    "Output goes to\n",
    "ROOT/<ens>/Masked/tmin_tmax\\  â€¦ *_tmin_tmax.nc4\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "\n",
    "ROOT_DIR  = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\"\n",
    "ENSEMBLES = [1, 11, 21, 31, 41, 51, 61, 66, 71, 76, 81, 86, 91, 96, 100]\n",
    "\n",
    "for ens in map(str, ENSEMBLES):\n",
    "    tmean_dir  = os.path.join(ROOT_DIR, ens, \"Masked\", \"tmean\")\n",
    "    trange_dir = os.path.join(ROOT_DIR, ens, \"Masked\", \"trange\")\n",
    "    out_dir    = os.path.join(ROOT_DIR, ens, \"Masked\", \"tmin_tmax\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for tmean_fp in glob.glob(os.path.join(tmean_dir, \"*_tmean.nc4\")):\n",
    "        stem       = os.path.basename(tmean_fp).replace(\"_tmean.nc4\", \"\")\n",
    "        trange_fp  = os.path.join(trange_dir, f\"{stem}_trange.nc4\")\n",
    "        if not os.path.exists(trange_fp):\n",
    "            print(f\"[{ens}] missing trange â†’ {stem} skipped\")\n",
    "            continue\n",
    "\n",
    "        out_fp = os.path.join(out_dir, f\"{stem}_tmin_tmax.nc4\")\n",
    "\n",
    "        with xr.open_dataset(tmean_fp) as ds_mean, xr.open_dataset(trange_fp) as ds_rng:\n",
    "            tmin = ds_mean[\"tmean\"] - ds_rng[\"trange\"] / 2.0\n",
    "            tmax = ds_mean[\"tmean\"] + ds_rng[\"trange\"] / 2.0\n",
    "\n",
    "            xr.Dataset(\n",
    "                {\"tmin\": tmin, \"tmax\": tmax},\n",
    "                attrs=ds_mean.attrs          # keep global metadata\n",
    "            ).to_netcdf(\n",
    "                out_fp,\n",
    "                encoding={v: {\"zlib\": True, \"complevel\": 4}\n",
    "                          for v in (\"tmin\", \"tmax\")}\n",
    "            )\n",
    "    print(f\"[{ens}] âœ“ Tmin/Tmax complete\")\n",
    "\n",
    "print(\"\\nAll ensembles processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c954a-60b0-4322-b424-febb10b203a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the extracted (tmin-tmax or prcp) to a single nc file from 1991-2013 for each ensemble\n",
    "\n",
    "\"\"\"\n",
    "Merge yearly Tmin/Tmax files for every ensemble, add CF metadata,\n",
    "and overwrite the merged NetCDF (1991-2013).\n",
    "\n",
    "Directory layout assumed\n",
    "ROOT/<ens>/Masked/tmin_tmax/\n",
    "    â€¦ EMDNA_<ens>_1991_tmin_tmax.nc4\n",
    "    â€¦ EMDNA_<ens>_1992_tmin_tmax.nc4\n",
    "    â€¦\n",
    "The script produces (or overwrites) in the same folder:\n",
    "    EMDNA_<ens>_merged_tmin_tmax_1991_2013.nc\n",
    "\"\"\"\n",
    "\n",
    "import os, re, glob\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ USER SETTINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT_DIR  = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\"\n",
    "ENSEMBLES = [1, 11, 21, 31, 41, 51, 61, 66, 71, 76, 81, 86, 91, 96, 100]\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "YEAR_RE = re.compile(r\"(19|20)\\d{2}\")\n",
    "\n",
    "def add_cf_attrs(ds: xr.Dataset) -> xr.Dataset:\n",
    "    ds[\"lon\"]  = ds[\"lon\"].assign_attrs(long_name=\"longitude\",\n",
    "                                        units=\"degrees_east\")\n",
    "    ds[\"lat\"]  = ds[\"lat\"].assign_attrs(long_name=\"latitude\",\n",
    "                                        units=\"degrees_north\")\n",
    "    ds[\"tmin\"] = ds[\"tmin\"].assign_attrs(long_name=\"daily minimum 2 m temperature\",\n",
    "                                         units=\"degC\")\n",
    "    ds[\"tmax\"] = ds[\"tmax\"].assign_attrs(long_name=\"daily maximum 2 m temperature\",\n",
    "                                         units=\"degC\")\n",
    "    return ds\n",
    "\n",
    "for ens in ENSEMBLES:\n",
    "    ens_dir   = os.path.join(ROOT_DIR, str(ens), \"Masked\", \"tmin_tmax\")\n",
    "    yr_files  = sorted(glob.glob(os.path.join(ens_dir, \"*_tmin_tmax.nc4\")))\n",
    "    if not yr_files:\n",
    "        print(f\"[{ens}] no yearly files found â†’ skipped\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[{ens}] merging {len(yr_files)} yearly files\")\n",
    "    datasets = []\n",
    "    for fp in yr_files:\n",
    "        year_match = YEAR_RE.search(os.path.basename(fp))\n",
    "        if not year_match:\n",
    "            print(f\"    ! skipped {os.path.basename(fp)} (no year in name)\")\n",
    "            continue\n",
    "        year  = int(year_match.group(0))\n",
    "        dates = pd.date_range(f\"{year}-01-01\", f\"{year}-12-31\", freq=\"D\")\n",
    "\n",
    "        ds = xr.open_dataset(fp)\n",
    "        if ds.dims[\"time\"] != len(dates):\n",
    "            raise ValueError(f\"{os.path.basename(fp)}: expected {len(dates)} days,\"\n",
    "                             f\" found {ds.dims['time']}\")\n",
    "        datasets.append(ds.assign_coords(time=dates))\n",
    "\n",
    "    merged = xr.concat(datasets, dim=\"time\").sortby(\"time\")\n",
    "    merged = merged.transpose(\"time\", \"lat\", \"lon\")\n",
    "    merged = add_cf_attrs(merged)\n",
    "\n",
    "    out_fp = os.path.join(ens_dir,\n",
    "              f\"EMDNA_{ens}_merged_tmin_tmax_1991_2013.nc\")\n",
    "    tmp_fp = out_fp + \".tmp\"        # safe write, then atomic replace\n",
    "    merged.to_netcdf(tmp_fp)\n",
    "    os.replace(tmp_fp, out_fp)\n",
    "    print(f\"    âœ“ wrote {out_fp}\")\n",
    "\n",
    "print(\"\\nAll ensembles processed and CF-metadata added.\")\n",
    "\n",
    "# The next step is making the resampled MERIT DEM for EMDNA spatial resolution (0.1) and adding elevation to the prcp and tmin/tmax netcdf files, the same as what has been done for PRISM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29259f-2df3-494c-9079-88625ef11a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                            ############################\n",
    "                                                            ######## RDRS v 2.1 ########\n",
    "                                                            ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa28531-3894-4d4e-a407-057c69b76bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking RDRS dataset to GLB and converting its coordinates from rotated to regular \n",
    "# Here we have precipitation, but the same thing will be done on both tmin and tmax variables\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "from clisops.core import subset\n",
    "import xesmf as xe\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For nicer lat/lon tick formatting\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "###################################################\n",
    "#DEFINE INPUT AND OUTPUT\n",
    "input_file  = r\"D:\\RDRS-V2.1\\pr_day_GovCan_rdrs-v21_20120101-20151231.nc\"\n",
    "out_subset  = r\"D:\\RDRS-V2.1\\subset\\pr_subset_rotated_20120101-20151231.nc\"\n",
    "out_regrid  = r\"D:\\RDRS-V2.1\\subset\\pr_subset_regular_20120101-20151231.nc\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(os.path.dirname(out_subset), exist_ok=True)\n",
    "\n",
    "###################################################\n",
    "#OPEN THE DATASET WITH CHUNKS\n",
    "print(\"Opening dataset with rotated coords...\")\n",
    "ds = xr.open_dataset(\n",
    "    input_file,\n",
    "    chunks=dict(time=1461, rlon=50, rlat=50)\n",
    ")\n",
    "print(ds)\n",
    "\n",
    "###################################################\n",
    "#SUBSET TO BOUNDING BOX (in rotated coords, using lat/lon)\n",
    "min_lon, max_lon = -95.5, -72\n",
    "min_lat, max_lat = 38.5, 52.5\n",
    "\n",
    "print(\"\\nLoading lat/lon arrays to memory...\")\n",
    "ds[\"lat\"] = ds[\"lat\"].load()\n",
    "ds[\"lon\"] = ds[\"lon\"].load()\n",
    "\n",
    "print(f\"Subsetting to bounding box lon=[{min_lon},{max_lon}], lat=[{min_lat},{max_lat}]...\")\n",
    "ds_subset = subset.subset_bbox(\n",
    "    ds,\n",
    "    lon_bnds=[min_lon, max_lon],\n",
    "    lat_bnds=[min_lat, max_lat]\n",
    ")\n",
    "\n",
    "print(\"\\nPerforming the subset with dask progress bar...\")\n",
    "with ProgressBar():\n",
    "    ds_subset = ds_subset.load()\n",
    "\n",
    "# Save the subset\n",
    "ds_subset.to_netcdf(out_subset)\n",
    "print(f\"Subset (still rotated) saved to: {out_subset}\")\n",
    "\n",
    "print(ds_subset.rotated_pole)\n",
    "print(ds_subset.rotated_pole.attrs)\n",
    "\n",
    "###################################################\n",
    "#REGRID THE SUBSET TO A REGULAR LAT-LON GRID\n",
    "print(\"\\nDefining a regular lat-lon grid for regridding...\")\n",
    "\n",
    "target_lats = np.arange(min_lat, max_lat + 0.01, 0.1)\n",
    "target_lons = np.arange(min_lon, max_lon + 0.01, 0.1)\n",
    "\n",
    "ds_out = xr.Dataset(\n",
    "    {\n",
    "        \"lat\": ([\"lat\"], target_lats),\n",
    "        \"lon\": ([\"lon\"], target_lons),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Building the xESMF regridder...\")\n",
    "regridder = xe.Regridder(\n",
    "    ds_subset,\n",
    "    ds_out,\n",
    "    method=\"bilinear\",\n",
    "    reuse_weights=False\n",
    ")\n",
    "\n",
    "print(\"Applying the regridder to 'pr' variable with progress bar...\")\n",
    "with ProgressBar():\n",
    "    pr_reg = regridder(ds_subset[\"pr\"].load())\n",
    "\n",
    "ds_reg = pr_reg.to_dataset(name=\"pr\")\n",
    "\n",
    "# Copy original variable attributes so we can use them in plotting\n",
    "for attr_name in [\"long_name\", \"units\"]:\n",
    "    if attr_name in ds_subset[\"pr\"].attrs:\n",
    "        ds_reg[\"pr\"].attrs[attr_name] = ds_subset[\"pr\"].attrs[attr_name]\n",
    "\n",
    "ds_reg.to_netcdf(out_regrid)\n",
    "print(f\"Regridded data saved to: {out_regrid}\")\n",
    "\n",
    "###################################################\n",
    "#QUICK PLOT: ORIGINAL (ROTATED) VS. REGRIDDING RESULT\n",
    "print(\"\\nPlotting the original subset (rotated) and the final regridded subset...\")\n",
    "\n",
    "time_idx = 0\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Safely extract rotated pole attributes, fallback if missing\n",
    "pole_lon = ds_subset.rotated_pole.attrs.get(\"grid_north_pole_longitude\", 87.5970)\n",
    "pole_lat = ds_subset.rotated_pole.attrs.get(\"grid_north_pole_latitude\", 31.7583)\n",
    "rotpole = ccrs.RotatedPole(\n",
    "    pole_longitude=float(pole_lon),\n",
    "    pole_latitude=float(pole_lat)\n",
    ")\n",
    "\n",
    "# PROJECTION for the map: PlateCarree\n",
    "proj_pc = ccrs.PlateCarree()\n",
    "\n",
    "#######################\n",
    "# Original subset\n",
    "#######################\n",
    "ax1 = fig.add_subplot(2, 1, 1, projection=proj_pc)\n",
    "ax1.set_title(\"Original Subset on Rotated Grid (plotted via RotatedPole transform)\")\n",
    "ax1.set_extent([min_lon, max_lon, min_lat, max_lat], crs=proj_pc)\n",
    "\n",
    "mesh1 = ax1.pcolormesh(\n",
    "    ds_subset.rlon,\n",
    "    ds_subset.rlat,\n",
    "    ds_subset.pr.isel(time=time_idx),\n",
    "    transform=rotpole  # Tells Cartopy how to interpret these data coords\n",
    ")\n",
    "ax1.coastlines()\n",
    "\n",
    "# Turn on lat/lon grid labels\n",
    "gl1 = ax1.gridlines(draw_labels=True, alpha=0.5, linestyle=\"--\")\n",
    "gl1.xlabels_top = False\n",
    "gl1.ylabels_right = False\n",
    "gl1.xformatter = LONGITUDE_FORMATTER\n",
    "gl1.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "cb1 = plt.colorbar(mesh1, orientation=\"horizontal\", pad=0.08, ax=ax1)\n",
    "pr_long_name = ds_subset[\"pr\"].attrs.get(\"long_name\", \"Precipitation\")\n",
    "pr_units     = ds_subset[\"pr\"].attrs.get(\"units\", \"kg m-2 s-1\")\n",
    "cb1.set_label(f\"{pr_long_name} ({pr_units})\")\n",
    "\n",
    "#######################\n",
    "# Regridded subset\n",
    "#######################\n",
    "ax2 = fig.add_subplot(2, 1, 2, projection=proj_pc)\n",
    "ax2.set_title(\"Regridded Subset (Regular Lat-Lon)\")\n",
    "ax2.set_extent([min_lon, max_lon, min_lat, max_lat], crs=proj_pc)\n",
    "\n",
    "mesh2 = ax2.pcolormesh(\n",
    "    ds_reg.lon,\n",
    "    ds_reg.lat,\n",
    "    ds_reg.pr.isel(time=time_idx),\n",
    "    transform=proj_pc\n",
    ")\n",
    "ax2.coastlines()\n",
    "\n",
    "# Turn on lat/lon grid labels\n",
    "gl2 = ax2.gridlines(draw_labels=True, alpha=0.5, linestyle=\"--\")\n",
    "gl2.xlabels_top = False\n",
    "gl2.ylabels_right = False\n",
    "gl2.xformatter = LONGITUDE_FORMATTER\n",
    "gl2.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "cb2 = plt.colorbar(mesh2, orientation=\"horizontal\", pad=0.08, ax=ax2)\n",
    "pr_long_name = ds_reg[\"pr\"].attrs.get(\"long_name\", \"Precipitation\")\n",
    "pr_units     = ds_reg[\"pr\"].attrs.get(\"units\", \"kg m-2 s-1\")\n",
    "cb2.set_label(f\"{pr_long_name} ({pr_units})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3eaf1-a30f-495a-9aca-64f028e52c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the regular grid nc files to one single netcdf files of RDRS\n",
    "# Here we have precipitation, but the same thing will be done on both tmin and tmax variables\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "###############################################\n",
    "# 1) Define paths\n",
    "###############################################\n",
    "# Input folder containing your 7 netCDF files\n",
    "input_folder = r\"D:\\RDRS-V2.1\\subset\\Regular coordinates\"\n",
    "\n",
    "# Output file where weâ€™ll save the merged dataset\n",
    "output_file = r\"D:\\RDRS-V2.1\\subset\\Regular coordinates\\merged_prcp_1988-2015_mm_day.nc\"\n",
    "\n",
    "###############################################\n",
    "# 2) Open all netCDF files at once\n",
    "###############################################\n",
    "# We assume all relevant files match a pattern like \"*.nc\"\n",
    "file_pattern = os.path.join(input_folder, \"*.nc\")\n",
    "nc_files = sorted(glob.glob(file_pattern))\n",
    "print(\"Found files:\")\n",
    "for f in nc_files:\n",
    "    print(f)\n",
    "\n",
    "# Open and combine them by time (assuming each file spans a unique time chunk)\n",
    "ds = xr.open_mfdataset(nc_files, combine=\"by_coords\")\n",
    "\n",
    "###############################################\n",
    "# 3) Rename 'pr' to 'prcp'\n",
    "###############################################\n",
    "# Check if 'pr' exists\n",
    "if \"pr\" not in ds.variables:\n",
    "    raise ValueError(\"Variable 'pr' not found in the datasets!\")\n",
    "\n",
    "ds = ds.rename({\"pr\": \"prcp\"})\n",
    "\n",
    "###############################################\n",
    "# 4) Convert from kg m-2 s-1 to mm/day\n",
    "###############################################\n",
    "# Explanation: 1 kg m-2 s-1 = 1 mm/s of water flux = 86400 mm/day\n",
    "# because 1 day = 86400 seconds\n",
    "ds[\"prcp\"] = ds[\"prcp\"] * 86400.0\n",
    "\n",
    "# Update attributes\n",
    "ds[\"prcp\"].attrs[\"units\"] = \"mm/day\"\n",
    "old_long_name = ds[\"prcp\"].attrs.get(\"long_name\", \"\")\n",
    "ds[\"prcp\"].attrs[\"long_name\"] = f\"{old_long_name} (converted to mm/day)\"\n",
    "\n",
    "###############################################\n",
    "# 5) Save to a new netCDF file\n",
    "###############################################\n",
    "ds.to_netcdf(output_file)\n",
    "print(f\"\\nMerged and converted dataset saved to:\\n{output_file}\")\n",
    "\n",
    "# The next step is making the resampled MERIT DEM for RDRS spatial resolution (0.1) and adding elevation to the prcp and tmin/tmax netcdf files, the same as what has been done for PRISM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94df137-742e-49d2-987d-ca1ea732bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the same procedure for ERA5, MERRA-2, and CHIRPS datasets:\n",
    "#1. Masking to a box over the Great Lakes Basin\n",
    "#2. Merging the files to a period that includes at least 1991-2012 (daily)\n",
    "#3. Adding elevation to their netcdf files so that the dimensions of each dataset include time, lat, lon, elevation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
