{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215bd774-1af4-40b2-97d9-e633ee209589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  PRISM precipitation · 8-nearest locally weighted regression to stations\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATIONS\n",
    "###############################################################################\n",
    "# Input files\n",
    "station_file   = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\prcp_data.nc'\n",
    "prism_file     = r'D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\PRISM_GLB_Precipitation\\PRISM_prcp_GLB_1991-2013_with_Elevation.nc'\n",
    "physical_file  = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv'\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"   #For plotting\n",
    "lakes_shp      = r'D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp' #For plotting\n",
    "target_crs     = \"ESRI:102008\"\n",
    "\n",
    "# Paths for daily loop & metrics (same directory as PRISM file)\n",
    "daily_loop_dir    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\PRISM_GLB_Precipitation\\daily_loop3\"\n",
    "metrics_dir       = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\PRISM_GLB_Precipitation\\metrics3\"\n",
    "\n",
    "os.makedirs(daily_loop_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir,    exist_ok=True)\n",
    "\n",
    "# Variable names\n",
    "obs_var_name   = 'prcp'   # station variable\n",
    "prism_var_name = 'prcp'   # PRISM variable\n",
    "\n",
    "# LWR settings\n",
    "NEAREST = 8               # ← add this line (number of neighbouring grid-cells)\n",
    "\n",
    "# Time range\n",
    "start_date, end_date = '1991-01-01', '2012-12-31'\n",
    "\n",
    "###############################################################################\n",
    "# 2. DISTANCE & WEIGHT FUNCTIONS\n",
    "###############################################################################\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = (np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2)\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c\n",
    "\n",
    "def tricube_weight(distances, d_max):\n",
    "    if d_max == 0:\n",
    "        return np.ones_like(distances)\n",
    "    ratio = distances / d_max\n",
    "    w = (1 - ratio**3)**3\n",
    "    w[distances > d_max] = 0.0\n",
    "    return w\n",
    "\n",
    "###############################################################################\n",
    "# 3. OPTIONAL: FORCE prism TIME\n",
    "###############################################################################\n",
    "def force_prism_time(ds):\n",
    "    \"\"\"\n",
    "    If needed, convert time coordinate to datetime.\n",
    "    If ds['time'] is already a standard datetime, skip or simplify.\n",
    "    \"\"\"\n",
    "    if 'time' not in ds.coords and 'time' in ds.dims:\n",
    "        ds = ds.assign_coords(time=ds['time'])\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "    return ds\n",
    "\n",
    "###############################################################################\n",
    "# 4. LOAD DATASETS\n",
    "###############################################################################\n",
    "print(\"Loading station observations (NetCDF) ...\")\n",
    "obs_ds = xr.open_dataset(station_file)\n",
    "\n",
    "print(\"Loading prism reanalysis (NetCDF) ...\")\n",
    "prism_ds = xr.open_dataset(prism_file)\n",
    "\n",
    "# Fix station time coords if needed\n",
    "if 'time' in obs_ds.coords:\n",
    "    obs_ds['time'] = pd.to_datetime(obs_ds['time'].values)\n",
    "\n",
    "# Force prism time if needed\n",
    "prism_ds = force_prism_time(prism_ds)\n",
    "\n",
    "# Subset to 1991–2012\n",
    "obs_ds   = obs_ds.sel(time=slice(start_date, end_date))\n",
    "prism_ds = prism_ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "print(\"After subsetting:\")\n",
    "print(f\"obs_ds time steps = {obs_ds.sizes['time']}\")\n",
    "print(f\"prism_ds time steps = {prism_ds.sizes['time']}\")\n",
    "\n",
    "###############################################################################\n",
    "# 5. ENSURE EACH VARIABLE IS (time, lat, lon)\n",
    "###############################################################################\n",
    "d = prism_ds[prism_var_name].dims\n",
    "\n",
    "# ── native PRISM order -------------------------------------------------------\n",
    "if d == ('lat', 'lon', 'time'):\n",
    "    prism_ds[prism_var_name] = prism_ds[prism_var_name].transpose('time',\n",
    "                                                                  'lat', 'lon')\n",
    "\n",
    "# ── another common permutation ----------------------------------------------\n",
    "elif d == ('time', 'lon', 'lat'):\n",
    "    prism_ds[prism_var_name] = prism_ds[prism_var_name].transpose('time',\n",
    "                                                                  'lat', 'lon')\n",
    "\n",
    "# ── already correct (time,lat,lon) ------------------------------------------\n",
    "elif d == ('time', 'lat', 'lon'):\n",
    "    pass\n",
    "\n",
    "# ── anything else is unexpected ---------------------------------------------\n",
    "else:\n",
    "    warnings.warn(f\"Unexpected dim order for {prism_var_name}: {d}\")\n",
    "\n",
    "################################################################################\n",
    "# 5-B. LOAD VARIABLES & OBSERVATIONS INTO MEMORY (for speed) + GRID COORDS\n",
    "################################################################################\n",
    "print(\"Reading PRISM and station arrays into RAM …\")\n",
    "prism_arr = prism_ds[prism_var_name].values          # (time, lat, lon)\n",
    "obs_arr   = obs_ds [obs_var_name ].values            # (time, station)\n",
    "\n",
    "# --- grid coordinates --------------------------------------------------------\n",
    "lats = prism_ds['lat'].values\n",
    "lons = prism_ds['lon'].values\n",
    "lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "\n",
    "grid_elev = (prism_ds['elevation'].values\n",
    "             if 'elevation' in prism_ds else np.zeros_like(lon2d))\n",
    "\n",
    "grid_lat_flat  = lat2d.ravel()\n",
    "grid_lon_flat  = lon2d.ravel()\n",
    "grid_elev_flat = grid_elev.ravel()\n",
    "\n",
    "# keep station names before closing the files\n",
    "station_names_nc = obs_ds['station'].values\n",
    "\n",
    "prism_ds.close()\n",
    "obs_ds.close()\n",
    "\n",
    "###############################################################################\n",
    "# 6. EXTRACT PRISM GRID\n",
    "###############################################################################\n",
    "#lats = prism_ds['lat'].values\n",
    "#lons = prism_ds['lon'].values\n",
    "#lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "\n",
    "#if 'elevation' in prism_ds:\n",
    "#    grid_elev = prism_ds['elevation'].values\n",
    "#else:\n",
    "#    warnings.warn(\"No 'elevation' found in prism dataset => set elev=0.\")\n",
    "#    grid_elev = np.zeros_like(lat2d)\n",
    "\n",
    "#grid_lat_flat  = lat2d.flatten()\n",
    "#grid_lon_flat  = lon2d.flatten()\n",
    "#grid_elev_flat = grid_elev.flatten()\n",
    "\n",
    "###############################################################################\n",
    "# 7. LOAD STATIONS & MATCH\n",
    "###############################################################################\n",
    "print(\"Loading station metadata (CSV) …\")\n",
    "stations_df = pd.read_csv(physical_file).dropna(axis=1, how='all')\n",
    "\n",
    "station_index_map = {name: i for i, name in enumerate(station_names_nc)}\n",
    "stations_df['netcdf_index'] = stations_df['NAME'].map(station_index_map)\n",
    "\n",
    "stations_df = (stations_df\n",
    "               .dropna(subset=['netcdf_index'])\n",
    "               .reset_index(drop=True)\n",
    "               .astype({'netcdf_index': int}))\n",
    "print(f\"Total matched stations: {len(stations_df)}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 8. PRECOMPUTE THE 8 NEAREST GRIDS FOR EACH STATION\n",
    "###############################################################################\n",
    "station_neighbors = {}\n",
    "print(f\"Pre-computing the {NEAREST} nearest PRISM grids for each station …\")\n",
    "\n",
    "for i, row in stations_df.iterrows():\n",
    "    d = haversine_distance(row['LATITUDE'], row['LONGITUDE'],\n",
    "                           grid_lat_flat, grid_lon_flat)\n",
    "    station_neighbors[i] = np.argsort(d)[:NEAREST]\n",
    "\n",
    "print(\"Neighbour pre-computation complete.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 9. LOCAL-WEIGHTED REGRESSION FUNCTION  (robust to NaNs & rank-deficiency)\n",
    "###############################################################################\n",
    "def local_weighted_regression(\n",
    "    station_lat, station_lon, station_elev,\n",
    "    neighbor_indices,\n",
    "    grid_lat, grid_lon, grid_elev, grid_val\n",
    "):\n",
    "    \"\"\"\n",
    "    Return the LWR estimate for one station on one day, using an\n",
    "    inverse-distance tricube-weighted multiple-linear model:\n",
    "\n",
    "        value ~ 1 + lat + lon + elev\n",
    "\n",
    "    • If fewer than 3 valid neighbours remain after NaN-filtering, the\n",
    "      function returns NaN.\n",
    "    • If np.linalg.lstsq fails (rare), it falls back to a simple\n",
    "      distance-weighted mean.\n",
    "    • Negative predictions are clipped to 0 (precipitation can’t be < 0).\n",
    "    \"\"\"\n",
    "    # ── sanity check ---------------------------------------------------------\n",
    "    if neighbor_indices is None or len(neighbor_indices) == 0:\n",
    "        return np.nan                           # no neighbours → cannot interpolate\n",
    "\n",
    "    # ── slice neighbour data -------------------------------------------------\n",
    "    lat_n   = grid_lat [neighbor_indices]\n",
    "    lon_n   = grid_lon [neighbor_indices]\n",
    "    elev_n  = grid_elev[neighbor_indices]\n",
    "    val_n   = grid_val [neighbor_indices]\n",
    "\n",
    "    # ── discard neighbours whose value is NaN --------------------------------\n",
    "    valid   = ~np.isnan(val_n)\n",
    "    if valid.sum() < 3:                         # need ≥3 points for 4-parameter fit\n",
    "        return np.nan\n",
    "    lat_n, lon_n, elev_n, val_n = (\n",
    "        lat_n[valid], lon_n[valid], elev_n[valid], val_n[valid]\n",
    "    )\n",
    "\n",
    "    # ── tricube weights from great-circle distance ---------------------------\n",
    "    dist    = haversine_distance(station_lat, station_lon, lat_n, lon_n)\n",
    "    w       = tricube_weight(dist, dist.max())\n",
    "    if np.all(w == 0.0):\n",
    "        return np.nan\n",
    "\n",
    "    # ── weighted multiple-linear regression ----------------------------------\n",
    "    X       = np.column_stack([np.ones_like(lat_n), lat_n, lon_n, elev_n])\n",
    "    sqrt_w  = np.sqrt(w)\n",
    "\n",
    "    try:\n",
    "        beta, *_ = np.linalg.lstsq(\n",
    "            X * sqrt_w[:, None],                # weight columns\n",
    "            val_n * sqrt_w,                     # weight response\n",
    "            rcond=None\n",
    "        )\n",
    "        pred = np.dot([1, station_lat, station_lon, station_elev], beta)\n",
    "    except np.linalg.LinAlgError:               # rare SVD failure\n",
    "        pred = np.average(val_n, weights=w)     # fallback: weighted mean\n",
    "\n",
    "    return max(pred, 0.0)                       # precipitation cannot be negative\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 10. DAILY LOOP — PRISM → STATIONS (fast, RAM-based)\n",
    "###############################################################################\n",
    "results  = []\n",
    "n_days   = prism_arr.shape[0]\n",
    "print(f\"Processing {n_days} days …\")\n",
    "\n",
    "for t_idx in range(n_days):\n",
    "    curr_time = pd.to_datetime(start_date) + pd.Timedelta(days=int(t_idx))\n",
    "\n",
    "    grid_flat = prism_arr[t_idx].ravel()   # (lat*lon,)\n",
    "    obs_day   = obs_arr[t_idx]             # (station,)\n",
    "\n",
    "    for i, row in stations_df.iterrows():\n",
    "        pred = local_weighted_regression(\n",
    "            row['LATITUDE'], row['LONGITUDE'], row.get('Elevation', 0.0),\n",
    "            station_neighbors[i],\n",
    "            grid_lat_flat, grid_lon_flat, grid_elev_flat, grid_flat\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'time'         : curr_time,\n",
    "            'station_index': i,\n",
    "            'station_name' : row['NAME'],\n",
    "            'obs'          : obs_day[row['netcdf_index']],\n",
    "            'prism_lwr8_val': pred\n",
    "        })\n",
    "\n",
    "    if (t_idx + 1) % 500 == 0 or t_idx == n_days-1:\n",
    "        print(f\"  {t_idx+1}/{n_days} days done\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 11. BUILD A DATAFRAME & SAVE\n",
    "###############################################################################\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['time'] = pd.to_datetime(results_df['time'])\n",
    "print(\"Sample of daily results:\\n\", results_df.head())\n",
    "\n",
    "# CSV output with the 8-nearest LWR data\n",
    "daily_loop_csv = os.path.join(daily_loop_dir, \"prism_vs_stations_8Nearest_LWR_1991_2012.csv\")\n",
    "results_df.to_csv(daily_loop_csv, index=False)\n",
    "print(f\"Daily interpolation results saved to {daily_loop_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 12. (OPTIONAL) MAPPING\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"Creating a quick map of average ERA5 precipitation at each station (8-nearest LWR)...\")\n",
    "    glb  = gpd.read_file(shapefile_path).to_crs(target_crs)\n",
    "    lakes= gpd.read_file(lakes_shp).to_crs(target_crs)\n",
    "\n",
    "    station_mean = results_df.groupby('station_index')['prism_lwr8_val'].mean().reset_index()\n",
    "    merged = stations_df.copy()\n",
    "    merged['mean_prism_lwr8'] = station_mean['prism_lwr8_val']\n",
    "\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(merged['LONGITUDE'], merged['LATITUDE'])]\n",
    "    stations_gdf = gpd.GeoDataFrame(merged, geometry=geometry, crs=\"EPSG:4326\").to_crs(target_crs)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    glb.boundary.plot(ax=ax, edgecolor='black')\n",
    "    lakes.plot(ax=ax, color='blue', alpha=0.5)\n",
    "    stations_gdf.plot(column='mean_prism_lwr8', ax=ax, legend=True, cmap='viridis', markersize=50)\n",
    "    ax.set_title(\"Mean prism Precip (8-Nearest LWR), 1991-2012\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Mapping step skipped due to error or missing shapefiles:\", str(e))\n",
    "\n",
    "###############################################################################\n",
    "# 13. METRICS & SAVING\n",
    "###############################################################################\n",
    "def remove_nan_pairs(obs, pred):\n",
    "    mask = ~np.isnan(obs) & ~np.isnan(pred)\n",
    "    return obs[mask], pred[mask]\n",
    "\n",
    "def mean_bias_error(obs, pred):\n",
    "    return np.mean(pred - obs)\n",
    "\n",
    "def root_mean_square_error(obs, pred):\n",
    "    return np.sqrt(np.mean((pred - obs)**2))\n",
    "\n",
    "def std_of_residuals(obs, pred):\n",
    "    return np.std(pred - obs, ddof=1)\n",
    "\n",
    "def pearson_correlation(obs, pred):\n",
    "    if len(obs) < 2:\n",
    "        return np.nan\n",
    "    return np.corrcoef(obs, pred)[0, 1]\n",
    "\n",
    "def index_of_agreement(obs, pred):\n",
    "    obs_mean = np.mean(obs)\n",
    "    numerator = np.sum((pred - obs)**2)\n",
    "    denominator = np.sum((np.abs(pred - obs_mean) + np.abs(obs - obs_mean))**2)\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "obs_all = results_df['obs'].values\n",
    "prism_all = results_df['prism_lwr8_val'].values\n",
    "obs_all, prism_all = remove_nan_pairs(obs_all, prism_all)\n",
    "\n",
    "if len(obs_all) > 0:\n",
    "    metrics_all = {\n",
    "        'MBE': mean_bias_error(obs_all, prism_all),\n",
    "        'RMSE': root_mean_square_error(obs_all, prism_all),\n",
    "        'STD': std_of_residuals(obs_all, prism_all),\n",
    "        'CC': pearson_correlation(obs_all, prism_all),\n",
    "        'Index_of_Agreement': index_of_agreement(obs_all, prism_all)\n",
    "    }\n",
    "    print(\"\\nOverall Metrics (All Stations, prism 8-Nearest LWR, 1991-2012):\")\n",
    "    for k, v in metrics_all.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid (obs, prism_lwr8_val) pairs found for overall metrics.\")\n",
    "\n",
    "# Station-level metrics\n",
    "station_groups = results_df.groupby('station_index')\n",
    "per_station_metrics = []\n",
    "\n",
    "for st_idx, grp in station_groups:\n",
    "    obs_st = grp['obs'].values\n",
    "    prism_st = grp['prism_lwr8_val'].values\n",
    "    obs_st, prism_st = remove_nan_pairs(obs_st, prism_st)\n",
    "    if len(obs_st) == 0:\n",
    "        per_station_metrics.append({\n",
    "            'station_index': st_idx,\n",
    "            'station_name': grp['station_name'].iloc[0],\n",
    "            'MBE': np.nan,\n",
    "            'RMSE': np.nan,\n",
    "            'STD': np.nan,\n",
    "            'CC': np.nan,\n",
    "            'Index_of_Agreement': np.nan\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    row_dict = {\n",
    "        'station_index': st_idx,\n",
    "        'station_name': grp['station_name'].iloc[0],\n",
    "        'MBE':  mean_bias_error(obs_st, prism_st),\n",
    "        'RMSE': root_mean_square_error(obs_st, prism_st),\n",
    "        'STD':  std_of_residuals(obs_st, prism_st),\n",
    "        'CC':   pearson_correlation(obs_st, prism_st),\n",
    "        'Index_of_Agreement': index_of_agreement(obs_st, prism_st)\n",
    "    }\n",
    "    per_station_metrics.append(row_dict)\n",
    "\n",
    "metrics_df = pd.DataFrame(per_station_metrics)\n",
    "print(\"\\nSample of per-station metrics:\")\n",
    "print(metrics_df.head())\n",
    "\n",
    "metrics_csv = os.path.join(metrics_dir, \"station_metrics_8Nearest_LWR_prism_1991_2012.csv\")\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "print(f\"Station metrics saved to {metrics_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 14. (OPTIONAL) SAVE FULL DAILY RESULTS AS NETCDF\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nSaving daily station results to NetCDF file in daily_loop folder...\")\n",
    "    netcdf_file = os.path.join(daily_loop_dir, \"prism_vs_stations_8Nearest_LWR_1991_2012.nc\")\n",
    "\n",
    "    # 1) Pivot so 'time' is the row index, 'station_index' are columns\n",
    "    pivot_prism = results_df.pivot(index='time', columns='station_index', values='prism_lwr8_val')\n",
    "    pivot_obs  = results_df.pivot(index='time', columns='station_index', values='obs')\n",
    "\n",
    "    # 2) Ensure the pivoted index is recognized as real datetime\n",
    "    pivot_prism.index = pd.to_datetime(pivot_prism.index, errors='coerce')\n",
    "    pivot_obs.index  = pd.to_datetime(pivot_obs.index,  errors='coerce')\n",
    "\n",
    "    # 3) Convert each pivoted DataFrame into an xarray Dataset\n",
    "    ds_prism = pivot_prism.to_xarray()\n",
    "    ds_obs  = pivot_obs.to_xarray()\n",
    "\n",
    "    if 'index' in ds_prism.dims:\n",
    "        ds_prism = ds_prism.rename({'index': 'time'})\n",
    "    if 'columns' in ds_prism.dims:\n",
    "        ds_prism = ds_prism.rename({'columns': 'station_index'})\n",
    "\n",
    "    if 'index' in ds_obs.dims:\n",
    "        ds_obs = ds_obs.rename({'index': 'time'})\n",
    "    if 'columns' in ds_obs.dims:\n",
    "        ds_obs = ds_obs.rename({'columns': 'station_index'})\n",
    "\n",
    "    da_prism = ds_prism.to_array(name='prism_lwr8_val').squeeze()\n",
    "    da_obs  = ds_obs.to_array(name='obs').squeeze()\n",
    "\n",
    "    ds_out = xr.Dataset({\n",
    "        'prism_lwr8_val': da_prism,\n",
    "        'obs':           da_obs\n",
    "    })\n",
    "\n",
    "    ds_out.to_netcdf(netcdf_file)\n",
    "    print(f\"NetCDF saved to: {netcdf_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error saving NetCDF:\", e)\n",
    "\n",
    "print(\"\\nDone. The 8-nearest LWR interpolation for prism dataset is complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc89f1d-f257-4482-b341-cd8f04600724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMDNA\n",
    "# The approach of making the interpolated EMDNA data at the station location with LWR 25 km for prcp \"10 ensembles\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "###############################################################################\n",
    "# 0. ENSEMBLE LIST  &  ROOT PATH\n",
    "###############################################################################\n",
    "ENSEMBLES = [1, 11, 21, 31, 41, 51, 61, 71, 81, 91]\n",
    "\n",
    "root_dir = (\n",
    "    r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\"\n",
    "    r\"\\Ensemble files\\EMDNA_GLB_Precipitation\"\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATIONS\n",
    "###############################################################################\n",
    "for ENS in ENSEMBLES:\n",
    "    print(f\"\\n================  ENSEMBLE {ENS}  ===========================\")\n",
    "\n",
    "    # ---- generic, ensemble-independent files --------------------------------\n",
    "    station_file  = r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\prcp_data.nc\"\n",
    "    physical_file = (\n",
    "        r\"D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv\"\n",
    "    )\n",
    "    shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "    lakes_shp      = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp\"\n",
    "    target_crs     = \"ESRI:102008\"\n",
    "\n",
    "    # ---- ensemble-specific paths & files ------------------------------------\n",
    "    ens_dir     = os.path.join(root_dir, str(ENS))\n",
    "    emdna_file  = os.path.join(\n",
    "        ens_dir,\n",
    "        f\"EMDNA_{ENS:03d}_merged_prcp_1991_2013_with_Elevation.nc\"\n",
    "    )\n",
    "\n",
    "    daily_loop_dir = os.path.join(ens_dir, \"daily_loop\")\n",
    "    metrics_dir    = os.path.join(ens_dir, \"metrics\")\n",
    "    os.makedirs(daily_loop_dir, exist_ok=True)\n",
    "    os.makedirs(metrics_dir,    exist_ok=True)\n",
    "\n",
    "    # ─── variables we will handle ───────────────────────────────────────────────\n",
    "    VAR_LIST = [\"prcp\"]                 # ◄─ just precipitation this time\n",
    "    \n",
    "    # Local-Weighted Regression settings\n",
    "    search_radius_km = 25\n",
    "    min_points       = 5\n",
    "    \n",
    "    start_date, end_date = \"1991-01-01\", \"2012-12-31\"\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 2. DISTANCE & WEIGHT FUNCTIONS\n",
    "    ###############################################################################\n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = (np.sin(dlat / 2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon / 2)**2)\n",
    "        c = 2*np.arcsin(np.sqrt(a))\n",
    "        return 6371 * c\n",
    "    \n",
    "    def tricube_weight(distances, d_max):\n",
    "        \"\"\"\n",
    "        w_j = [1 - (dist_j / d_max)^3]^3  for dist_j <= d_max; else 0\n",
    "        \"\"\"\n",
    "        if d_max == 0:\n",
    "            return np.ones_like(distances)\n",
    "        ratio = distances / d_max\n",
    "        w = (1 - ratio**3)**3\n",
    "        w[distances > d_max] = 0.0\n",
    "        return w\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 3. FORCE EMDNA TIME (IF NEEDED)\n",
    "    ###############################################################################\n",
    "    def force_emdna_time(ds):\n",
    "        if 'time' not in ds.coords and 'time' in ds.dims:\n",
    "            ds = ds.assign_coords(time=ds['time'])\n",
    "        if np.issubdtype(ds['time'].dtype, np.integer):\n",
    "            day0 = pd.to_datetime(\"1991-01-01\")\n",
    "            numeric_days = ds['time'].values\n",
    "            real_times = [day0 + pd.Timedelta(days=int(d)) for d in numeric_days]\n",
    "            ds = ds.assign_coords(time=(\"time\", real_times))\n",
    "        ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "        return ds\n",
    "    \n",
    "    print(\"Loading station observations (NetCDF) ...\")\n",
    "    obs_ds = xr.open_dataset(station_file)\n",
    "    \n",
    "    print(\"Loading EMDNA reanalysis (NetCDF) ...\")\n",
    "    emdna_ds = xr.open_dataset(emdna_file)\n",
    "    \n",
    "    # ─── fix / force time coordinates ───────────────────────────────────────────\n",
    "    if 'time' in obs_ds.coords:\n",
    "        obs_ds['time'] = pd.to_datetime(obs_ds['time'].values)\n",
    "    emdna_ds = force_emdna_time(emdna_ds)\n",
    "    \n",
    "    # ─── convert rotated-pole coordinates if present ────────────────────────────\n",
    "    if {'rlat', 'rlon'}.issubset(emdna_ds.coords):\n",
    "        print(\"Renaming rlat/rlon ➜ lat/lon ...\")\n",
    "        emdna_ds = emdna_ds.rename({'rlat': 'lat', 'rlon': 'lon'})\n",
    "    \n",
    "    # ─── subset analysis period ─────────────────────────────────────────────────\n",
    "    obs_ds  = obs_ds.sel(time=slice(start_date, end_date))\n",
    "    emdna_ds = emdna_ds.sel(time=slice(start_date, end_date))\n",
    "    \n",
    "    print(\"After subsetting:\")\n",
    "    print(f\"obs_ds time steps  = {obs_ds.sizes['time']}\")\n",
    "    print(f\"emdna_ds time steps = {emdna_ds.sizes['time']}\")\n",
    "    \n",
    "    # ─── ensure each variable is (time, lat, lon) ───────────────────────────────\n",
    "    for v in VAR_LIST:\n",
    "        dims_now = emdna_ds[v].dims\n",
    "        print(f\"Current dims for EMDNA {v}: {dims_now}\")\n",
    "        if dims_now == ('time', 'lon', 'lat'):\n",
    "            print(f\"Transposing {v} from (time,lon,lat) ➜ (time,lat,lon)\")\n",
    "            emdna_ds[v] = emdna_ds[v].transpose('time', 'lat', 'lon')\n",
    "        elif dims_now != ('time', 'lat', 'lon'):\n",
    "            warnings.warn(f\"Unexpected dimension order for {v}: {dims_now}\")\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 3-B. READ THE VARIABLE INTO MEMORY (avoids slow disk hit each day)\n",
    "    ###############################################################################\n",
    "    print(\"Loading EMDNA and station-obs arrays into memory …\")\n",
    "    emdna_arr = {v: emdna_ds[v].values for v in VAR_LIST}   # dict: v → (time,lat,lon)\n",
    "    obs_arr   = {v:  obs_ds[v].values for v in VAR_LIST}    # dict: v → (time,station)\n",
    "    \n",
    "    # ─── build flattened coordinate arrays for neighbour search ────────────────\n",
    "    lats = emdna_ds['lat'].values\n",
    "    lons = emdna_ds['lon'].values\n",
    "    lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "    \n",
    "    if 'elevation' in emdna_ds:\n",
    "        grid_elev = emdna_ds['elevation'].values\n",
    "    else:\n",
    "        warnings.warn(\"No 'elevation' variable in EMDNA file – using zeros.\")\n",
    "        grid_elev = np.zeros_like(lat2d)\n",
    "    \n",
    "    grid_lat_flat  = lat2d.ravel()\n",
    "    grid_lon_flat  = lon2d.ravel()\n",
    "    grid_elev_flat = grid_elev.ravel()\n",
    "    \n",
    "    # ── keep station names *before* closing the NetCDF handles ────────────────\n",
    "    station_names_nc = obs_ds['station'].values\n",
    "    \n",
    "    # all arrays & coords are now in RAM – close the files\n",
    "    emdna_ds.close()\n",
    "    obs_ds.close()\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 4. LOAD STATIONS & MATCH\n",
    "    ###############################################################################\n",
    "    print(\"Loading station metadata (CSV) ...\")\n",
    "    stations_df = pd.read_csv(physical_file).dropna(axis=1, how='all')\n",
    "    print(stations_df.head())\n",
    "    \n",
    "    print(\"Matching station names to NetCDF index ...\")\n",
    "    station_index_map = {name: i for i, name in enumerate(station_names_nc)}\n",
    "    stations_df['netcdf_index'] = stations_df['NAME'].map(station_index_map)\n",
    "    stations_df = stations_df.dropna(subset=['netcdf_index']).reset_index(drop=True)\n",
    "    stations_df['netcdf_index'] = stations_df['netcdf_index'].astype(int)\n",
    "    print(\"Total matched stations:\", len(stations_df))\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 5. PRECOMPUTE NEIGHBORS FOR EACH STATION\n",
    "    ###############################################################################\n",
    "    station_neighbors = {}\n",
    "    print(f\"Precomputing neighbors within {search_radius_km} km...\")\n",
    "    \n",
    "    for i, row in stations_df.iterrows():\n",
    "        st_lat = row['LATITUDE']\n",
    "        st_lon = row['LONGITUDE']\n",
    "        dist_all = haversine_distance(st_lat, st_lon, grid_lat_flat, grid_lon_flat)\n",
    "        neighbor_idx = np.where(dist_all <= search_radius_km)[0]\n",
    "        if len(neighbor_idx) < min_points:\n",
    "            station_neighbors[i] = None\n",
    "        else:\n",
    "            station_neighbors[i] = neighbor_idx\n",
    "    \n",
    "    print(\"Neighbor precomputation complete.\")\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 6. LOCAL WEIGHTED REGRESSION FUNCTION\n",
    "    ###############################################################################\n",
    "    def local_weighted_regression(\n",
    "        station_lat, station_lon, station_elev,\n",
    "        neighbor_indices,\n",
    "        grid_lat, grid_lon, grid_elev, grid_val\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Predict daily precipitation at one station with a 25-km LWR:\n",
    "    \n",
    "            prcp ~ 1 + lat + lon + elev\n",
    "    \n",
    "        • Weights: tricube of great-circle distance.\n",
    "        • Requires ≥ min_points valid neighbours; otherwise returns NaN.\n",
    "        • Negative predictions are *not* allowed → clipped to 0.0 mm.\n",
    "        \"\"\"\n",
    "        if neighbor_indices is None or len(neighbor_indices) < min_points:\n",
    "            return np.nan\n",
    "    \n",
    "        lat_n  = grid_lat [neighbor_indices]\n",
    "        lon_n  = grid_lon [neighbor_indices]\n",
    "        elev_n = grid_elev[neighbor_indices]\n",
    "        val_n  = grid_val [neighbor_indices]\n",
    "    \n",
    "        mask = ~np.isnan(val_n)\n",
    "        if mask.sum() < min_points:\n",
    "            return np.nan\n",
    "        lat_n, lon_n, elev_n, val_n = lat_n[mask], lon_n[mask], elev_n[mask], val_n[mask]\n",
    "    \n",
    "        dist = haversine_distance(station_lat, station_lon, lat_n, lon_n)\n",
    "        w    = tricube_weight(dist, dist.max())\n",
    "        if np.all(w == 0):\n",
    "            return np.nan\n",
    "    \n",
    "        X       = np.column_stack([np.ones_like(lat_n), lat_n, lon_n, elev_n])\n",
    "        sqrt_w  = np.sqrt(w)\n",
    "        try:\n",
    "            beta, *_ = np.linalg.lstsq(X * sqrt_w[:, None], val_n * sqrt_w, rcond=None)\n",
    "            pred = np.dot([1, station_lat, station_lon, station_elev], beta)\n",
    "        except np.linalg.LinAlgError:\n",
    "            pred = np.average(val_n, weights=w)\n",
    "    \n",
    "        return max(pred, 0.0)           # precipitation cannot be negative\n",
    "    ###############################################################################\n",
    "    # 7. DAILY LOOP  – EMDNA ➜ STATIONS  (RAM-based, no disk reads in loop)\n",
    "    ###############################################################################\n",
    "    results  = []\n",
    "    n_days   = emdna_arr[VAR_LIST[0]].shape[0]\n",
    "    print(f\"Processing {n_days} days …\")\n",
    "    \n",
    "    for t_index in range(n_days):\n",
    "        current_time = pd.to_datetime(start_date) + pd.Timedelta(days=t_index)\n",
    "    \n",
    "        for var in VAR_LIST:\n",
    "            # (lat,lon) slice for this day already in memory\n",
    "            day_data_2d       = emdna_arr[var][t_index, :, :]\n",
    "            grid_val_flat_day = day_data_2d.ravel()\n",
    "    \n",
    "            obs_day = obs_arr[var][t_index, :]   # shape (station,)\n",
    "    \n",
    "            for i, row in stations_df.iterrows():\n",
    "                netcdf_idx = row[\"netcdf_index\"]\n",
    "                st_obs     = obs_day[netcdf_idx] if netcdf_idx < len(obs_day) else np.nan\n",
    "    \n",
    "                st_emdna = local_weighted_regression(\n",
    "                    station_lat  = row[\"LATITUDE\"],\n",
    "                    station_lon  = row[\"LONGITUDE\"],\n",
    "                    station_elev = row.get(\"Elevation\", 0.0),\n",
    "                    neighbor_indices = station_neighbors[i],\n",
    "                    grid_lat  = grid_lat_flat,\n",
    "                    grid_lon  = grid_lon_flat,\n",
    "                    grid_elev = grid_elev_flat,\n",
    "                    grid_val  = grid_val_flat_day\n",
    "                )\n",
    "    \n",
    "                results.append({\n",
    "                    \"time\"         : current_time,\n",
    "                    \"var\"          : var,\n",
    "                    \"station_index\": i,\n",
    "                    \"station_name\" : row[\"NAME\"],\n",
    "                    \"obs\"          : st_obs,\n",
    "                    \"emdna_lwr25_val\": st_emdna\n",
    "                })\n",
    "    \n",
    "        if (t_index + 1) % 100 == 0:\n",
    "            print(f\"Processed {t_index+1} / {n_days} days\")\n",
    "    \n",
    "    \n",
    "    ###############################################################################\n",
    "    # 8. BUILD A DATAFRAME & SAVE\n",
    "    ###############################################################################\n",
    "    results_df            = pd.DataFrame(results)\n",
    "    results_df['time']    = pd.to_datetime(results_df['time'])\n",
    "    print(\"Sample of daily results:\\n\", results_df.head())\n",
    "    \n",
    "    daily_loop_csv = os.path.join(\n",
    "        daily_loop_dir,\n",
    "        f\"emdna_vs_stations_25km_LWR_1991_2012_prcp_{ENS:03d}.csv\"\n",
    "    )\n",
    "    results_df.to_csv(daily_loop_csv, index=False)\n",
    "    print(f\"Daily interpolation results saved to {daily_loop_csv}\")\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 9. (OPTIONAL) MAPPING  – one map per variable\n",
    "    ###############################################################################\n",
    "    try:\n",
    "        glb   = gpd.read_file(shapefile_path).to_crs(target_crs)\n",
    "        lakes = gpd.read_file(lakes_shp).to_crs(target_crs)\n",
    "    \n",
    "        for var in VAR_LIST:\n",
    "            print(f\"Creating map for {var} …\")\n",
    "            sub = results_df[results_df[\"var\"] == var]\n",
    "    \n",
    "            # mean value at each station_index\n",
    "            station_mean = (\n",
    "                sub.groupby(\"station_index\")[\"emdna_lwr25_val\"]\n",
    "                   .mean()\n",
    "                   .reset_index()\n",
    "                   .rename(columns={\"emdna_lwr25_val\": \"mean_emdna\"})\n",
    "            )\n",
    "    \n",
    "            # join to station metadata so rows line up correctly\n",
    "            merged = stations_df.merge(station_mean, on=\"station_index\", how=\"left\")\n",
    "    \n",
    "            geometry = [\n",
    "                Point(lon, lat)\n",
    "                for lon, lat in zip(merged[\"LONGITUDE\"], merged[\"LATITUDE\"])\n",
    "            ]\n",
    "            stations_gdf = (\n",
    "                gpd.GeoDataFrame(merged, geometry=geometry, crs=\"EPSG:4326\")\n",
    "                .to_crs(target_crs)\n",
    "            )\n",
    "    \n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            glb.boundary.plot(ax=ax, edgecolor=\"black\")\n",
    "            lakes.plot(ax=ax, color=\"blue\", alpha=0.5)\n",
    "            stations_gdf.plot(column=\"mean_emdna\", ax=ax,\n",
    "                              cmap=\"viridis\", legend=True, markersize=50)\n",
    "            ax.set_title(f\"Mean EMDNA {var.upper()} (LWR 25 km), 1991-2012\")\n",
    "            plt.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Mapping step skipped:\", e)\n",
    "    \n",
    "    ###############################################################################\n",
    "    # 10. METRICS & SAVING\n",
    "    ###############################################################################\n",
    "    def remove_nan_pairs(obs, pred):\n",
    "        m = ~np.isnan(obs) & ~np.isnan(pred)\n",
    "        return obs[m], pred[m]\n",
    "    \n",
    "    # ─── overall metrics per variable ───────────────────────────────────────────\n",
    "    for var, grp in results_df.groupby('var'):\n",
    "        obs_all, emdna_all = remove_nan_pairs(grp['obs'].values,\n",
    "                                             grp['emdna_lwr25_val'].values)\n",
    "        if len(obs_all) == 0:\n",
    "            print(f\"\\nNo valid pairs for {var}.\")\n",
    "            continue\n",
    "        mbe  = np.mean(emdna_all - obs_all)\n",
    "        rmse = np.sqrt(np.mean((emdna_all - obs_all) ** 2))\n",
    "        std  = np.std(emdna_all - obs_all, ddof=1)\n",
    "        cc   = np.corrcoef(obs_all, emdna_all)[0, 1]\n",
    "        ia   = 1 - np.sum((emdna_all - obs_all) ** 2) / \\\n",
    "                  np.sum((np.abs(emdna_all - obs_all.mean()) +\n",
    "                          np.abs(obs_all - obs_all.mean())) ** 2)\n",
    "        print(f\"\\nOverall metrics ({var}, EMDNA 25-km LWR, 1991-2012):\")\n",
    "        for k, v in zip(['MBE','RMSE','STD','CC','Index_of_Agreement'],\n",
    "                        [mbe, rmse, std, cc, ia]):\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "    \n",
    "    # ─── per-station metrics (still per variable) ───────────────────────────────\n",
    "    station_metrics = []\n",
    "    for (var, st_idx), g in results_df.groupby(['var', 'station_index']):\n",
    "        o, p = remove_nan_pairs(g['obs'].values, g['emdna_lwr25_val'].values)\n",
    "        if len(o) == 0:\n",
    "            station_metrics.append({'var':var,'station_index':st_idx,\n",
    "                'station_name':g['station_name'].iloc[0],\n",
    "                'MBE':np.nan,'RMSE':np.nan,'STD':np.nan,'CC':np.nan,'Index_of_Agreement':np.nan})\n",
    "            continue\n",
    "        station_metrics.append({\n",
    "            'var'  : var,\n",
    "            'station_index' : st_idx,\n",
    "            'station_name'  : g['station_name'].iloc[0],\n",
    "            'MBE'  : np.mean(p - o),\n",
    "            'RMSE' : np.sqrt(np.mean((p - o) ** 2)),\n",
    "            'STD'  : np.std(p - o, ddof=1),\n",
    "            'CC'   : np.corrcoef(o, p)[0, 1],\n",
    "            'Index_of_Agreement':\n",
    "                1 - np.sum((p - o) ** 2) /\n",
    "                    np.sum((np.abs(p - o.mean()) + np.abs(o - o.mean())) ** 2)\n",
    "        })\n",
    "    \n",
    "    metrics_df  = pd.DataFrame(station_metrics)\n",
    "\n",
    "    metrics_csv = os.path.join(\n",
    "        metrics_dir,\n",
    "        f\"station_metrics_25km_LWR_EMDNA_1991_2012_prcp_{ENS:03d}.csv\"\n",
    "    )\n",
    "    metrics_df.to_csv(metrics_csv, index=False)\n",
    "    print(f\"Station metrics saved to {metrics_csv}\")\n",
    "    \n",
    "    # (Optional) NetCDF\n",
    "    ###############################################################################\n",
    "    # 10. (OPTIONAL) SAVE FULL DAILY RESULTS AS NETCDF\n",
    "    ###############################################################################\n",
    "    try:\n",
    "        print(\"\\nSaving daily station results to NetCDF …\")\n",
    "        netcdf_file = os.path.join(\n",
    "        daily_loop_dir,\n",
    "        f\"emdna_vs_stations_25km_LWR_1991_2012_prcp_{ENS:03d}.nc\"\n",
    "    )\n",
    "    \n",
    "        ds_vars = {}\n",
    "        for var in VAR_LIST:\n",
    "            pivot_emdna = (\n",
    "                results_df[results_df[\"var\"] == var]\n",
    "                .pivot(index=\"time\", columns=\"station_index\", values=\"emdna_lwr25_val\")\n",
    "                .sort_index()\n",
    "            )\n",
    "            pivot_obs  = (\n",
    "                results_df[results_df[\"var\"] == var]\n",
    "                .pivot(index=\"time\", columns=\"station_index\", values=\"obs\")\n",
    "                .sort_index()\n",
    "            )\n",
    "    \n",
    "            common_cols   = pivot_emdna.columns.intersection(pivot_obs.columns)\n",
    "            pivot_emdna, pivot_obs = pivot_emdna[common_cols], pivot_obs[common_cols]\n",
    "    \n",
    "            da_emdna = xr.DataArray(\n",
    "                data   = pivot_emdna.values,\n",
    "                dims   = (\"time\", \"station_index\"),\n",
    "                coords = {\"time\": pivot_emdna.index,\n",
    "                          \"station_index\": common_cols},\n",
    "                name   = f\"emdna_lwr25_{var}\"\n",
    "            )\n",
    "            da_obs  = xr.DataArray(\n",
    "                data   = pivot_obs.values,\n",
    "                dims   = (\"time\", \"station_index\"),\n",
    "                coords = {\"time\": pivot_obs.index,\n",
    "                          \"station_index\": common_cols},\n",
    "                name   = f\"obs_{var}\"\n",
    "            )\n",
    "            ds_vars[da_emdna.name] = da_emdna\n",
    "            ds_vars[da_obs.name]  = da_obs\n",
    "    \n",
    "        xr.Dataset(ds_vars).to_netcdf(netcdf_file)\n",
    "        print(f\"NetCDF saved to: {netcdf_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error saving NetCDF:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bb7be-dc4c-4525-a5ca-df6d8a1dba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ERA5 Precipitation\n",
    "# This script performs an 8-nearest-grid LWR interpolation using ERA5 data.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATIONS\n",
    "###############################################################################\n",
    "# Input files\n",
    "station_file   = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\prcp_data.nc'\n",
    "era5_file      = r'D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Total_Precipitaion\\Modified\\ERA5_GLB_prcp_daily_1991_2013_with_Elevation_mm.nc'\n",
    "physical_file  = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv'\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r'D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp'\n",
    "target_crs     = \"ESRI:102008\"\n",
    "\n",
    "# Output directories\n",
    "# Adjust to match your desired folders\n",
    "# This example uses a path near the ERA5 file location\n",
    "\n",
    "daily_loop_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Total_Precipitaion\\Modified\\daily_loop\"\n",
    "metrics_dir    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Total_Precipitaion\\Modified\\metrics\"\n",
    "\n",
    "os.makedirs(daily_loop_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir,    exist_ok=True)\n",
    "\n",
    "# Variable names\n",
    "obs_var_name  = 'prcp'  # Station daily precipitation var\n",
    "era5_var_name = 'prcp'  # ERA5 daily precipitation var (change if needed)\n",
    "\n",
    "# Time range: 1991–2012\n",
    "start_date = '1991-01-01'\n",
    "end_date   = '2012-12-31'\n",
    "\n",
    "###############################################################################\n",
    "# 2. DISTANCE & WEIGHT FUNCTIONS\n",
    "###############################################################################\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = (np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2)\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c\n",
    "\n",
    "def tricube_weight(distances, d_max):\n",
    "    if d_max == 0:\n",
    "        return np.ones_like(distances)\n",
    "    ratio = distances / d_max\n",
    "    w = (1 - ratio**3)**3\n",
    "    w[distances > d_max] = 0.0\n",
    "    return w\n",
    "\n",
    "###############################################################################\n",
    "# 3. OPTIONAL: FORCE ERA5 TIME\n",
    "###############################################################################\n",
    "def force_era5_time(ds):\n",
    "    \"\"\"\n",
    "    If needed, convert time coordinate to datetime.\n",
    "    If ds['time'] is already a standard datetime, skip or simplify.\n",
    "    \"\"\"\n",
    "    if 'time' not in ds.coords and 'time' in ds.dims:\n",
    "        ds = ds.assign_coords(time=ds['time'])\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "    return ds\n",
    "\n",
    "###############################################################################\n",
    "# 4. LOAD DATASETS\n",
    "###############################################################################\n",
    "print(\"Loading station observations (NetCDF) ...\")\n",
    "obs_ds = xr.open_dataset(station_file)\n",
    "\n",
    "print(\"Loading ERA5 reanalysis (NetCDF) ...\")\n",
    "era5_ds = xr.open_dataset(era5_file)\n",
    "\n",
    "# Fix station time coords if needed\n",
    "if 'time' in obs_ds.coords:\n",
    "    obs_ds['time'] = pd.to_datetime(obs_ds['time'].values)\n",
    "\n",
    "# Force ERA5 time if needed\n",
    "era5_ds = force_era5_time(era5_ds)\n",
    "\n",
    "# Subset to 1991–2012\n",
    "obs_ds   = obs_ds.sel(time=slice(start_date, end_date))\n",
    "era5_ds = era5_ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "print(\"After subsetting:\")\n",
    "print(f\"obs_ds time steps = {obs_ds.sizes['time']}\")\n",
    "print(f\"era5_ds time steps = {era5_ds.sizes['time']}\")\n",
    "\n",
    "###############################################################################\n",
    "# 5. IF dims are (time, lon, lat), transpose to (time, lat, lon)\n",
    "###############################################################################\n",
    "actual_dims = era5_ds[era5_var_name].dims\n",
    "print(\"Current dims for ERA5 prcp:\", actual_dims)\n",
    "if actual_dims == (\"time\",\"lon\",\"lat\"):\n",
    "    print(\"Transposing prcp from (time,lon,lat) -> (time,lat,lon).\")\n",
    "    era5_ds[era5_var_name] = era5_ds[era5_var_name].transpose(\"time\",\"lat\",\"lon\")\n",
    "    print(\"New dims:\", era5_ds[era5_var_name].dims)\n",
    "else:\n",
    "    print(\"No transpose needed or already (time,lat,lon).\")\n",
    "\n",
    "###############################################################################\n",
    "# 6. EXTRACT ERA5 GRID\n",
    "###############################################################################\n",
    "lats = era5_ds['lat'].values\n",
    "lons = era5_ds['lon'].values\n",
    "lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "\n",
    "if 'elevation' in era5_ds:\n",
    "    grid_elev = era5_ds['elevation'].values\n",
    "else:\n",
    "    warnings.warn(\"No 'elevation' found in ERA5 dataset => set elev=0.\")\n",
    "    grid_elev = np.zeros_like(lat2d)\n",
    "\n",
    "grid_lat_flat  = lat2d.flatten()\n",
    "grid_lon_flat  = lon2d.flatten()\n",
    "grid_elev_flat = grid_elev.flatten()\n",
    "\n",
    "###############################################################################\n",
    "# 7. LOAD STATIONS & MATCH\n",
    "###############################################################################\n",
    "print(\"Loading station metadata (CSV) ...\")\n",
    "stations_df = pd.read_csv(physical_file).dropna(axis=1, how='all')\n",
    "print(stations_df.head())\n",
    "\n",
    "# If station_file has dimension station, match by name\n",
    "station_names_nc = obs_ds['station'].values\n",
    "station_index_map = {name: i for i, name in enumerate(station_names_nc)}\n",
    "\n",
    "stations_df['netcdf_index'] = stations_df['NAME'].map(station_index_map)\n",
    "stations_df = stations_df.dropna(subset=['netcdf_index']).reset_index(drop=True)\n",
    "stations_df['netcdf_index'] = stations_df['netcdf_index'].astype(int)\n",
    "print(\"Total matched stations:\", len(stations_df))\n",
    "\n",
    "###############################################################################\n",
    "# 8. PRECOMPUTE THE 8 NEAREST GRIDS FOR EACH STATION\n",
    "###############################################################################\n",
    "station_neighbors = {}\n",
    "print(\"Precomputing the 8 nearest ERA5 grids for each station ...\")\n",
    "\n",
    "for i, row in stations_df.iterrows():\n",
    "    st_lat = row['LATITUDE']\n",
    "    st_lon = row['LONGITUDE']\n",
    "    dist_all = haversine_distance(st_lat, st_lon, grid_lat_flat, grid_lon_flat)\n",
    "    \n",
    "    # Sort by distance and take the indices of the 8 nearest\n",
    "    sorted_idx = np.argsort(dist_all)\n",
    "    neighbor_idx = sorted_idx[:8]\n",
    "    \n",
    "    station_neighbors[i] = neighbor_idx\n",
    "\n",
    "print(\"Neighbor precomputation complete.\")\n",
    "\n",
    "###############################################################################\n",
    "# 9. LOCAL WEIGHTED REGRESSION FUNCTION\n",
    "###############################################################################\n",
    "def local_weighted_regression(\n",
    "    station_lat, station_lon, station_elev,\n",
    "    neighbor_indices,\n",
    "    grid_lat, grid_lon, grid_elev, grid_val\n",
    "):\n",
    "    \"\"\"\n",
    "    Weighted linear regression of grid_val ~ [1, lat, lon, elev].\n",
    "    Uses tricube weighting based on distances.\n",
    "    Clip negative precipitation to zero.\n",
    "    \"\"\"\n",
    "    if (neighbor_indices is None) or (len(neighbor_indices) < 1):\n",
    "        return np.nan\n",
    "\n",
    "    lat_n  = grid_lat[neighbor_indices]\n",
    "    lon_n  = grid_lon[neighbor_indices]\n",
    "    elev_n = grid_elev[neighbor_indices]\n",
    "    val_n  = grid_val[neighbor_indices]\n",
    "\n",
    "    dist_n = haversine_distance(station_lat, station_lon, lat_n, lon_n)\n",
    "    d_max  = dist_n.max()\n",
    "    w = tricube_weight(dist_n, d_max)\n",
    "    if np.all(w == 0):\n",
    "        return np.nan\n",
    "\n",
    "    # Build design matrix X: [1, lat, lon, elev]\n",
    "    X = np.column_stack([np.ones_like(lat_n), lat_n, lon_n, elev_n])\n",
    "\n",
    "    sqrt_w = np.sqrt(w)\n",
    "    X_w = X * sqrt_w[:, None]\n",
    "    y_w = val_n * sqrt_w\n",
    "\n",
    "    try:\n",
    "        beta, residuals, rank, s = np.linalg.lstsq(X_w, y_w, rcond=None)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.nan\n",
    "\n",
    "    # Predict at station location\n",
    "    X_station = np.array([1, station_lat, station_lon, station_elev])\n",
    "    pred = X_station @ beta\n",
    "\n",
    "    # Clip negative precipitation to zero\n",
    "    if pred < 0:\n",
    "        pred = 0.0\n",
    "\n",
    "    return pred\n",
    "\n",
    "###############################################################################\n",
    "# 10. DAILY LOOP: ERA5 -> STATIONS\n",
    "###############################################################################\n",
    "results = []\n",
    "era5_times = era5_ds['time'].values\n",
    "print(f\"Processing ERA5 days: {len(era5_times)}\")\n",
    "\n",
    "for t_index, t_val in enumerate(era5_times):\n",
    "    current_time = pd.to_datetime(t_val)\n",
    "    # shape => (lat, lon)\n",
    "    day_data_2d = era5_ds[era5_var_name].isel(time=t_index).values\n",
    "    grid_val_flat_day = day_data_2d.flatten()\n",
    "\n",
    "    obs_day = obs_ds[obs_var_name].isel(time=t_index).values\n",
    "\n",
    "    for i, row in stations_df.iterrows():\n",
    "        st_name    = row['NAME']\n",
    "        st_lat     = row['LATITUDE']\n",
    "        st_lon     = row['LONGITUDE']\n",
    "        st_elev    = row['Elevation'] if 'Elevation' in row else 0.0\n",
    "        netcdf_idx = row['netcdf_index']\n",
    "\n",
    "        st_obs = obs_day[netcdf_idx] if netcdf_idx < len(obs_day) else np.nan\n",
    "\n",
    "        neigh_idx = station_neighbors[i]\n",
    "        st_era5 = local_weighted_regression(\n",
    "            station_lat=st_lat,\n",
    "            station_lon=st_lon,\n",
    "            station_elev=st_elev,\n",
    "            neighbor_indices=neigh_idx,\n",
    "            grid_lat=grid_lat_flat,\n",
    "            grid_lon=grid_lon_flat,\n",
    "            grid_elev=grid_elev_flat,\n",
    "            grid_val=grid_val_flat_day\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'time': current_time,\n",
    "            'station_index': i,\n",
    "            'station_name': st_name,\n",
    "            'obs': st_obs,\n",
    "            'era5_lwr8_val': st_era5\n",
    "        })\n",
    "\n",
    "    if (t_index + 1) % 100 == 0:\n",
    "        print(f\"Processed day {t_index+1} / {len(era5_times)}\")\n",
    "\n",
    "###############################################################################\n",
    "# 11. BUILD A DATAFRAME & SAVE\n",
    "###############################################################################\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['time'] = pd.to_datetime(results_df['time'])\n",
    "print(\"Sample of daily results:\\n\", results_df.head())\n",
    "\n",
    "# CSV output with the 8-nearest LWR data\n",
    "daily_loop_csv = os.path.join(daily_loop_dir, \"era5_vs_stations_8Nearest_LWR_1991_2012.csv\")\n",
    "results_df.to_csv(daily_loop_csv, index=False)\n",
    "print(f\"Daily interpolation results saved to {daily_loop_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 12. (OPTIONAL) MAPPING\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"Creating a quick map of average ERA5 precipitation at each station (8-nearest LWR)...\")\n",
    "    glb  = gpd.read_file(shapefile_path).to_crs(target_crs)\n",
    "    lakes= gpd.read_file(lakes_shp).to_crs(target_crs)\n",
    "\n",
    "    station_mean = results_df.groupby('station_index')['era5_lwr8_val'].mean().reset_index()\n",
    "    merged = stations_df.copy()\n",
    "    merged['mean_era5_lwr8'] = station_mean['era5_lwr8_val']\n",
    "\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(merged['LONGITUDE'], merged['LATITUDE'])]\n",
    "    stations_gdf = gpd.GeoDataFrame(merged, geometry=geometry, crs=\"EPSG:4326\").to_crs(target_crs)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    glb.boundary.plot(ax=ax, edgecolor='black')\n",
    "    lakes.plot(ax=ax, color='blue', alpha=0.5)\n",
    "    stations_gdf.plot(column='mean_era5_lwr8', ax=ax, legend=True, cmap='viridis', markersize=50)\n",
    "    ax.set_title(\"Mean ERA5 Precip (8-Nearest LWR), 1991-2012\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Mapping step skipped due to error or missing shapefiles:\", str(e))\n",
    "\n",
    "###############################################################################\n",
    "# 13. METRICS & SAVING\n",
    "###############################################################################\n",
    "def remove_nan_pairs(obs, pred):\n",
    "    mask = ~np.isnan(obs) & ~np.isnan(pred)\n",
    "    return obs[mask], pred[mask]\n",
    "\n",
    "def mean_bias_error(obs, pred):\n",
    "    return np.mean(pred - obs)\n",
    "\n",
    "def root_mean_square_error(obs, pred):\n",
    "    return np.sqrt(np.mean((pred - obs)**2))\n",
    "\n",
    "def std_of_residuals(obs, pred):\n",
    "    return np.std(pred - obs, ddof=1)\n",
    "\n",
    "def pearson_correlation(obs, pred):\n",
    "    if len(obs) < 2:\n",
    "        return np.nan\n",
    "    return np.corrcoef(obs, pred)[0, 1]\n",
    "\n",
    "def index_of_agreement(obs, pred):\n",
    "    obs_mean = np.mean(obs)\n",
    "    numerator = np.sum((pred - obs)**2)\n",
    "    denominator = np.sum((np.abs(pred - obs_mean) + np.abs(obs - obs_mean))**2)\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "obs_all = results_df['obs'].values\n",
    "era5_all = results_df['era5_lwr8_val'].values\n",
    "obs_all, era5_all = remove_nan_pairs(obs_all, era5_all)\n",
    "\n",
    "if len(obs_all) > 0:\n",
    "    metrics_all = {\n",
    "        'MBE': mean_bias_error(obs_all, era5_all),\n",
    "        'RMSE': root_mean_square_error(obs_all, era5_all),\n",
    "        'STD': std_of_residuals(obs_all, era5_all),\n",
    "        'CC': pearson_correlation(obs_all, era5_all),\n",
    "        'Index_of_Agreement': index_of_agreement(obs_all, era5_all)\n",
    "    }\n",
    "    print(\"\\nOverall Metrics (All Stations, ERA5 8-Nearest LWR, 1991-2012):\")\n",
    "    for k, v in metrics_all.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid (obs, era5_lwr8_val) pairs found for overall metrics.\")\n",
    "\n",
    "# Station-level metrics\n",
    "station_groups = results_df.groupby('station_index')\n",
    "per_station_metrics = []\n",
    "\n",
    "for st_idx, grp in station_groups:\n",
    "    obs_st = grp['obs'].values\n",
    "    era5_st = grp['era5_lwr8_val'].values\n",
    "    obs_st, era5_st = remove_nan_pairs(obs_st, era5_st)\n",
    "    if len(obs_st) == 0:\n",
    "        per_station_metrics.append({\n",
    "            'station_index': st_idx,\n",
    "            'station_name': grp['station_name'].iloc[0],\n",
    "            'MBE': np.nan,\n",
    "            'RMSE': np.nan,\n",
    "            'STD': np.nan,\n",
    "            'CC': np.nan,\n",
    "            'Index_of_Agreement': np.nan\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    row_dict = {\n",
    "        'station_index': st_idx,\n",
    "        'station_name': grp['station_name'].iloc[0],\n",
    "        'MBE':  mean_bias_error(obs_st, era5_st),\n",
    "        'RMSE': root_mean_square_error(obs_st, era5_st),\n",
    "        'STD':  std_of_residuals(obs_st, era5_st),\n",
    "        'CC':   pearson_correlation(obs_st, era5_st),\n",
    "        'Index_of_Agreement': index_of_agreement(obs_st, era5_st)\n",
    "    }\n",
    "    per_station_metrics.append(row_dict)\n",
    "\n",
    "metrics_df = pd.DataFrame(per_station_metrics)\n",
    "print(\"\\nSample of per-station metrics:\")\n",
    "print(metrics_df.head())\n",
    "\n",
    "metrics_csv = os.path.join(metrics_dir, \"station_metrics_8Nearest_LWR_ERA5_1991_2012.csv\")\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "print(f\"Station metrics saved to {metrics_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 14. (OPTIONAL) SAVE FULL DAILY RESULTS AS NETCDF\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nSaving daily station results to NetCDF file in daily_loop folder...\")\n",
    "    netcdf_file = os.path.join(daily_loop_dir, \"era5_vs_stations_8Nearest_LWR_1991_2012.nc\")\n",
    "\n",
    "    # 1) Pivot so 'time' is the row index, 'station_index' are columns\n",
    "    pivot_era5 = results_df.pivot(index='time', columns='station_index', values='era5_lwr8_val')\n",
    "    pivot_obs  = results_df.pivot(index='time', columns='station_index', values='obs')\n",
    "\n",
    "    # 2) Ensure the pivoted index is recognized as real datetime\n",
    "    pivot_era5.index = pd.to_datetime(pivot_era5.index, errors='coerce')\n",
    "    pivot_obs.index  = pd.to_datetime(pivot_obs.index,  errors='coerce')\n",
    "\n",
    "    # 3) Convert each pivoted DataFrame into an xarray Dataset\n",
    "    ds_era5 = pivot_era5.to_xarray()\n",
    "    ds_obs  = pivot_obs.to_xarray()\n",
    "\n",
    "    if 'index' in ds_era5.dims:\n",
    "        ds_era5 = ds_era5.rename({'index': 'time'})\n",
    "    if 'columns' in ds_era5.dims:\n",
    "        ds_era5 = ds_era5.rename({'columns': 'station_index'})\n",
    "\n",
    "    if 'index' in ds_obs.dims:\n",
    "        ds_obs = ds_obs.rename({'index': 'time'})\n",
    "    if 'columns' in ds_obs.dims:\n",
    "        ds_obs = ds_obs.rename({'columns': 'station_index'})\n",
    "\n",
    "    da_era5 = ds_era5.to_array(name='era5_lwr8_val').squeeze()\n",
    "    da_obs  = ds_obs.to_array(name='obs').squeeze()\n",
    "\n",
    "    ds_out = xr.Dataset({\n",
    "        'era5_lwr8_val': da_era5,\n",
    "        'obs':           da_obs\n",
    "    })\n",
    "\n",
    "    ds_out.to_netcdf(netcdf_file)\n",
    "    print(f\"NetCDF saved to: {netcdf_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error saving NetCDF:\", e)\n",
    "\n",
    "print(\"\\nDone. The 8-nearest LWR interpolation for ERA5 dataset is complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f510295e-7252-442e-ac58-c6ea349ba4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ERA5 Temperature\n",
    "# This script performs an 8-nearest-grid LWR interpolation using ERA5 data for tmin-max\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATIONS\n",
    "###############################################################################\n",
    "# Input files\n",
    "station_file   = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\tmin_tmax_data.nc'\n",
    "era5_file      = r'D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Temperature\\Temperature ERA5\\tmin-tmax\\ERA5_GLB_daily_tmin_tmax_1991-2013_with_Elevation.nc'\n",
    "physical_file  = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation_Temperature.csv'\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r'D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp'\n",
    "target_crs     = \"ESRI:102008\"\n",
    "\n",
    "# Output directories\n",
    "# Adjust to match your desired folders\n",
    "# This example uses a path near the ERA5 file location\n",
    "\n",
    "daily_loop_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Temperature\\Temperature ERA5\\tmin-tmax\\daily_loop\"\n",
    "metrics_dir    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\ERA5_GLB_Temperature\\Temperature ERA5\\tmin-tmax\\metrics\"\n",
    "\n",
    "os.makedirs(daily_loop_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir,    exist_ok=True)\n",
    "\n",
    "# Variable names\n",
    "VAR_LIST = ['tmin', 'tmax']       # <-- new\n",
    "\n",
    "# Time range: 1991–2012\n",
    "start_date = '1991-01-01'\n",
    "end_date   = '1991-12-31'\n",
    "\n",
    "###############################################################################\n",
    "# 2. DISTANCE & WEIGHT FUNCTIONS\n",
    "###############################################################################\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = (np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2)\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c\n",
    "\n",
    "def tricube_weight(distances, d_max):\n",
    "    if d_max == 0:\n",
    "        return np.ones_like(distances)\n",
    "    ratio = distances / d_max\n",
    "    w = (1 - ratio**3)**3\n",
    "    w[distances > d_max] = 0.0\n",
    "    return w\n",
    "\n",
    "###############################################################################\n",
    "# 3. OPTIONAL: FORCE ERA5 TIME\n",
    "###############################################################################\n",
    "def force_era5_time(ds):\n",
    "    \"\"\"\n",
    "    If needed, convert time coordinate to datetime.\n",
    "    If ds['time'] is already a standard datetime, skip or simplify.\n",
    "    \"\"\"\n",
    "    if 'time' not in ds.coords and 'time' in ds.dims:\n",
    "        ds = ds.assign_coords(time=ds['time'])\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "    return ds\n",
    "\n",
    "###############################################################################\n",
    "# 4. LOAD DATASETS\n",
    "###############################################################################\n",
    "print(\"Loading station observations (NetCDF) ...\")\n",
    "obs_ds = xr.open_dataset(station_file)\n",
    "\n",
    "print(\"Loading ERA5 reanalysis (NetCDF) ...\")\n",
    "era5_ds = xr.open_dataset(era5_file)\n",
    "\n",
    "# Fix station time coords if needed\n",
    "if 'time' in obs_ds.coords:\n",
    "    obs_ds['time'] = pd.to_datetime(obs_ds['time'].values)\n",
    "\n",
    "# Force ERA5 time if needed\n",
    "era5_ds = force_era5_time(era5_ds)\n",
    "\n",
    "# Subset to 1991–2012\n",
    "obs_ds   = obs_ds.sel(time=slice(start_date, end_date))\n",
    "era5_ds = era5_ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "print(\"After subsetting:\")\n",
    "print(f\"obs_ds time steps = {obs_ds.sizes['time']}\")\n",
    "print(f\"era5_ds time steps = {era5_ds.sizes['time']}\")\n",
    "\n",
    "###############################################################################\n",
    "# 5. IF dims are (time, lon, lat), transpose to (time, lat, lon)\n",
    "###############################################################################\n",
    "for v in VAR_LIST:\n",
    "    actual_dims = era5_ds[v].dims\n",
    "    print(f\"Current dims for ERA5 {v}:\", actual_dims)\n",
    "    if actual_dims == (\"time\", \"lon\", \"lat\"):\n",
    "        print(f\"Transposing {v} from (time,lon,lat) -> (time,lat,lon).\")\n",
    "        era5_ds[v] = era5_ds[v].transpose(\"time\", \"lat\", \"lon\")\n",
    "        print(\"New dims:\", era5_ds[v].dims)\n",
    "    else:\n",
    "        print(f\"No transpose needed for {v}.\")\n",
    "\n",
    "###############################################################################\n",
    "# 6. EXTRACT ERA5 GRID\n",
    "###############################################################################\n",
    "lats = era5_ds['lat'].values\n",
    "lons = era5_ds['lon'].values\n",
    "lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "\n",
    "if 'elevation' in era5_ds:\n",
    "    grid_elev = era5_ds['elevation'].values\n",
    "else:\n",
    "    warnings.warn(\"No 'elevation' found in ERA5 dataset => set elev=0.\")\n",
    "    grid_elev = np.zeros_like(lat2d)\n",
    "\n",
    "grid_lat_flat  = lat2d.flatten()\n",
    "grid_lon_flat  = lon2d.flatten()\n",
    "grid_elev_flat = grid_elev.flatten()\n",
    "\n",
    "###############################################################################\n",
    "# 7. LOAD STATIONS & MATCH\n",
    "###############################################################################\n",
    "print(\"Loading station metadata (CSV) ...\")\n",
    "stations_df = pd.read_csv(physical_file).dropna(axis=1, how='all')\n",
    "print(stations_df.head())\n",
    "\n",
    "# If station_file has dimension station, match by name\n",
    "station_names_nc = obs_ds['station'].values\n",
    "station_index_map = {name: i for i, name in enumerate(station_names_nc)}\n",
    "\n",
    "stations_df['netcdf_index'] = stations_df['NAME'].map(station_index_map)\n",
    "stations_df = stations_df.dropna(subset=['netcdf_index']).reset_index(drop=True)\n",
    "stations_df['netcdf_index'] = stations_df['netcdf_index'].astype(int)\n",
    "print(\"Total matched stations:\", len(stations_df))\n",
    "\n",
    "###############################################################################\n",
    "# 8. PRECOMPUTE THE 8 NEAREST GRIDS FOR EACH STATION\n",
    "###############################################################################\n",
    "station_neighbors = {}\n",
    "print(\"Precomputing the 8 nearest ERA5 grids for each station ...\")\n",
    "\n",
    "for i, row in stations_df.iterrows():\n",
    "    st_lat = row['LATITUDE']\n",
    "    st_lon = row['LONGITUDE']\n",
    "    dist_all = haversine_distance(st_lat, st_lon, grid_lat_flat, grid_lon_flat)\n",
    "    \n",
    "    # Sort by distance and take the indices of the 8 nearest\n",
    "    sorted_idx = np.argsort(dist_all)\n",
    "    neighbor_idx = sorted_idx[:8]\n",
    "    \n",
    "    station_neighbors[i] = neighbor_idx\n",
    "\n",
    "print(\"Neighbor precomputation complete.\")\n",
    "\n",
    "###############################################################################\n",
    "# 9. LOCAL WEIGHTED REGRESSION FUNCTION  (updated for temperature)\n",
    "###############################################################################\n",
    "def local_weighted_regression(\n",
    "    station_lat, station_lon, station_elev,\n",
    "    neighbor_indices,\n",
    "    grid_lat, grid_lon, grid_elev, grid_val\n",
    "):\n",
    "    \"\"\"\n",
    "    Local Weighted Regression (LWR) using up to 8 nearest ERA5 grid points.\n",
    "\n",
    "    Regression model:\n",
    "        grid_val  ~  β0 + β1*lat + β2*lon + β3*elev\n",
    "\n",
    "    • Weights are tricube based on great-circle distance.\n",
    "    • **No clipping** is applied because negative temperatures are valid.\n",
    "    Returns NaN if regression cannot be solved.\n",
    "    \"\"\"\n",
    "    if (neighbor_indices is None) or (len(neighbor_indices) == 0):\n",
    "        return np.nan\n",
    "\n",
    "    # Neighbour data\n",
    "    lat_n  = grid_lat[neighbor_indices]\n",
    "    lon_n  = grid_lon[neighbor_indices]\n",
    "    elev_n = grid_elev[neighbor_indices]\n",
    "    val_n  = grid_val[neighbor_indices]\n",
    "\n",
    "    # Tricube weights\n",
    "    dist_n = haversine_distance(station_lat, station_lon, lat_n, lon_n)\n",
    "    d_max  = dist_n.max()\n",
    "    w      = tricube_weight(dist_n, d_max)\n",
    "    if np.all(w == 0):\n",
    "        return np.nan\n",
    "\n",
    "    # Design matrix with intercept\n",
    "    X = np.column_stack([np.ones_like(lat_n), lat_n, lon_n, elev_n])\n",
    "\n",
    "    # Weighted least squares\n",
    "    sqrt_w = np.sqrt(w)\n",
    "    X_w = X * sqrt_w[:, None]\n",
    "    y_w = val_n * sqrt_w\n",
    "    try:\n",
    "        beta, *_ = np.linalg.lstsq(X_w, y_w, rcond=None)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.nan\n",
    "\n",
    "    # Predict at station location (no clipping)\n",
    "    pred = np.array([1, station_lat, station_lon, station_elev]) @ beta\n",
    "    return pred\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 10. DAILY LOOP: ERA5 -> STATIONS\n",
    "###############################################################################\n",
    "results = []\n",
    "era5_times = era5_ds['time'].values\n",
    "print(f\"Processing ERA5 days: {len(era5_times)}\")\n",
    "\n",
    "for t_index, t_val in enumerate(era5_times):\n",
    "    current_time = pd.to_datetime(t_val)\n",
    "\n",
    "    for var in VAR_LIST:                      # ← outer loop over tmin / tmax\n",
    "        # ERA5 grid for this day & variable\n",
    "        day_data_2d       = era5_ds[var].isel(time=t_index).values\n",
    "        grid_val_flat_day = day_data_2d.flatten()\n",
    "\n",
    "        # Observed station values for same day & variable\n",
    "        obs_day = obs_ds[var].isel(time=t_index).values\n",
    "\n",
    "        # Loop stations\n",
    "        for i, row in stations_df.iterrows():\n",
    "            st_name = row['NAME']\n",
    "            st_lat  = row['LATITUDE']\n",
    "            st_lon  = row['LONGITUDE']\n",
    "            st_elev = row['Elevation'] if 'Elevation' in row else 0.0\n",
    "            netcdf_idx = row['netcdf_index']\n",
    "\n",
    "            st_obs = obs_day[netcdf_idx] if netcdf_idx < len(obs_day) else np.nan\n",
    "\n",
    "            neigh_idx = station_neighbors[i]\n",
    "            st_era5 = local_weighted_regression(\n",
    "                station_lat=st_lat, station_lon=st_lon, station_elev=st_elev,\n",
    "                neighbor_indices=neigh_idx,\n",
    "                grid_lat=grid_lat_flat, grid_lon=grid_lon_flat,\n",
    "                grid_elev=grid_elev_flat, grid_val=grid_val_flat_day\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                'time': current_time,\n",
    "                'var':  var,                   # identify tmin / tmax\n",
    "                'station_index': i,\n",
    "                'station_name':  st_name,\n",
    "                'obs':  st_obs,\n",
    "                'era5_lwr8_val': st_era5\n",
    "            })\n",
    "\n",
    "    if (t_index + 1) % 100 == 0:\n",
    "        print(f\"Processed day {t_index+1} / {len(era5_times)}\")\n",
    "\n",
    "###############################################################################\n",
    "# 11. BUILD A DATAFRAME & SAVE\n",
    "###############################################################################\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['time'] = pd.to_datetime(results_df['time'])\n",
    "print(\"Sample of daily results:\\n\", results_df.head())\n",
    "\n",
    "daily_loop_csv = os.path.join(\n",
    "    daily_loop_dir,\n",
    "    \"era5_vs_stations_8Nearest_LWR_1991_2012_tmin_tmax.csv\"\n",
    ")\n",
    "results_df.to_csv(daily_loop_csv, index=False)\n",
    "print(f\"Daily interpolation results saved to {daily_loop_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 12. (OPTIONAL) MAPPING\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"Creating a quick map of average ERA5 temperature at each station (8-nearest LWR)...\")\n",
    "    glb  = gpd.read_file(shapefile_path).to_crs(target_crs)\n",
    "    lakes= gpd.read_file(lakes_shp).to_crs(target_crs)\n",
    "\n",
    "    station_mean = results_df.groupby('station_index')['era5_lwr8_val'].mean().reset_index()\n",
    "    merged = stations_df.copy()\n",
    "    merged['mean_era5_lwr8'] = station_mean['era5_lwr8_val']\n",
    "\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(merged['LONGITUDE'], merged['LATITUDE'])]\n",
    "    stations_gdf = gpd.GeoDataFrame(merged, geometry=geometry, crs=\"EPSG:4326\").to_crs(target_crs)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    glb.boundary.plot(ax=ax, edgecolor='black')\n",
    "    lakes.plot(ax=ax, color='blue', alpha=0.5)\n",
    "    stations_gdf.plot(column='mean_era5_lwr8', ax=ax, legend=True, cmap='viridis', markersize=50)\n",
    "    ax.set_title(\"Mean ERA5 temperature (8-Nearest LWR), 1991-2012\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Mapping step skipped due to error or missing shapefiles:\", str(e))\n",
    "\n",
    "###############################################################################\n",
    "# 13. METRICS & SAVING\n",
    "###############################################################################\n",
    "def remove_nan_pairs(obs, pred):\n",
    "    m = ~np.isnan(obs) & ~np.isnan(pred)\n",
    "    return obs[m], pred[m]\n",
    "\n",
    "# Overall metrics per variable\n",
    "for var, sub in results_df.groupby('var'):\n",
    "    obs_all, era5_all = remove_nan_pairs(sub['obs'].values, sub['era5_lwr8_val'].values)\n",
    "    if len(obs_all) == 0:\n",
    "        print(f\"\\nNo valid pairs for {var}.\")\n",
    "        continue\n",
    "    metrics_all = {\n",
    "        'MBE':  np.mean(era5_all - obs_all),\n",
    "        'RMSE': np.sqrt(np.mean((era5_all - obs_all)**2)),\n",
    "        'STD':  np.std(era5_all - obs_all, ddof=1),\n",
    "        'CC':   np.corrcoef(obs_all, era5_all)[0,1],\n",
    "        'Index_of_Agreement':\n",
    "            1 - np.sum((era5_all - obs_all)**2) /\n",
    "                np.sum((np.abs(era5_all - obs_all.mean()) +\n",
    "                        np.abs(obs_all - obs_all.mean()))**2)\n",
    "    }\n",
    "    print(f\"\\nOverall metrics ({var}, ERA5 8-Nearest LWR, 1991-2012):\")\n",
    "    for k, v in metrics_all.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "# Station-level metrics, still per variable\n",
    "station_metrics = []\n",
    "for (var, st_idx), grp in results_df.groupby(['var','station_index']):\n",
    "    obs_st, era5_st = remove_nan_pairs(grp['obs'].values, grp['era5_lwr8_val'].values)\n",
    "    if len(obs_st)==0:\n",
    "        station_metrics.append({'var':var,'station_index':st_idx,\n",
    "                                'station_name':grp['station_name'].iloc[0],\n",
    "                                'MBE':np.nan,'RMSE':np.nan,'STD':np.nan,\n",
    "                                'CC':np.nan,'Index_of_Agreement':np.nan})\n",
    "        continue\n",
    "    station_metrics.append({\n",
    "        'var': var,\n",
    "        'station_index': st_idx,\n",
    "        'station_name':  grp['station_name'].iloc[0],\n",
    "        'MBE':  np.mean(era5_st - obs_st),\n",
    "        'RMSE': np.sqrt(np.mean((era5_st - obs_st)**2)),\n",
    "        'STD':  np.std(era5_st - obs_st, ddof=1),\n",
    "        'CC':   np.corrcoef(obs_st, era5_st)[0,1],\n",
    "        'Index_of_Agreement':\n",
    "            1 - np.sum((era5_st - obs_st)**2) /\n",
    "                np.sum((np.abs(era5_st - obs_st.mean()) +\n",
    "                        np.abs(obs_st - obs_st.mean()))**2)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(station_metrics)\n",
    "metrics_csv = os.path.join(metrics_dir,\n",
    "    \"station_metrics_8Nearest_LWR_ERA5_1991_2012_tmin_tmax.csv\")\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "print(f\"Station metrics saved to {metrics_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 14. (OPTIONAL) SAVE FULL DAILY RESULTS AS NETCDF\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nSaving daily station results to NetCDF …\")\n",
    "\n",
    "    netcdf_file = os.path.join(\n",
    "        daily_loop_dir,\n",
    "        \"era5_vs_stations_8Nearest_LWR_1991_2012_tmin_tmax.nc\"\n",
    "    )\n",
    "\n",
    "    # ---- hold one DataArray per variable/dataset ----\n",
    "    ds_vars = {}\n",
    "\n",
    "    for var in VAR_LIST:\n",
    "        # Pivot to (time × station_index) tables\n",
    "        pivot_era5 = (\n",
    "            results_df[results_df[\"var\"] == var]\n",
    "            .pivot(index=\"time\", columns=\"station_index\", values=\"era5_lwr8_val\")\n",
    "            .sort_index()\n",
    "        )\n",
    "        pivot_obs = (\n",
    "            results_df[results_df[\"var\"] == var]\n",
    "            .pivot(index=\"time\", columns=\"station_index\", values=\"obs\")\n",
    "            .sort_index()\n",
    "        )\n",
    "\n",
    "        # Make sure both have identical columns (stations)\n",
    "        common_cols = pivot_era5.columns.intersection(pivot_obs.columns)\n",
    "        pivot_era5 = pivot_era5[common_cols]\n",
    "        pivot_obs  = pivot_obs [common_cols]\n",
    "\n",
    "        # Build DataArrays\n",
    "        da_era5 = xr.DataArray(\n",
    "            data   = pivot_era5.values,\n",
    "            dims   = (\"time\", \"station_index\"),\n",
    "            coords = {\"time\": pivot_era5.index,\n",
    "                      \"station_index\": common_cols},\n",
    "            name   = f\"era5_lwr8_{var}\"\n",
    "        )\n",
    "        da_obs  = xr.DataArray(\n",
    "            data   = pivot_obs.values,\n",
    "            dims   = (\"time\", \"station_index\"),\n",
    "            coords = {\"time\": pivot_obs.index,\n",
    "                      \"station_index\": common_cols},\n",
    "            name   = f\"obs_{var}\"\n",
    "        )\n",
    "\n",
    "        ds_vars[da_era5.name] = da_era5\n",
    "        ds_vars[da_obs.name]  = da_obs\n",
    "\n",
    "    # Assemble and save\n",
    "    ds_out = xr.Dataset(ds_vars)\n",
    "    ds_out.to_netcdf(netcdf_file)\n",
    "    print(f\"NetCDF saved to: {netcdf_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error saving NetCDF:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab7f2d-1f35-4e86-988b-80ed0533e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERRA-2\n",
    "# This script performs an 12-nearest-grid LWR interpolation using MERRA2 data.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATIONS\n",
    "###############################################################################\n",
    "# Input files\n",
    "station_file   = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\prcp_data.nc'\n",
    "merra2_file      = r'D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\MERRA2_GLB_Precipitation\\MERRA2_GLB_prcp_1991_2013_with_Elevation.nc'\n",
    "physical_file  = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv'\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r'D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp'\n",
    "target_crs     = \"ESRI:102008\"\n",
    "\n",
    "# Output directories\n",
    "# Adjust to match your desired folders\n",
    "# This example uses a path near the ERA5 file location\n",
    "\n",
    "daily_loop_dir = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\MERRA2_GLB_Precipitation\\daily_loop\"\n",
    "metrics_dir    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\MERRA2_GLB_Precipitation\\metrics\"\n",
    "\n",
    "os.makedirs(daily_loop_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir,    exist_ok=True)\n",
    "\n",
    "# Variable names\n",
    "obs_var_name  = 'prcp'  # Station daily precipitation var\n",
    "merra2_var_name = 'prcp'  # ERA5 daily precipitation var (change if needed)\n",
    "\n",
    "# Time range: 1991–2012\n",
    "start_date = '1991-01-01'\n",
    "end_date   = '2012-12-31'\n",
    "\n",
    "###############################################################################\n",
    "# 2. DISTANCE & WEIGHT FUNCTIONS\n",
    "###############################################################################\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = (np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2)\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c\n",
    "\n",
    "def tricube_weight(distances, d_max):\n",
    "    if d_max == 0:\n",
    "        return np.ones_like(distances)\n",
    "    ratio = distances / d_max\n",
    "    w = (1 - ratio**3)**3\n",
    "    w[distances > d_max] = 0.0\n",
    "    return w\n",
    "\n",
    "###############################################################################\n",
    "# 3. OPTIONAL: FORCE ERA5 TIME\n",
    "###############################################################################\n",
    "def force_merra2_time(ds):\n",
    "    \"\"\"\n",
    "    If needed, convert time coordinate to datetime.\n",
    "    If ds['time'] is already a standard datetime, skip or simplify.\n",
    "    \"\"\"\n",
    "    if 'time' not in ds.coords and 'time' in ds.dims:\n",
    "        ds = ds.assign_coords(time=ds['time'])\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "    return ds\n",
    "\n",
    "###############################################################################\n",
    "# 4. LOAD DATASETS\n",
    "###############################################################################\n",
    "print(\"Loading station observations (NetCDF) ...\")\n",
    "obs_ds = xr.open_dataset(station_file)\n",
    "\n",
    "print(\"Loading merra2 reanalysis (NetCDF) ...\")\n",
    "era5_ds = xr.open_dataset(merra2_file)\n",
    "\n",
    "# Fix station time coords if needed\n",
    "if 'time' in obs_ds.coords:\n",
    "    obs_ds['time'] = pd.to_datetime(obs_ds['time'].values)\n",
    "\n",
    "# Force ERA5 time if needed\n",
    "merra2_ds = force_merra2_time(merra2_ds)\n",
    "\n",
    "# Subset to 1991–2012\n",
    "obs_ds   = obs_ds.sel(time=slice(start_date, end_date))\n",
    "merra2_ds = merra2_ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "print(\"After subsetting:\")\n",
    "print(f\"obs_ds time steps = {obs_ds.sizes['time']}\")\n",
    "print(f\"merra2_ds time steps = {merra2_ds.sizes['time']}\")\n",
    "\n",
    "###############################################################################\n",
    "# 5. IF dims are (time, lon, lat), transpose to (time, lat, lon)\n",
    "###############################################################################\n",
    "actual_dims = merra2_ds[merra2_var_name].dims\n",
    "print(\"Current dims for merra2 prcp:\", actual_dims)\n",
    "if actual_dims == (\"time\",\"lon\",\"lat\"):\n",
    "    print(\"Transposing prcp from (time,lon,lat) -> (time,lat,lon).\")\n",
    "    merra2_ds[merra2_var_name] = merra2_ds[merra2_var_name].transpose(\"time\",\"lat\",\"lon\")\n",
    "    print(\"New dims:\", merra2_ds[merra2_var_name].dims)\n",
    "else:\n",
    "    print(\"No transpose needed or already (time,lat,lon).\")\n",
    "\n",
    "###############################################################################\n",
    "# 6. EXTRACT ERA5 GRID\n",
    "###############################################################################\n",
    "lats = merra2_ds['lat'].values\n",
    "lons = merra2_ds['lon'].values\n",
    "lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "\n",
    "if 'elevation' in merra2_ds:\n",
    "    grid_elev = merra2_ds['elevation'].values\n",
    "else:\n",
    "    warnings.warn(\"No 'elevation' found in merra2 dataset => set elev=0.\")\n",
    "    grid_elev = np.zeros_like(lat2d)\n",
    "\n",
    "grid_lat_flat  = lat2d.flatten()\n",
    "grid_lon_flat  = lon2d.flatten()\n",
    "grid_elev_flat = grid_elev.flatten()\n",
    "\n",
    "###############################################################################\n",
    "# 7. LOAD STATIONS & MATCH\n",
    "###############################################################################\n",
    "print(\"Loading station metadata (CSV) ...\")\n",
    "stations_df = pd.read_csv(physical_file).dropna(axis=1, how='all')\n",
    "print(stations_df.head())\n",
    "\n",
    "# If station_file has dimension station, match by name\n",
    "station_names_nc = obs_ds['station'].values\n",
    "station_index_map = {name: i for i, name in enumerate(station_names_nc)}\n",
    "\n",
    "stations_df['netcdf_index'] = stations_df['NAME'].map(station_index_map)\n",
    "stations_df = stations_df.dropna(subset=['netcdf_index']).reset_index(drop=True)\n",
    "stations_df['netcdf_index'] = stations_df['netcdf_index'].astype(int)\n",
    "print(\"Total matched stations:\", len(stations_df))\n",
    "\n",
    "###############################################################################\n",
    "# 8. PRECOMPUTE THE 9 NEAREST GRIDS FOR EACH STATION\n",
    "###############################################################################\n",
    "station_neighbors = {}\n",
    "print(\"Precomputing the 12 nearest ERA5 grids for each station ...\")\n",
    "\n",
    "for i, row in stations_df.iterrows():\n",
    "    st_lat = row['LATITUDE']\n",
    "    st_lon = row['LONGITUDE']\n",
    "    dist_all = haversine_distance(st_lat, st_lon, grid_lat_flat, grid_lon_flat)\n",
    "    \n",
    "    # Sort by distance and take the indices of the 12 nearest\n",
    "    sorted_idx = np.argsort(dist_all)\n",
    "    neighbor_idx = sorted_idx[:12]\n",
    "    \n",
    "    station_neighbors[i] = neighbor_idx\n",
    "\n",
    "print(\"Neighbor precomputation complete.\")\n",
    "\n",
    "###############################################################################\n",
    "# 9. LOCAL WEIGHTED REGRESSION FUNCTION\n",
    "###############################################################################\n",
    "def local_weighted_regression(\n",
    "    station_lat, station_lon, station_elev,\n",
    "    neighbor_indices,\n",
    "    grid_lat, grid_lon, grid_elev, grid_val\n",
    "):\n",
    "    \"\"\"\n",
    "    Weighted linear regression of grid_val ~ [1, lat, lon, elev].\n",
    "    Uses tricube weighting based on distances.\n",
    "    Clip negative precipitation to zero.\n",
    "    \"\"\"\n",
    "    if (neighbor_indices is None) or (len(neighbor_indices) < 1):\n",
    "        return np.nan\n",
    "\n",
    "    lat_n  = grid_lat[neighbor_indices]\n",
    "    lon_n  = grid_lon[neighbor_indices]\n",
    "    elev_n = grid_elev[neighbor_indices]\n",
    "    val_n  = grid_val[neighbor_indices]\n",
    "\n",
    "    dist_n = haversine_distance(station_lat, station_lon, lat_n, lon_n)\n",
    "    d_max  = dist_n.max()\n",
    "    w = tricube_weight(dist_n, d_max)\n",
    "    if np.all(w == 0):\n",
    "        return np.nan\n",
    "\n",
    "    # Build design matrix X: [1, lat, lon, elev]\n",
    "    X = np.column_stack([np.ones_like(lat_n), lat_n, lon_n, elev_n])\n",
    "\n",
    "    sqrt_w = np.sqrt(w)\n",
    "    X_w = X * sqrt_w[:, None]\n",
    "    y_w = val_n * sqrt_w\n",
    "\n",
    "    try:\n",
    "        beta, residuals, rank, s = np.linalg.lstsq(X_w, y_w, rcond=None)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.nan\n",
    "\n",
    "    # Predict at station location\n",
    "    X_station = np.array([1, station_lat, station_lon, station_elev])\n",
    "    pred = X_station @ beta\n",
    "\n",
    "    # Clip negative precipitation to zero\n",
    "    if pred < 0:\n",
    "        pred = 0.0\n",
    "\n",
    "    return pred\n",
    "\n",
    "###############################################################################\n",
    "# 10. DAILY LOOP: merra2 -> STATIONS\n",
    "###############################################################################\n",
    "results = []\n",
    "merra2_times = merra2_ds['time'].values\n",
    "print(f\"Processing merra2 days: {len(merra2_times)}\")\n",
    "\n",
    "for t_index, t_val in enumerate(merra2_times):\n",
    "    current_time = pd.to_datetime(t_val)\n",
    "    # shape => (lat, lon)\n",
    "    day_data_2d = merra2_ds[merra2_var_name].isel(time=t_index).values\n",
    "    grid_val_flat_day = day_data_2d.flatten()\n",
    "\n",
    "    obs_day = obs_ds[obs_var_name].isel(time=t_index).values\n",
    "\n",
    "    for i, row in stations_df.iterrows():\n",
    "        st_name    = row['NAME']\n",
    "        st_lat     = row['LATITUDE']\n",
    "        st_lon     = row['LONGITUDE']\n",
    "        st_elev    = row['Elevation'] if 'Elevation' in row else 0.0\n",
    "        netcdf_idx = row['netcdf_index']\n",
    "\n",
    "        st_obs = obs_day[netcdf_idx] if netcdf_idx < len(obs_day) else np.nan\n",
    "\n",
    "        neigh_idx = station_neighbors[i]\n",
    "        st_merra2 = local_weighted_regression(\n",
    "            station_lat=st_lat,\n",
    "            station_lon=st_lon,\n",
    "            station_elev=st_elev,\n",
    "            neighbor_indices=neigh_idx,\n",
    "            grid_lat=grid_lat_flat,\n",
    "            grid_lon=grid_lon_flat,\n",
    "            grid_elev=grid_elev_flat,\n",
    "            grid_val=grid_val_flat_day\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'time': current_time,\n",
    "            'station_index': i,\n",
    "            'station_name': st_name,\n",
    "            'obs': st_obs,\n",
    "            'merra2_lwr12_val': st_merra2\n",
    "        })\n",
    "\n",
    "    if (t_index + 1) % 100 == 0:\n",
    "        print(f\"Processed day {t_index+1} / {len(merra2_times)}\")\n",
    "\n",
    "###############################################################################\n",
    "# 11. BUILD A DATAFRAME & SAVE\n",
    "###############################################################################\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['time'] = pd.to_datetime(results_df['time'])\n",
    "print(\"Sample of daily results:\\n\", results_df.head())\n",
    "\n",
    "# CSV output with the 8-nearest LWR data\n",
    "daily_loop_csv = os.path.join(daily_loop_dir, \"merra2_vs_stations_12Nearest_LWR_1991_2012.csv\")\n",
    "results_df.to_csv(daily_loop_csv, index=False)\n",
    "print(f\"Daily interpolation results saved to {daily_loop_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 12. (OPTIONAL) MAPPING\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"Creating a quick map of average merra2 precipitation at each station (12-nearest LWR)...\")\n",
    "    glb  = gpd.read_file(shapefile_path).to_crs(target_crs)\n",
    "    lakes= gpd.read_file(lakes_shp).to_crs(target_crs)\n",
    "\n",
    "    station_mean = results_df.groupby('station_index')['merra2_lwr12_val'].mean().reset_index()\n",
    "    merged = stations_df.copy()\n",
    "    merged['mean_merra2_lwr12'] = station_mean['merra2_lwr12_val']\n",
    "\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(merged['LONGITUDE'], merged['LATITUDE'])]\n",
    "    stations_gdf = gpd.GeoDataFrame(merged, geometry=geometry, crs=\"EPSG:4326\").to_crs(target_crs)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    glb.boundary.plot(ax=ax, edgecolor='black')\n",
    "    lakes.plot(ax=ax, color='blue', alpha=0.5)\n",
    "    stations_gdf.plot(column='mean_merra2_lwr12', ax=ax, legend=True, cmap='viridis', markersize=50)\n",
    "    ax.set_title(\"Mean merra2 Precip (12-Nearest LWR), 1991-2012\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Mapping step skipped due to error or missing shapefiles:\", str(e))\n",
    "\n",
    "###############################################################################\n",
    "# 13. METRICS & SAVING\n",
    "###############################################################################\n",
    "def remove_nan_pairs(obs, pred):\n",
    "    mask = ~np.isnan(obs) & ~np.isnan(pred)\n",
    "    return obs[mask], pred[mask]\n",
    "\n",
    "def mean_bias_error(obs, pred):\n",
    "    return np.mean(pred - obs)\n",
    "\n",
    "def root_mean_square_error(obs, pred):\n",
    "    return np.sqrt(np.mean((pred - obs)**2))\n",
    "\n",
    "def std_of_residuals(obs, pred):\n",
    "    return np.std(pred - obs, ddof=1)\n",
    "\n",
    "def pearson_correlation(obs, pred):\n",
    "    if len(obs) < 2:\n",
    "        return np.nan\n",
    "    return np.corrcoef(obs, pred)[0, 1]\n",
    "\n",
    "def index_of_agreement(obs, pred):\n",
    "    obs_mean = np.mean(obs)\n",
    "    numerator = np.sum((pred - obs)**2)\n",
    "    denominator = np.sum((np.abs(pred - obs_mean) + np.abs(obs - obs_mean))**2)\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "obs_all = results_df['obs'].values\n",
    "merra2_all = results_df['merra2_lwr12_val'].values\n",
    "obs_all, merra2_all = remove_nan_pairs(obs_all, merra2_all)\n",
    "\n",
    "if len(obs_all) > 0:\n",
    "    metrics_all = {\n",
    "        'MBE': mean_bias_error(obs_all, merra2_all),\n",
    "        'RMSE': root_mean_square_error(obs_all, merra2_all),\n",
    "        'STD': std_of_residuals(obs_all, merra2_all),\n",
    "        'CC': pearson_correlation(obs_all, merra2_all),\n",
    "        'Index_of_Agreement': index_of_agreement(obs_all, merra2_all)\n",
    "    }\n",
    "    print(\"\\nOverall Metrics (All Stations, merra2 12-Nearest LWR, 1991-2012):\")\n",
    "    for k, v in metrics_all.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid (obs, merra2_lwr12_val) pairs found for overall metrics.\")\n",
    "\n",
    "# Station-level metrics\n",
    "station_groups = results_df.groupby('station_index')\n",
    "per_station_metrics = []\n",
    "\n",
    "for st_idx, grp in station_groups:\n",
    "    obs_st = grp['obs'].values\n",
    "    merra2_st = grp['merra2_lwr12_val'].values\n",
    "    obs_st, merra2_st = remove_nan_pairs(obs_st, merra2_st)\n",
    "    if len(obs_st) == 0:\n",
    "        per_station_metrics.append({\n",
    "            'station_index': st_idx,\n",
    "            'station_name': grp['station_name'].iloc[0],\n",
    "            'MBE': np.nan,\n",
    "            'RMSE': np.nan,\n",
    "            'STD': np.nan,\n",
    "            'CC': np.nan,\n",
    "            'Index_of_Agreement': np.nan\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    row_dict = {\n",
    "        'station_index': st_idx,\n",
    "        'station_name': grp['station_name'].iloc[0],\n",
    "        'MBE':  mean_bias_error(obs_st, merra2_st),\n",
    "        'RMSE': root_mean_square_error(obs_st, merra2_st),\n",
    "        'STD':  std_of_residuals(obs_st, merra2_st),\n",
    "        'CC':   pearson_correlation(obs_st, merra2_st),\n",
    "        'Index_of_Agreement': index_of_agreement(obs_st, merra2_st)\n",
    "    }\n",
    "    per_station_metrics.append(row_dict)\n",
    "\n",
    "metrics_df = pd.DataFrame(per_station_metrics)\n",
    "print(\"\\nSample of per-station metrics:\")\n",
    "print(metrics_df.head())\n",
    "\n",
    "metrics_csv = os.path.join(metrics_dir, \"station_metrics_12Nearest_LWR_merra2_1991_2012.csv\")\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "print(f\"Station metrics saved to {metrics_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 14. (OPTIONAL) SAVE FULL DAILY RESULTS AS NETCDF\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nSaving daily station results to NetCDF file in daily_loop folder...\")\n",
    "    netcdf_file = os.path.join(daily_loop_dir, \"merra2_vs_stations_12Nearest_LWR_1991_2012.nc\")\n",
    "\n",
    "    # 1) Pivot so 'time' is the row index, 'station_index' are columns\n",
    "    pivot_merra2 = results_df.pivot(index='time', columns='station_index', values='merra2_lwr12_val')\n",
    "    pivot_obs  = results_df.pivot(index='time', columns='station_index', values='obs')\n",
    "\n",
    "    # 2) Ensure the pivoted index is recognized as real datetime\n",
    "    pivot_merra2.index = pd.to_datetime(pivot_merra2.index, errors='coerce')\n",
    "    pivot_obs.index  = pd.to_datetime(pivot_obs.index,  errors='coerce')\n",
    "\n",
    "    # 3) Convert each pivoted DataFrame into an xarray Dataset\n",
    "    ds_merra2 = pivot_merra2.to_xarray()\n",
    "    ds_obs  = pivot_obs.to_xarray()\n",
    "\n",
    "    if 'index' in ds_merra2.dims:\n",
    "        ds_merra2 = ds_merra2.rename({'index': 'time'})\n",
    "    if 'columns' in ds_merra2.dims:\n",
    "        ds_merra2 = ds_merra2.rename({'columns': 'station_index'})\n",
    "\n",
    "    if 'index' in ds_obs.dims:\n",
    "        ds_obs = ds_obs.rename({'index': 'time'})\n",
    "    if 'columns' in ds_obs.dims:\n",
    "        ds_obs = ds_obs.rename({'columns': 'station_index'})\n",
    "\n",
    "    da_merra2 = ds_merra2.to_array(name='merra2_lwr12_val').squeeze()\n",
    "    da_obs  = ds_obs.to_array(name='obs').squeeze()\n",
    "\n",
    "    ds_out = xr.Dataset({\n",
    "        'merra2_lwr12_val': da_merra2,\n",
    "        'obs':           da_obs\n",
    "    })\n",
    "\n",
    "    ds_out.to_netcdf(netcdf_file)\n",
    "    print(f\"NetCDF saved to: {netcdf_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error saving NetCDF:\", e)\n",
    "\n",
    "print(\"\\nDone. The 12-nearest LWR interpolation for merra2 dataset is complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe73acd-1696-4436-8492-1b5698e274cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDRS v2.1\n",
    "# The approach of making the interpolated RDRS data at the station location with LWR 25 km\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATIONS\n",
    "###############################################################################\n",
    "station_file   = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\prcp_data.nc'\n",
    "rdrs_file      = r'D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\RDRS v2.1_GLB_Precipitation\\RDRS_v2.1_merged_prcp_1988_2015_with_Elevation.nc'\n",
    "physical_file  = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv'\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r'D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp'\n",
    "target_crs     = \"ESRI:102008\"\n",
    "\n",
    "# Paths for daily loop & metrics (same directory as ERA5 file)\n",
    "daily_loop_dir    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\RDRS v2.1_GLB_Precipitation\\daily_loop\"\n",
    "metrics_dir       = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\RDRS v2.1_GLB_Precipitation\\metrics\"\n",
    "\n",
    "os.makedirs(daily_loop_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir,    exist_ok=True)\n",
    "\n",
    "# Variable names\n",
    "obs_var_name = 'prcp'\n",
    "rdrs_var     = 'prcp'\n",
    "\n",
    "# Local Weighted Regression settings\n",
    "search_radius_km = 25\n",
    "min_points = 5\n",
    "\n",
    "# Time range: 1991–2012\n",
    "start_date = '1991-01-01'\n",
    "end_date   = '2012-12-31'\n",
    "\n",
    "###############################################################################\n",
    "# 2. DISTANCE & WEIGHT FUNCTIONS\n",
    "###############################################################################\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = (np.sin(dlat / 2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon / 2)**2)\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c\n",
    "\n",
    "def tricube_weight(distances, d_max):\n",
    "    \"\"\"\n",
    "    w_j = [1 - (dist_j / d_max)^3]^3  for dist_j <= d_max; else 0\n",
    "    \"\"\"\n",
    "    if d_max == 0:\n",
    "        return np.ones_like(distances)\n",
    "    ratio = distances / d_max\n",
    "    w = (1 - ratio**3)**3\n",
    "    w[distances > d_max] = 0.0\n",
    "    return w\n",
    "\n",
    "###############################################################################\n",
    "# 3. FORCE EMDNA TIME (IF NEEDED)\n",
    "###############################################################################\n",
    "def force_rdrs_time(ds):\n",
    "    if 'time' not in ds.coords and 'time' in ds.dims:\n",
    "        ds = ds.assign_coords(time=ds['time'])\n",
    "    if np.issubdtype(ds['time'].dtype, np.integer):\n",
    "        day0 = pd.to_datetime(\"1991-01-01\")\n",
    "        numeric_days = ds['time'].values\n",
    "        real_times = [day0 + pd.Timedelta(days=int(d)) for d in numeric_days]\n",
    "        ds = ds.assign_coords(time=(\"time\", real_times))\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "    return ds\n",
    "\n",
    "print(\"Loading station observations (NetCDF) ...\")\n",
    "obs_ds = xr.open_dataset(station_file)\n",
    "\n",
    "print(\"Loading RDRS reanalysis (NetCDF) ...\")\n",
    "rdrs_ds = xr.open_dataset(rdrs_file)\n",
    "\n",
    "# Fix time coords if needed\n",
    "if 'time' in obs_ds.coords:\n",
    "    obs_ds['time'] = pd.to_datetime(obs_ds['time'].values)\n",
    "rdrs_ds = force_rdrs_time(rdrs_ds)\n",
    "\n",
    "# Subset to 1991–2012\n",
    "obs_ds   = obs_ds.sel(time=slice(start_date, end_date))\n",
    "rdrs_ds  = rdrs_ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "print(\"After subsetting:\")\n",
    "print(f\"obs_ds time steps = {obs_ds.sizes['time']}\")\n",
    "print(f\"rdrs_ds time steps = {rdrs_ds.sizes['time']}\")\n",
    "\n",
    "# If dims are (time, lon, lat), transpose to (time, lat, lon)\n",
    "print(\"Checking dimension order of the RDRS variable ...\")\n",
    "actual_dims = rdrs_ds[rdrs_var].dims\n",
    "print(\"Current dims for prcp:\", actual_dims)\n",
    "if actual_dims == (\"time\", \"lon\", \"lat\"):\n",
    "    print(\"Transposing prcp from (time,lon,lat) -> (time,lat,lon).\")\n",
    "    rdrs_ds[rdrs_var] = rdrs_ds[rdrs_var].transpose(\"time\", \"lat\", \"lon\")\n",
    "    print(\"New dims:\", rdrs_ds[rdrs_var].dims)\n",
    "else:\n",
    "    print(\"No transpose needed or dims are already (time,lat,lon).\")\n",
    "\n",
    "lats = rdrs_ds['lat'].values\n",
    "lons = rdrs_ds['lon'].values\n",
    "lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "\n",
    "if 'elevation' in rdrs_ds:\n",
    "    grid_elev = rdrs_ds['elevation'].values\n",
    "else:\n",
    "    warnings.warn(\"No 'elevation' found in RDRS dataset => set elev=0.\")\n",
    "    grid_elev = np.zeros_like(lat2d)\n",
    "\n",
    "grid_lat_flat  = lat2d.flatten()\n",
    "grid_lon_flat  = lon2d.flatten()\n",
    "grid_elev_flat = grid_elev.flatten()\n",
    "\n",
    "###############################################################################\n",
    "# 4. LOAD STATIONS & MATCH\n",
    "###############################################################################\n",
    "print(\"Loading station metadata (CSV) ...\")\n",
    "stations_df = pd.read_csv(physical_file).dropna(axis=1, how='all')\n",
    "print(stations_df.head())\n",
    "\n",
    "print(\"Matching station names to NetCDF index ...\")\n",
    "station_names_nc = obs_ds['station'].values\n",
    "station_index_map = {name: i for i, name in enumerate(station_names_nc)}\n",
    "stations_df['netcdf_index'] = stations_df['NAME'].map(station_index_map)\n",
    "stations_df = stations_df.dropna(subset=['netcdf_index']).reset_index(drop=True)\n",
    "stations_df['netcdf_index'] = stations_df['netcdf_index'].astype(int)\n",
    "print(\"Total matched stations:\", len(stations_df))\n",
    "\n",
    "###############################################################################\n",
    "# 5. PRECOMPUTE NEIGHBORS FOR EACH STATION\n",
    "###############################################################################\n",
    "station_neighbors = {}\n",
    "print(f\"Precomputing neighbors within {search_radius_km} km...\")\n",
    "\n",
    "for i, row in stations_df.iterrows():\n",
    "    st_lat = row['LATITUDE']\n",
    "    st_lon = row['LONGITUDE']\n",
    "    dist_all = haversine_distance(st_lat, st_lon, grid_lat_flat, grid_lon_flat)\n",
    "    neighbor_idx = np.where(dist_all <= search_radius_km)[0]\n",
    "    if len(neighbor_idx) < min_points:\n",
    "        station_neighbors[i] = None\n",
    "    else:\n",
    "        station_neighbors[i] = neighbor_idx\n",
    "\n",
    "print(\"Neighbor precomputation complete.\")\n",
    "\n",
    "###############################################################################\n",
    "# 6. LOCAL WEIGHTED REGRESSION FUNCTION\n",
    "###############################################################################\n",
    "def local_weighted_regression(\n",
    "    station_lat, station_lon, station_elev,\n",
    "    neighbor_indices,\n",
    "    grid_lat, grid_lon, grid_elev, grid_val\n",
    "):\n",
    "    \"\"\"\n",
    "    Weighted linear regression of grid_val ~ [1, lat, lon, elev],\n",
    "    with tricube weighting based on station->grid distance.\n",
    "    We clip negative results to zero for precipitation.\n",
    "    \"\"\"\n",
    "    if neighbor_indices is None or len(neighbor_indices) < min_points:\n",
    "        return np.nan\n",
    "\n",
    "    lat_n  = grid_lat[neighbor_indices]\n",
    "    lon_n  = grid_lon[neighbor_indices]\n",
    "    elev_n = grid_elev[neighbor_indices]\n",
    "    val_n  = grid_val[neighbor_indices]\n",
    "\n",
    "    # Distances\n",
    "    dist_n = haversine_distance(station_lat, station_lon, lat_n, lon_n)\n",
    "    d_max  = dist_n.max()\n",
    "    w = tricube_weight(dist_n, d_max)\n",
    "    if np.all(w == 0):\n",
    "        return np.nan\n",
    "\n",
    "    # Build design matrix X: [1, lat, lon, elev]\n",
    "    X = np.column_stack([np.ones_like(lat_n), lat_n, lon_n, elev_n])\n",
    "    sqrt_w = np.sqrt(w)\n",
    "    X_w = X * sqrt_w[:, None]\n",
    "    y_w = val_n * sqrt_w\n",
    "\n",
    "    # Solve Weighted LS\n",
    "    try:\n",
    "        beta, residuals, rank, s = np.linalg.lstsq(X_w, y_w, rcond=None)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.nan\n",
    "\n",
    "    # Predict at station location\n",
    "    X_station = np.array([1, station_lat, station_lon, station_elev])\n",
    "    pred = X_station @ beta\n",
    "\n",
    "    # Clip negative precipitation to zero\n",
    "    if pred < 0:\n",
    "        pred = 0.0\n",
    "\n",
    "    return pred\n",
    "\n",
    "###############################################################################\n",
    "# 7. DAILY LOOP: RDRS -> STATIONS\n",
    "###############################################################################\n",
    "results = []\n",
    "rdrs_times = rdrs_ds['time'].values\n",
    "print(f\"Processing RDRS days: {len(rdrs_times)}\")\n",
    "\n",
    "for t_index, t_val in enumerate(rdrs_times):\n",
    "    current_time = pd.to_datetime(t_val)\n",
    "    day_data_2d = rdrs_ds[rdrs_var].isel(time=t_index).values  # shape => (lat, lon)\n",
    "    grid_val_flat = day_data_2d.flatten()\n",
    "\n",
    "    obs_day = obs_ds[obs_var_name].isel(time=t_index).values\n",
    "\n",
    "    for i, row in stations_df.iterrows():\n",
    "        st_name   = row['NAME']\n",
    "        st_lat    = row['LATITUDE']\n",
    "        st_lon    = row['LONGITUDE']\n",
    "        st_elev   = row['Elevation']  # If station has elevation, else 0\n",
    "        netcdf_idx = row['netcdf_index']\n",
    "\n",
    "        st_obs = obs_day[netcdf_idx] if netcdf_idx < len(obs_day) else np.nan\n",
    "\n",
    "        neigh_idx = station_neighbors[i]\n",
    "        st_rdrs = local_weighted_regression(\n",
    "            station_lat=st_lat,\n",
    "            station_lon=st_lon,\n",
    "            station_elev=st_elev,\n",
    "            neighbor_indices=neigh_idx,\n",
    "            grid_lat=grid_lat_flat,\n",
    "            grid_lon=grid_lon_flat,\n",
    "            grid_elev=grid_elev_flat,\n",
    "            grid_val=grid_val_flat\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'time': current_time,\n",
    "            'station_index': i,\n",
    "            'station_name': st_name,\n",
    "            'obs': st_obs,\n",
    "            'rdrs_val': st_rdrs\n",
    "        })\n",
    "\n",
    "    if (t_index+1) % 100 == 0:\n",
    "        print(f\"Processed day {t_index+1} / {len(rdrs_times)}\")\n",
    "\n",
    "###############################################################################\n",
    "# 8. BUILD A DATAFRAME & SAVE\n",
    "###############################################################################\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['time'] = pd.to_datetime(results_df['time'])\n",
    "print(\"Sample of daily results:\\n\", results_df.head())\n",
    "\n",
    "# Save the daily loop CSV\n",
    "daily_loop_csv = os.path.join(daily_loop_dir, \"rdrs_vs_stations_25km_LWR_1991_2012.csv\")\n",
    "results_df.to_csv(daily_loop_csv, index=False)\n",
    "print(f\"Daily interpolation results saved to {daily_loop_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 9. (OPTIONAL) MAPPING\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"Creating a quick map of average RDRS precipitation at each station ...\")\n",
    "    glb  = gpd.read_file(shapefile_path).to_crs(target_crs)\n",
    "    lakes= gpd.read_file(lakes_shp).to_crs(target_crs)\n",
    "\n",
    "    station_mean = results_df.groupby('station_index')['rdrs_val'].mean().reset_index()\n",
    "    merged = stations_df.copy()\n",
    "    merged['mean_rdrs'] = station_mean['rdrs_val']\n",
    "\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(merged['LONGITUDE'], merged['LATITUDE'])]\n",
    "    stations_gdf = gpd.GeoDataFrame(merged, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    stations_gdf = stations_gdf.to_crs(target_crs)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    glb.boundary.plot(ax=ax, edgecolor='black')\n",
    "    lakes.plot(ax=ax, color='blue', alpha=0.5)\n",
    "    stations_gdf.plot(column='mean_rdrs', ax=ax, legend=True,\n",
    "                      cmap='viridis', markersize=50)\n",
    "    ax.set_title(\"Mean RDRS Precip (LWR, 25km radius), 1991-2012\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Mapping step skipped due to error or missing shapefiles:\", str(e))\n",
    "\n",
    "###############################################################################\n",
    "# 10. METRICS & SAVING\n",
    "###############################################################################\n",
    "def remove_nan_pairs(obs, pred):\n",
    "    mask = ~np.isnan(obs) & ~np.isnan(pred)\n",
    "    return obs[mask], pred[mask]\n",
    "\n",
    "def mean_bias_error(obs, pred):\n",
    "    return np.mean(pred - obs)\n",
    "\n",
    "def root_mean_square_error(obs, pred):\n",
    "    return np.sqrt(np.mean((pred - obs)**2))\n",
    "\n",
    "def std_of_residuals(obs, pred):\n",
    "    return np.std(pred - obs, ddof=1)\n",
    "\n",
    "def pearson_correlation(obs, pred):\n",
    "    if len(obs) < 2:\n",
    "        return np.nan\n",
    "    return np.corrcoef(obs, pred)[0, 1]\n",
    "\n",
    "def index_of_agreement(obs, pred):\n",
    "    obs_mean = np.mean(obs)\n",
    "    numerator = np.sum((pred - obs)**2)\n",
    "    denominator = np.sum((abs(pred - obs_mean) + abs(obs - obs_mean))**2)\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "obs_all = results_df['obs'].values\n",
    "rdrs_all = results_df['rdrs_val'].values\n",
    "obs_all, rdrs_all = remove_nan_pairs(obs_all, rdrs_all)\n",
    "\n",
    "if len(obs_all) > 0:\n",
    "    metrics_all = {\n",
    "        'MBE': mean_bias_error(obs_all, rdrs_all),\n",
    "        'RMSE': root_mean_square_error(obs_all, rdrs_all),\n",
    "        'STD': std_of_residuals(obs_all, rdrs_all),\n",
    "        'CC': pearson_correlation(obs_all, rdrs_all),\n",
    "        'Index_of_Agreement': index_of_agreement(obs_all, rdrs_all)\n",
    "    }\n",
    "    print(\"\\nOverall Metrics (All Stations, LWR, 1991-2012):\")\n",
    "    for k, v in metrics_all.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid (obs, rdrs_val) pairs found for overall metrics.\")\n",
    "\n",
    "# Station-level metrics\n",
    "station_groups = results_df.groupby('station_index')\n",
    "per_station_metrics = []\n",
    "for st_idx, grp in station_groups:\n",
    "    obs_st = grp['obs'].values\n",
    "    rdrs_st = grp['rdrs_val'].values\n",
    "    obs_st, rdrs_st = remove_nan_pairs(obs_st, rdrs_st)\n",
    "    if len(obs_st) == 0:\n",
    "        per_station_metrics.append({\n",
    "            'station_index': st_idx,\n",
    "            'station_name': grp['station_name'].iloc[0],\n",
    "            'MBE': np.nan,\n",
    "            'RMSE': np.nan,\n",
    "            'STD': np.nan,\n",
    "            'CC': np.nan,\n",
    "            'Index_of_Agreement': np.nan\n",
    "        })\n",
    "        continue\n",
    "    row_dict = {\n",
    "        'station_index': st_idx,\n",
    "        'station_name': grp['station_name'].iloc[0],\n",
    "        'MBE':  mean_bias_error(obs_st, rdrs_st),\n",
    "        'RMSE': root_mean_square_error(obs_st, rdrs_st),\n",
    "        'STD':  std_of_residuals(obs_st, rdrs_st),\n",
    "        'CC':   pearson_correlation(obs_st, rdrs_st),\n",
    "        'Index_of_Agreement': index_of_agreement(obs_st, rdrs_st)\n",
    "    }\n",
    "    per_station_metrics.append(row_dict)\n",
    "\n",
    "metrics_df = pd.DataFrame(per_station_metrics)\n",
    "print(\"\\nSample of per-station metrics:\")\n",
    "print(metrics_df.head())\n",
    "\n",
    "metrics_csv = os.path.join(metrics_dir, \"station_metrics_25km_LWR_1991_2012.csv\")\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "print(f\"Station metrics saved to {metrics_csv}\")\n",
    "\n",
    "# (Optional) NetCDF\n",
    "###############################################################################\n",
    "# 10. (OPTIONAL) SAVE FULL DAILY RESULTS AS NETCDF\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nSaving daily station results to NetCDF file in daily_loop folder...\")\n",
    "    netcdf_file = os.path.join(daily_loop_dir, \"rdrs_vs_stations_25km_LWR_1991_2012.nc\")\n",
    "\n",
    "    # 1) Pivot so 'time' is the row index, 'station_index' are columns\n",
    "    pivot_rdrs = results_df.pivot(index='time', columns='station_index', values='rdrs_val')\n",
    "    pivot_obs  = results_df.pivot(index='time', columns='station_index', values='obs')\n",
    "\n",
    "    # 2) Ensure the pivoted index is recognized as real datetime\n",
    "    pivot_rdrs.index = pd.to_datetime(pivot_rdrs.index, errors='coerce')\n",
    "    pivot_obs.index  = pd.to_datetime(pivot_obs.index, errors='coerce')\n",
    "\n",
    "    # 3) Convert each pivoted DataFrame into an xarray Dataset\n",
    "    ds_rdrs = pivot_rdrs.to_xarray()   # might have dims: ('time', 'station_index')\n",
    "    ds_obs  = pivot_obs.to_xarray()\n",
    "\n",
    "    #    If needed, rename dims. For instance, some versions produce dims ('index','columns').\n",
    "    if 'index' in ds_rdrs.dims:\n",
    "        ds_rdrs = ds_rdrs.rename({'index': 'time'})\n",
    "    if 'columns' in ds_rdrs.dims:\n",
    "        ds_rdrs = ds_rdrs.rename({'columns': 'station_index'})\n",
    "    if 'index' in ds_obs.dims:\n",
    "        ds_obs = ds_obs.rename({'index': 'time'})\n",
    "    if 'columns' in ds_obs.dims:\n",
    "        ds_obs = ds_obs.rename({'columns': 'station_index'})\n",
    "\n",
    "    # 4) Turn each into a single DataArray:\n",
    "    da_rdrs = ds_rdrs.to_array(name='rdrs_val').squeeze()  # dims => ('rdrs_val','time','station_index') => squeeze => ('time','station_index')\n",
    "    da_obs  = ds_obs.to_array(name='obs').squeeze()\n",
    "\n",
    "    # 5) Combine into a single Dataset with two DataArrays\n",
    "    ds_out = xr.Dataset({\n",
    "        'rdrs_val': da_rdrs,\n",
    "        'obs':      da_obs\n",
    "    })\n",
    "\n",
    "    ds_out.to_netcdf(netcdf_file)\n",
    "    print(f\"NetCDF saved to: {netcdf_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error saving NetCDF:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6c075-b27e-49e9-a61a-c113ef9ad550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHIRPS v2.0\n",
    "# The approach of making the interpolated CHIRPS data at the station location with LWR (25km)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATIONS\n",
    "###############################################################################\n",
    "station_file   = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\prcp_data.nc'\n",
    "chirps_file    = r'D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\CHIRPS_GLB_Precipitation\\Masked to GLB\\CHIRPS_GLB_1991_2013_with_Elevation.nc'\n",
    "physical_file  = r'D:\\PhD\\GLB\\Merged USA and CA\\Entire GLB\\filtered_stations_with_elevation.csv'\n",
    "shapefile_path = r\"D:\\PhD\\GLB\\greatlakes_subbasins\\New folder\\Great_Lakes.shp\"\n",
    "lakes_shp      = r'D:\\PhD\\GLB\\greatlakes_subbasins\\GLB_Water_Bodies\\Main_Lakes_GLB.shp'\n",
    "target_crs     = \"ESRI:102008\"\n",
    "\n",
    "# Paths for daily loop & metrics (using CHIRPS directory)\n",
    "daily_loop_dir    = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\CHIRPS_GLB_Precipitation\\Masked to GLB\\daily_loop\"\n",
    "metrics_dir       = r\"D:\\PhD\\GLB\\EMDNA(Historical data)\\Ensembles\\New folder\\Ensemble files\\CHIRPS_GLB_Precipitation\\Masked to GLB\\metrics\"\n",
    "\n",
    "os.makedirs(daily_loop_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir,    exist_ok=True)\n",
    "\n",
    "# Variable names (both observed and gridded CHIRPS use 'prcp')\n",
    "obs_var_name   = 'prcp'\n",
    "chirps_var     = 'prcp'\n",
    "\n",
    "# Local Weighted Regression settings\n",
    "search_radius_km = 25   # Reduced from 50 km to 25 km\n",
    "min_points = 5\n",
    "\n",
    "# Time range: 1991–2012 (adjust if needed)\n",
    "start_date = '1991-01-01'\n",
    "end_date   = '2012-12-31'\n",
    "\n",
    "###############################################################################\n",
    "# 2. DISTANCE & WEIGHT FUNCTIONS\n",
    "###############################################################################\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = (np.sin(dlat / 2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon / 2)**2)\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c\n",
    "\n",
    "def tricube_weight(distances, d_max):\n",
    "    \"\"\"\n",
    "    w_j = [1 - (dist_j / d_max)^3]^3  for dist_j <= d_max; else 0\n",
    "    \"\"\"\n",
    "    if d_max == 0:\n",
    "        return np.ones_like(distances)\n",
    "    ratio = distances / d_max\n",
    "    w = (1 - ratio**3)**3\n",
    "    w[distances > d_max] = 0.0\n",
    "    return w\n",
    "\n",
    "###############################################################################\n",
    "# 3. FORCE CHIRPS TIME (IF NEEDED)\n",
    "###############################################################################\n",
    "def force_chirps_time(ds):\n",
    "    if 'time' not in ds.coords and 'time' in ds.dims:\n",
    "        ds = ds.assign_coords(time=ds['time'])\n",
    "    if np.issubdtype(ds['time'].dtype, np.integer):\n",
    "        day0 = pd.to_datetime(\"1991-01-01\")\n",
    "        numeric_days = ds['time'].values\n",
    "        real_times = [day0 + pd.Timedelta(days=int(d)) for d in numeric_days]\n",
    "        ds = ds.assign_coords(time=(\"time\", real_times))\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "    return ds\n",
    "\n",
    "print(\"Loading station observations (NetCDF) ...\")\n",
    "obs_ds = xr.open_dataset(station_file)\n",
    "\n",
    "print(\"Loading CHIRPS gridded dataset (NetCDF) ...\")\n",
    "chirps_ds = xr.open_dataset(chirps_file)\n",
    "\n",
    "# Fix time coordinates if needed\n",
    "if 'time' in obs_ds.coords:\n",
    "    obs_ds['time'] = pd.to_datetime(obs_ds['time'].values)\n",
    "chirps_ds = force_chirps_time(chirps_ds)\n",
    "\n",
    "# Subset to 1991–2012\n",
    "obs_ds   = obs_ds.sel(time=slice(start_date, end_date))\n",
    "chirps_ds  = chirps_ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "print(\"After subsetting:\")\n",
    "print(f\"obs_ds time steps = {obs_ds.sizes['time']}\")\n",
    "print(f\"chirps_ds time steps = {chirps_ds.sizes['time']}\")\n",
    "\n",
    "# If dims are (time, lon, lat), transpose to (time, lat, lon)\n",
    "print(\"Checking dimension order of the CHIRPS variable ...\")\n",
    "actual_dims = chirps_ds[chirps_var].dims\n",
    "print(\"Current dims for prcp:\", actual_dims)\n",
    "if actual_dims == (\"time\", \"lon\", \"lat\"):\n",
    "    print(\"Transposing prcp from (time,lon,lat) -> (time,lat,lon).\")\n",
    "    chirps_ds[chirps_var] = chirps_ds[chirps_var].transpose(\"time\", \"lat\", \"lon\")\n",
    "    print(\"New dims:\", chirps_ds[chirps_var].dims)\n",
    "else:\n",
    "    print(\"No transpose needed or dims are already (time,lat,lon).\")\n",
    "\n",
    "lats = chirps_ds['lat'].values\n",
    "lons = chirps_ds['lon'].values\n",
    "lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "\n",
    "if 'elevation' in chirps_ds:\n",
    "    grid_elev = chirps_ds['elevation'].values\n",
    "else:\n",
    "    warnings.warn(\"No 'elevation' found in CHIRPS dataset => setting elev=0.\")\n",
    "    grid_elev = np.zeros_like(lat2d)\n",
    "\n",
    "grid_lat_flat  = lat2d.flatten()\n",
    "grid_lon_flat  = lon2d.flatten()\n",
    "grid_elev_flat = grid_elev.flatten()\n",
    "\n",
    "###############################################################################\n",
    "# 4. LOAD STATIONS & MATCH\n",
    "###############################################################################\n",
    "print(\"Loading station metadata (CSV) ...\")\n",
    "stations_df = pd.read_csv(physical_file).dropna(axis=1, how='all')\n",
    "print(stations_df.head())\n",
    "\n",
    "print(\"Matching station names to NetCDF index ...\")\n",
    "station_names_nc = obs_ds['station'].values\n",
    "station_index_map = {name: i for i, name in enumerate(station_names_nc)}\n",
    "stations_df['netcdf_index'] = stations_df['NAME'].map(station_index_map)\n",
    "stations_df = stations_df.dropna(subset=['netcdf_index']).reset_index(drop=True)\n",
    "stations_df['netcdf_index'] = stations_df['netcdf_index'].astype(int)\n",
    "print(\"Total matched stations:\", len(stations_df))\n",
    "\n",
    "###############################################################################\n",
    "# 5. PRECOMPUTE NEIGHBORS FOR EACH STATION\n",
    "###############################################################################\n",
    "station_neighbors = {}\n",
    "print(f\"Precomputing neighbors within {search_radius_km} km...\")\n",
    "\n",
    "for i, row in stations_df.iterrows():\n",
    "    st_lat = row['LATITUDE']\n",
    "    st_lon = row['LONGITUDE']\n",
    "    dist_all = haversine_distance(st_lat, st_lon, grid_lat_flat, grid_lon_flat)\n",
    "    neighbor_idx = np.where(dist_all <= search_radius_km)[0]\n",
    "    if len(neighbor_idx) < min_points:\n",
    "        station_neighbors[i] = None\n",
    "    else:\n",
    "        station_neighbors[i] = neighbor_idx\n",
    "\n",
    "print(\"Neighbor precomputation complete.\")\n",
    "\n",
    "###############################################################################\n",
    "# 6. LOCAL WEIGHTED REGRESSION FUNCTION\n",
    "###############################################################################\n",
    "def local_weighted_regression(\n",
    "    station_lat, station_lon, station_elev,\n",
    "    neighbor_indices,\n",
    "    grid_lat, grid_lon, grid_elev, grid_val\n",
    "):\n",
    "    \"\"\"\n",
    "    Weighted linear regression of grid_val ~ [1, lat, lon, elev],\n",
    "    with tricube weighting based on station->grid distance.\n",
    "    Negative precipitation predictions are clipped to zero.\n",
    "    \"\"\"\n",
    "    if neighbor_indices is None or len(neighbor_indices) < min_points:\n",
    "        return np.nan\n",
    "\n",
    "    lat_n  = grid_lat[neighbor_indices]\n",
    "    lon_n  = grid_lon[neighbor_indices]\n",
    "    elev_n = grid_elev[neighbor_indices]\n",
    "    val_n  = grid_val[neighbor_indices]\n",
    "\n",
    "    # Compute distances and weights\n",
    "    dist_n = haversine_distance(station_lat, station_lon, lat_n, lon_n)\n",
    "    d_max  = dist_n.max()\n",
    "    w = tricube_weight(dist_n, d_max)\n",
    "    if np.all(w == 0):\n",
    "        return np.nan\n",
    "\n",
    "    # Build design matrix X: [1, lat, lon, elev]\n",
    "    X = np.column_stack([np.ones_like(lat_n), lat_n, lon_n, elev_n])\n",
    "    sqrt_w = np.sqrt(w)\n",
    "    X_w = X * sqrt_w[:, None]\n",
    "    y_w = val_n * sqrt_w\n",
    "\n",
    "    # Solve weighted least squares\n",
    "    try:\n",
    "        beta, residuals, rank, s = np.linalg.lstsq(X_w, y_w, rcond=None)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.nan\n",
    "\n",
    "    # Predict at station location\n",
    "    X_station = np.array([1, station_lat, station_lon, station_elev])\n",
    "    pred = X_station @ beta\n",
    "\n",
    "    # Clip negative precipitation to zero\n",
    "    if pred < 0:\n",
    "        pred = 0.0\n",
    "\n",
    "    return pred\n",
    "\n",
    "###############################################################################\n",
    "# 7. DAILY LOOP: CHIRPS -> STATIONS\n",
    "###############################################################################\n",
    "results = []\n",
    "chirps_times = chirps_ds['time'].values\n",
    "print(f\"Processing CHIRPS days: {len(chirps_times)}\")\n",
    "\n",
    "for t_index, t_val in enumerate(chirps_times):\n",
    "    current_time = pd.to_datetime(t_val)\n",
    "    day_data_2d = chirps_ds[chirps_var].isel(time=t_index).values  # shape => (lat, lon)\n",
    "    grid_val_flat = day_data_2d.flatten()\n",
    "\n",
    "    obs_day = obs_ds[obs_var_name].isel(time=t_index).values\n",
    "\n",
    "    for i, row in stations_df.iterrows():\n",
    "        st_name   = row['NAME']\n",
    "        st_lat    = row['LATITUDE']\n",
    "        st_lon    = row['LONGITUDE']\n",
    "        st_elev   = row['Elevation']  # Use station elevation if available; else 0\n",
    "        netcdf_idx = row['netcdf_index']\n",
    "\n",
    "        st_obs = obs_day[netcdf_idx] if netcdf_idx < len(obs_day) else np.nan\n",
    "\n",
    "        neigh_idx = station_neighbors[i]\n",
    "        st_chirps = local_weighted_regression(\n",
    "            station_lat=st_lat,\n",
    "            station_lon=st_lon,\n",
    "            station_elev=st_elev,\n",
    "            neighbor_indices=neigh_idx,\n",
    "            grid_lat=grid_lat_flat,\n",
    "            grid_lon=grid_lon_flat,\n",
    "            grid_elev=grid_elev_flat,\n",
    "            grid_val=grid_val_flat\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'time': current_time,\n",
    "            'station_index': i,\n",
    "            'station_name': st_name,\n",
    "            'obs': st_obs,\n",
    "            'chirps_val': st_chirps\n",
    "        })\n",
    "\n",
    "    if (t_index + 1) % 100 == 0:\n",
    "        print(f\"Processed day {t_index + 1} / {len(chirps_times)}\")\n",
    "\n",
    "###############################################################################\n",
    "# 8. BUILD A DATAFRAME & SAVE\n",
    "###############################################################################\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['time'] = pd.to_datetime(results_df['time'])\n",
    "print(\"Sample of daily results:\\n\", results_df.head())\n",
    "\n",
    "# Save the daily loop CSV (note the filename reflects 50km radius)\n",
    "daily_loop_csv = os.path.join(daily_loop_dir, \"chirps_vs_stations_25km_LWR_1991_2012.csv\")\n",
    "results_df.to_csv(daily_loop_csv, index=False)\n",
    "print(f\"Daily interpolation results saved to {daily_loop_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 9. (OPTIONAL) MAPPING\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"Creating a quick map of average CHIRPS precipitation at each station ...\")\n",
    "    glb  = gpd.read_file(shapefile_path).to_crs(target_crs)\n",
    "    lakes = gpd.read_file(lakes_shp).to_crs(target_crs)\n",
    "\n",
    "    station_mean = results_df.groupby('station_index')['chirps_val'].mean().reset_index()\n",
    "    merged = stations_df.copy()\n",
    "    merged['mean_chirps'] = station_mean['chirps_val']\n",
    "\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(merged['LONGITUDE'], merged['LATITUDE'])]\n",
    "    stations_gdf = gpd.GeoDataFrame(merged, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    stations_gdf = stations_gdf.to_crs(target_crs)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    glb.boundary.plot(ax=ax, edgecolor='black')\n",
    "    lakes.plot(ax=ax, color='blue', alpha=0.5)\n",
    "    stations_gdf.plot(column='mean_chirps', ax=ax, legend=True,\n",
    "                      cmap='viridis', markersize=50)\n",
    "    ax.set_title(\"Mean CHIRPS Precip (LWR, 25km radius), 1991-2012\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Mapping step skipped due to error or missing shapefiles:\", str(e))\n",
    "\n",
    "###############################################################################\n",
    "# 10. METRICS & SAVING\n",
    "###############################################################################\n",
    "def remove_nan_pairs(obs, pred):\n",
    "    mask = ~np.isnan(obs) & ~np.isnan(pred)\n",
    "    return obs[mask], pred[mask]\n",
    "\n",
    "def mean_bias_error(obs, pred):\n",
    "    return np.mean(pred - obs)\n",
    "\n",
    "def root_mean_square_error(obs, pred):\n",
    "    return np.sqrt(np.mean((pred - obs)**2))\n",
    "\n",
    "def std_of_residuals(obs, pred):\n",
    "    return np.std(pred - obs, ddof=1)\n",
    "\n",
    "def pearson_correlation(obs, pred):\n",
    "    if len(obs) < 2:\n",
    "        return np.nan\n",
    "    return np.corrcoef(obs, pred)[0, 1]\n",
    "\n",
    "def index_of_agreement(obs, pred):\n",
    "    obs_mean = np.mean(obs)\n",
    "    numerator = np.sum((pred - obs)**2)\n",
    "    denominator = np.sum((np.abs(pred - obs_mean) + np.abs(obs - obs_mean))**2)\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "obs_all = results_df['obs'].values\n",
    "chirps_all = results_df['chirps_val'].values\n",
    "obs_all, chirps_all = remove_nan_pairs(obs_all, chirps_all)\n",
    "\n",
    "if len(obs_all) > 0:\n",
    "    metrics_all = {\n",
    "        'MBE': mean_bias_error(obs_all, chirps_all),\n",
    "        'RMSE': root_mean_square_error(obs_all, chirps_all),\n",
    "        'STD': std_of_residuals(obs_all, chirps_all),\n",
    "        'CC': pearson_correlation(obs_all, chirps_all),\n",
    "        'Index_of_Agreement': index_of_agreement(obs_all, chirps_all)\n",
    "    }\n",
    "    print(\"\\nOverall Metrics (All Stations, LWR, 1991-2012):\")\n",
    "    for k, v in metrics_all.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid (obs, chirps_val) pairs found for overall metrics.\")\n",
    "\n",
    "# Station-level metrics\n",
    "station_groups = results_df.groupby('station_index')\n",
    "per_station_metrics = []\n",
    "for st_idx, grp in station_groups:\n",
    "    obs_st = grp['obs'].values\n",
    "    chirps_st = grp['chirps_val'].values\n",
    "    obs_st, chirps_st = remove_nan_pairs(obs_st, chirps_st)\n",
    "    if len(obs_st) == 0:\n",
    "        per_station_metrics.append({\n",
    "            'station_index': st_idx,\n",
    "            'station_name': grp['station_name'].iloc[0],\n",
    "            'MBE': np.nan,\n",
    "            'RMSE': np.nan,\n",
    "            'STD': np.nan,\n",
    "            'CC': np.nan,\n",
    "            'Index_of_Agreement': np.nan\n",
    "        })\n",
    "        continue\n",
    "    row_dict = {\n",
    "        'station_index': st_idx,\n",
    "        'station_name': grp['station_name'].iloc[0],\n",
    "        'MBE':  mean_bias_error(obs_st, chirps_st),\n",
    "        'RMSE': root_mean_square_error(obs_st, chirps_st),\n",
    "        'STD':  std_of_residuals(obs_st, chirps_st),\n",
    "        'CC':   pearson_correlation(obs_st, chirps_st),\n",
    "        'Index_of_Agreement': index_of_agreement(obs_st, chirps_st)\n",
    "    }\n",
    "    per_station_metrics.append(row_dict)\n",
    "\n",
    "metrics_df = pd.DataFrame(per_station_metrics)\n",
    "print(\"\\nSample of per-station metrics:\")\n",
    "print(metrics_df.head())\n",
    "\n",
    "metrics_csv = os.path.join(metrics_dir, \"station_metrics_25km_LWR_1991_2012.csv\")\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "print(f\"Station metrics saved to {metrics_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 11. (OPTIONAL) SAVE FULL DAILY RESULTS AS NETCDF\n",
    "###############################################################################\n",
    "try:\n",
    "    print(\"\\nSaving daily station results to NetCDF file in daily_loop folder...\")\n",
    "    netcdf_file = os.path.join(daily_loop_dir, \"chirps_vs_stations_25km_LWR_1991_2012.nc\")\n",
    "\n",
    "    # Pivot so that 'time' is rows and 'station_index' are columns\n",
    "    pivot_chirps = results_df.pivot(index='time', columns='station_index', values='chirps_val')\n",
    "    pivot_obs    = results_df.pivot(index='time', columns='station_index', values='obs')\n",
    "\n",
    "    # Ensure pivoted index is in datetime format\n",
    "    pivot_chirps.index = pd.to_datetime(pivot_chirps.index, errors='coerce')\n",
    "    pivot_obs.index    = pd.to_datetime(pivot_obs.index, errors='coerce')\n",
    "\n",
    "    # Convert each pivoted DataFrame into an xarray Dataset\n",
    "    ds_chirps = pivot_chirps.to_xarray()   # dims might be ('time', 'station_index')\n",
    "    ds_obs    = pivot_obs.to_xarray()\n",
    "\n",
    "    # Rename dims if necessary\n",
    "    if 'index' in ds_chirps.dims:\n",
    "        ds_chirps = ds_chirps.rename({'index': 'time'})\n",
    "    if 'columns' in ds_chirps.dims:\n",
    "        ds_chirps = ds_chirps.rename({'columns': 'station_index'})\n",
    "    if 'index' in ds_obs.dims:\n",
    "        ds_obs = ds_obs.rename({'index': 'time'})\n",
    "    if 'columns' in ds_obs.dims:\n",
    "        ds_obs = ds_obs.rename({'columns': 'station_index'})\n",
    "\n",
    "    # Convert each into a DataArray and combine into one Dataset\n",
    "    da_chirps = ds_chirps.to_array(name='chirps_val').squeeze()\n",
    "    da_obs    = ds_obs.to_array(name='obs').squeeze()\n",
    "\n",
    "    ds_out = xr.Dataset({\n",
    "        'chirps_val': da_chirps,\n",
    "        'obs':        da_obs\n",
    "    })\n",
    "\n",
    "    ds_out.to_netcdf(netcdf_file)\n",
    "    print(f\"NetCDF saved to: {netcdf_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error saving NetCDF:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
